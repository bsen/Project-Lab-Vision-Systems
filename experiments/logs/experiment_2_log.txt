== Status ==
Memory usage on this node: 21.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+-------+--------------+-------------+------------+-------------+--------------+-----------+
| Trial name        | status   | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |
|-------------------+----------+-------+--------------+-------------+------------+-------------+--------------+-----------|
| train_b1c14_00000 | RUNNING  |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |
| train_b1c14_00001 | PENDING  |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |
| train_b1c14_00002 | PENDING  |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |
| train_b1c14_00003 | PENDING  |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |
| train_b1c14_00004 | PENDING  |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |
| train_b1c14_00005 | PENDING  |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |
| train_b1c14_00006 | PENDING  |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |
| train_b1c14_00007 | PENDING  |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |
| train_b1c14_00008 | PENDING  |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |
| train_b1c14_00009 | PENDING  |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |
| train_b1c14_00010 | PENDING  |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |
| train_b1c14_00011 | PENDING  |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |
| train_b1c14_00012 | PENDING  |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |
| train_b1c14_00013 | PENDING  |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |
| train_b1c14_00014 | PENDING  |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |
| train_b1c14_00015 | PENDING  |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |
| train_b1c14_00016 | PENDING  |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |
| train_b1c14_00017 | PENDING  |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |
| train_b1c14_00018 | PENDING  |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |
| train_b1c14_00019 | PENDING  |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |
+-------------------+----------+-------+--------------+-------------+------------+-------------+--------------+-----------+
... 5 more trials not shown (5 PENDING)


== Status ==
Memory usage on this node: 23.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+-------+--------------+-------------+------------+-------------+--------------+-----------+
| Trial name        | status   | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |
|-------------------+----------+-------+--------------+-------------+------------+-------------+--------------+-----------|
| train_b1c14_00000 | RUNNING  |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |
| train_b1c14_00001 | PENDING  |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |
| train_b1c14_00002 | PENDING  |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |
| train_b1c14_00003 | PENDING  |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |
| train_b1c14_00004 | PENDING  |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |
| train_b1c14_00005 | PENDING  |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |
| train_b1c14_00006 | PENDING  |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |
| train_b1c14_00007 | PENDING  |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |
| train_b1c14_00008 | PENDING  |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |
| train_b1c14_00009 | PENDING  |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |
| train_b1c14_00010 | PENDING  |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |
| train_b1c14_00011 | PENDING  |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |
| train_b1c14_00012 | PENDING  |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |
| train_b1c14_00013 | PENDING  |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |
| train_b1c14_00014 | PENDING  |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |
| train_b1c14_00015 | PENDING  |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |
| train_b1c14_00016 | PENDING  |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |
| train_b1c14_00017 | PENDING  |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |
| train_b1c14_00018 | PENDING  |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |
| train_b1c14_00019 | PENDING  |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |
+-------------------+----------+-------+--------------+-------------+------------+-------------+--------------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m Epoch:
[2m[36m(pid=13132)[0m 0, 
[2m[36m(pid=13132)[0m  train loss: 31.71228563785553
[2m[36m(pid=13132)[0m  eval loss: 63.482441177368166, eval err: 0.9631260848045349
Result for train_b1c14_00000:
  date: 2021-09-09_09-24-28
  done: false
  err: 0.9631260848045349
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 63.482441177368166
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 115.56735444068909
  time_this_iter_s: 115.56735444068909
  time_total_s: 115.56735444068909
  timestamp: 1631172268
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |      1 |          115.567 | 63.4824 | 0.963126 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 1, 
[2m[36m(pid=13132)[0m  train loss: 22.35868725180626
[2m[36m(pid=13132)[0m  eval loss: 23.253192596435547, eval err: 0.9074643635749817
[2m[36m(pid=13132)[0m 2, 
[2m[36m(pid=13132)[0m  train loss: 12.809211552143097
[2m[36m(pid=13132)[0m  eval loss: 13.225324821472167, eval err: 0.8549908065795898
[2m[36m(pid=13132)[0m 3, 
[2m[36m(pid=13132)[0m  train loss: 10.20140889286995
Result for train_b1c14_00000:
  date: 2021-09-09_09-30-04
  done: false
  err: 0.8306565594673156
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 11.114878845214843
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 451.3105010986328
  time_this_iter_s: 335.7431466579437
  time_total_s: 451.3105010986328
  timestamp: 1631172604
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00000
  
[2m[36m(pid=13132)[0m  eval loss: 11.114878845214843, eval err: 0.8306565594673156
== Status ==
Memory usage on this node: 23.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |      2 |          451.311 | 11.1149 | 0.830657 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 4, 
[2m[36m(pid=13132)[0m  train loss: 10.190433725714684
[2m[36m(pid=13132)[0m  eval loss: 11.909292411804199, eval err: 0.8362823963165283
[2m[36m(pid=13132)[0m 5, 
[2m[36m(pid=13132)[0m  train loss: 9.932838201522827
[2m[36m(pid=13132)[0m  eval loss: 10.88476583480835, eval err: 0.8214732384681702
[2m[36m(pid=13132)[0m 6, 
[2m[36m(pid=13132)[0m  train loss: 8.904580518603325
[2m[36m(pid=13132)[0m  eval loss: 10.662493438720704, eval err: 0.812508475780487
Result for train_b1c14_00000:
  date: 2021-09-09_09-35-38
  done: false
  err: 0.812508475780487
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 10.662493438720704
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 785.0803637504578
  time_this_iter_s: 333.76986265182495
  time_total_s: 785.0803637504578
  timestamp: 1631172938
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |      3 |           785.08 | 10.6625 | 0.812508 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 7, 
[2m[36m(pid=13132)[0m  train loss: 9.27547413110733
[2m[36m(pid=13132)[0m  eval loss: 10.821979484558106, eval err: 0.8246200704574584
[2m[36m(pid=13132)[0m 8, 
[2m[36m(pid=13132)[0m  train loss: 8.75040939450264
[2m[36m(pid=13132)[0m  eval loss: 10.149395027160644, eval err: 0.8143996834754944
[2m[36m(pid=13132)[0m 9, 
[2m[36m(pid=13132)[0m  train loss: 9.951840206980705
Result for train_b1c14_00000:
  date: 2021-09-09_09-41-11
  done: false
  err: 0.7942314291000366
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 8.883556270599366
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 1118.9057083129883
  time_this_iter_s: 333.8253445625305
  time_total_s: 1118.9057083129883
  timestamp: 1631173271
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00000
  
[2m[36m(pid=13132)[0m  eval loss: 8.883556270599366, eval err: 0.7942314291000366
== Status ==
Memory usage on this node: 23.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |      4 |          1118.91 | 8.88356 | 0.794231 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 10, 
[2m[36m(pid=13132)[0m  train loss: 8.478757813572884
[2m[36m(pid=13132)[0m  eval loss: 10.601419906616211, eval err: 0.8231989884376526
[2m[36m(pid=13132)[0m 11, 
[2m[36m(pid=13132)[0m  train loss: 9.126132994890213
[2m[36m(pid=13132)[0m  eval loss: 8.760686740875244, eval err: 0.7998562979698182
[2m[36m(pid=13132)[0m 12, 
[2m[36m(pid=13132)[0m  train loss: 8.280559107661247
[2m[36m(pid=13132)[0m  eval loss: 8.560266342163086, eval err: 0.7866342329978943
Result for train_b1c14_00000:
  date: 2021-09-09_09-46-45
  done: false
  err: 0.7866342329978943
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 8.560266342163086
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 1452.403124332428
  time_this_iter_s: 333.4974160194397
  time_total_s: 1452.403124332428
  timestamp: 1631173605
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |      5 |           1452.4 | 8.56027 | 0.786634 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 13, 
[2m[36m(pid=13132)[0m  train loss: 8.385919079184532
[2m[36m(pid=13132)[0m  eval loss: 10.548120155334473, eval err: 0.8147017121315002
[2m[36m(pid=13132)[0m 14, 
[2m[36m(pid=13132)[0m  train loss: 8.64998160302639
[2m[36m(pid=13132)[0m  eval loss: 9.35927583694458, eval err: 0.7994255137443542
[2m[36m(pid=13132)[0m 15, 
[2m[36m(pid=13132)[0m  train loss: 7.600814685225487
Result for train_b1c14_00000:
  date: 2021-09-09_09-52-19
  done: false
  err: 0.78743332862854
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 9.001401119232177
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 1786.275889635086
  time_this_iter_s: 333.8727653026581
  time_total_s: 1786.275889635086
  timestamp: 1631173939
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+--------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |   loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+--------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |      6 |          1786.28 | 9.0014 | 0.787433 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |        |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |        |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |        |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |        |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |        |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |        |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |        |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |        |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |        |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |        |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |        |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |        |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |        |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |        |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |        |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |        |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |        |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |        |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |        |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+--------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m  eval loss: 9.001401119232177, eval err: 0.78743332862854
[2m[36m(pid=13132)[0m 16, 
[2m[36m(pid=13132)[0m  train loss: 7.63011808693409
[2m[36m(pid=13132)[0m  eval loss: 9.728339576721192, eval err: 0.7943499898910522
[2m[36m(pid=13132)[0m 17, 
[2m[36m(pid=13132)[0m  train loss: 7.99054154753685
[2m[36m(pid=13132)[0m  eval loss: 9.304281311035156, eval err: 0.7930925631523132
[2m[36m(pid=13132)[0m 18, 
[2m[36m(pid=13132)[0m  train loss: 8.612399503588676
[2m[36m(pid=13132)[0m  eval loss: 8.570366802215576, eval err: 0.7662453436851502
Result for train_b1c14_00000:
  date: 2021-09-09_09-57-53
  done: false
  err: 0.7662453436851502
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 8.570366802215576
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 2120.502162694931
  time_this_iter_s: 334.22627305984497
  time_total_s: 2120.502162694931
  timestamp: 1631174273
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |      7 |           2120.5 | 8.57037 | 0.766245 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 19, 
[2m[36m(pid=13132)[0m  train loss: 8.160623118281364
[2m[36m(pid=13132)[0m  eval loss: 8.752904243469239, eval err: 0.7777644228935242
[2m[36m(pid=13132)[0m 20, 
[2m[36m(pid=13132)[0m  train loss: 7.623980686068535
[2m[36m(pid=13132)[0m  eval loss: 8.768113803863525, eval err: 0.7773588943481445
[2m[36m(pid=13132)[0m 21, 
[2m[36m(pid=13132)[0m  train loss: 7.520523577928543
[2m[36m(pid=13132)[0m  eval loss: 9.226712074279785, eval err: 0.7824974393844605
Result for train_b1c14_00000:
  date: 2021-09-09_10-03-27
  done: false
  err: 0.7824974393844605
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 9.226712074279785
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 2454.838683128357
  time_this_iter_s: 334.3365204334259
  time_total_s: 2454.838683128357
  timestamp: 1631174607
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |      8 |          2454.84 | 9.22671 | 0.782497 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 22, 
[2m[36m(pid=13132)[0m  train loss: 7.430163741111755
[2m[36m(pid=13132)[0m  eval loss: 8.417773170471191, eval err: 0.7714545178413391
[2m[36m(pid=13132)[0m 23, 
[2m[36m(pid=13132)[0m  train loss: 7.07400968670845
[2m[36m(pid=13132)[0m  eval loss: 7.302136936187744, eval err: 0.7350298953056336
[2m[36m(pid=13132)[0m 24, 
[2m[36m(pid=13132)[0m  train loss: 7.455036953091621
[2m[36m(pid=13132)[0m  eval loss: 8.569528636932374, eval err: 0.7521141314506531
Result for train_b1c14_00000:
  date: 2021-09-09_10-09-02
  done: false
  err: 0.7521141314506531
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 8.569528636932374
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 2789.8946566581726
  time_this_iter_s: 335.0559735298157
  time_total_s: 2789.8946566581726
  timestamp: 1631174942
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |      9 |          2789.89 | 8.56953 | 0.752114 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 25, 
[2m[36m(pid=13132)[0m  train loss: 6.900865241885185
[2m[36m(pid=13132)[0m  eval loss: 8.596105728149414, eval err: 0.7483307337760925
[2m[36m(pid=13132)[0m 26, 
[2m[36m(pid=13132)[0m  train loss: 7.180384874343872
[2m[36m(pid=13132)[0m  eval loss: 7.162730369567871, eval err: 0.7098858571052551
[2m[36m(pid=13132)[0m 27, 
[2m[36m(pid=13132)[0m  train loss: 7.333011977374554
Result for train_b1c14_00000:
  date: 2021-09-09_10-14-37
  done: false
  err: 0.7447078227996826
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 7.9155178642272945
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 3124.6776995658875
  time_this_iter_s: 334.78304290771484
  time_total_s: 3124.6776995658875
  timestamp: 1631175277
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00000
  
[2m[36m(pid=13132)[0m  eval loss: 7.9155178642272945, eval err: 0.7447078227996826
== Status ==
Memory usage on this node: 23.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     10 |          3124.68 | 7.91552 | 0.744708 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 28, 
[2m[36m(pid=13132)[0m  train loss: 6.921327747404575
[2m[36m(pid=13132)[0m  eval loss: 7.0469973754882815, eval err: 0.6921234631538391
[2m[36m(pid=13132)[0m 29, 
[2m[36m(pid=13132)[0m  train loss: 6.208646893501282
[2m[36m(pid=13132)[0m  eval loss: 6.422502279281616, eval err: 0.6691216778755188
[2m[36m(pid=13132)[0m 30, 
[2m[36m(pid=13132)[0m  train loss: 6.619483858346939
[2m[36m(pid=13132)[0m  eval loss: 6.389851989746094, eval err: 0.6562115311622619
Result for train_b1c14_00000:
  date: 2021-09-09_10-20-11
  done: false
  err: 0.6562115311622619
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 6.389851989746094
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 3458.8264541625977
  time_this_iter_s: 334.1487545967102
  time_total_s: 3458.8264541625977
  timestamp: 1631175611
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     11 |          3458.83 | 6.38985 | 0.656212 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 31, 
[2m[36m(pid=13132)[0m  train loss: 5.944970607757568
[2m[36m(pid=13132)[0m  eval loss: 5.943853092193604, eval err: 0.6247432827949524
[2m[36m(pid=13132)[0m 32, 
[2m[36m(pid=13132)[0m  train loss: 5.486469462513924
[2m[36m(pid=13132)[0m  eval loss: 6.450814552307129, eval err: 0.640217456817627
[2m[36m(pid=13132)[0m 33, 
[2m[36m(pid=13132)[0m  train loss: 6.022330075502396
[2m[36m(pid=13132)[0m  eval loss: 4.704162769317627, eval err: 0.5300236821174622
Result for train_b1c14_00000:
  date: 2021-09-09_10-25-46
  done: false
  err: 0.5300236821174622
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 4.704162769317627
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 3792.9580430984497
  time_this_iter_s: 334.13158893585205
  time_total_s: 3792.9580430984497
  timestamp: 1631175946
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     12 |          3792.96 | 4.70416 | 0.530024 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 34, 
[2m[36m(pid=13132)[0m  train loss: 5.110136434435844
[2m[36m(pid=13132)[0m  eval loss: 5.191802473068237, eval err: 0.5338029980659484
[2m[36m(pid=13132)[0m 35, 
[2m[36m(pid=13132)[0m  train loss: 4.59163361787796
[2m[36m(pid=13132)[0m  eval loss: 4.123090410232544, eval err: 0.4562522292137146
[2m[36m(pid=13132)[0m 36, 
[2m[36m(pid=13132)[0m  train loss: 4.1236304342746735
[2m[36m(pid=13132)[0m  eval loss: 4.548859252929687, eval err: 0.45106329917907717
Result for train_b1c14_00000:
  date: 2021-09-09_10-31-20
  done: false
  err: 0.45106329917907717
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 4.548859252929687
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 4127.081387519836
  time_this_iter_s: 334.1233444213867
  time_total_s: 4127.081387519836
  timestamp: 1631176280
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     13 |          4127.08 | 4.54886 | 0.451063 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 37, 
[2m[36m(pid=13132)[0m  train loss: 4.259102880954742
[2m[36m(pid=13132)[0m  eval loss: 4.541480207443238, eval err: 0.40372485876083375
[2m[36m(pid=13132)[0m 38, 
[2m[36m(pid=13132)[0m  train loss: 3.6566015258431435
[2m[36m(pid=13132)[0m  eval loss: 3.989148988723755, eval err: 0.36591056823730467
[2m[36m(pid=13132)[0m 39, 
[2m[36m(pid=13132)[0m  train loss: 3.868297763168812
[2m[36m(pid=13132)[0m  eval loss: 4.097422285079956, eval err: 0.3609402060508728
Result for train_b1c14_00000:
  date: 2021-09-09_10-36-53
  done: false
  err: 0.3609402060508728
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 4.097422285079956
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 4460.596706390381
  time_this_iter_s: 333.51531887054443
  time_total_s: 4460.596706390381
  timestamp: 1631176613
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 23.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+---------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |     err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+---------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     14 |           4460.6 | 4.09742 | 0.36094 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |         |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |         |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |         |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |         |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |         |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |         |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |         |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |         |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |         |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |         |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |         |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |         |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |         |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |         |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |         |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |         |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |         |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |         |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |         |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+---------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 40, 
[2m[36m(pid=13132)[0m  train loss: 3.669654406607151
[2m[36m(pid=13132)[0m  eval loss: 3.563951406478882, eval err: 0.324293897151947
[2m[36m(pid=13132)[0m 41, 
[2m[36m(pid=13132)[0m  train loss: 3.3157313764095306
[2m[36m(pid=13132)[0m  eval loss: 2.492238450050354, eval err: 0.24043068885803223
[2m[36m(pid=13132)[0m 42, 
[2m[36m(pid=13132)[0m  train loss: 3.0997281596064568
[2m[36m(pid=13132)[0m  eval loss: 2.4158492851257325, eval err: 0.2371630573272705
Result for train_b1c14_00000:
  date: 2021-09-09_10-42-27
  done: false
  err: 0.2371630573272705
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 2.4158492851257325
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 4793.984593153
  time_this_iter_s: 333.387886762619
  time_total_s: 4793.984593153
  timestamp: 1631176947
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 24.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     15 |          4793.98 | 2.41585 | 0.237163 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 43, 
[2m[36m(pid=13132)[0m  train loss: 2.593116592615843
[2m[36m(pid=13132)[0m  eval loss: 2.283081512451172, eval err: 0.2145582365989685
[2m[36m(pid=13132)[0m 44, 
[2m[36m(pid=13132)[0m  train loss: 2.482877105474472
[2m[36m(pid=13132)[0m  eval loss: 2.099433422088623, eval err: 0.19641638517379761
[2m[36m(pid=13132)[0m 45, 
[2m[36m(pid=13132)[0m  train loss: 2.3792714290320873
[2m[36m(pid=13132)[0m  eval loss: 2.010348711013794, eval err: 0.18476515293121337
Result for train_b1c14_00000:
  date: 2021-09-09_10-48-00
  done: false
  err: 0.18476515293121337
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 2.010348711013794
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 5127.302640199661
  time_this_iter_s: 333.3180470466614
  time_total_s: 5127.302640199661
  timestamp: 1631177280
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     16 |           5127.3 | 2.01035 | 0.184765 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 46, 
[2m[36m(pid=13132)[0m  train loss: 2.4525256510823965
[2m[36m(pid=13132)[0m  eval loss: 1.7211792492866516, eval err: 0.16106730937957764
[2m[36m(pid=13132)[0m 47, 
[2m[36m(pid=13132)[0m  train loss: 2.2516547106206417
[2m[36m(pid=13132)[0m  eval loss: 1.8341976714134216, eval err: 0.16161861896514892
[2m[36m(pid=13132)[0m 48, 
[2m[36m(pid=13132)[0m  train loss: 2.1264519207179546
[2m[36m(pid=13132)[0m  eval loss: 1.5752487397193908, eval err: 0.14219204902648927
Result for train_b1c14_00000:
  date: 2021-09-09_10-53-33
  done: false
  err: 0.14219204902648927
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 1.5752487397193908
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 5460.952769994736
  time_this_iter_s: 333.65012979507446
  time_total_s: 5460.952769994736
  timestamp: 1631177613
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 24.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     17 |          5460.95 | 1.57525 | 0.142192 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 49, 
[2m[36m(pid=13132)[0m  train loss: 2.2340447828173637
[2m[36m(pid=13132)[0m  eval loss: 1.5214349961280822, eval err: 0.13596297979354857
[2m[36m(pid=13132)[0m 50, 
[2m[36m(pid=13132)[0m  train loss: 1.9936857875436544
[2m[36m(pid=13132)[0m  eval loss: 1.4224258875846862, eval err: 0.12186521291732788
[2m[36m(pid=13132)[0m 51, 
[2m[36m(pid=13132)[0m  train loss: 2.1066122949123383
[2m[36m(pid=13132)[0m  eval loss: 1.4394695329666138, eval err: 0.12551790952682496
Result for train_b1c14_00000:
  date: 2021-09-09_10-59-07
  done: false
  err: 0.12551790952682496
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 1.4394695329666138
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 5794.4345536231995
  time_this_iter_s: 333.48178362846375
  time_total_s: 5794.4345536231995
  timestamp: 1631177947
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 24.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     18 |          5794.43 | 1.43947 | 0.125518 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 52, 
[2m[36m(pid=13132)[0m  train loss: 2.0928167439997196
[2m[36m(pid=13132)[0m  eval loss: 1.3294237494468688, eval err: 0.11624092817306518
[2m[36m(pid=13132)[0m 53, 
[2m[36m(pid=13132)[0m  train loss: 1.841431686654687
[2m[36m(pid=13132)[0m  eval loss: 1.3901830744743346, eval err: 0.1168439507484436
[2m[36m(pid=13132)[0m 54, 
[2m[36m(pid=13132)[0m  train loss: 2.0775139331817627
[2m[36m(pid=13132)[0m  eval loss: 1.3211027693748474, eval err: 0.11082741975784302
Result for train_b1c14_00000:
  date: 2021-09-09_11-04-40
  done: false
  err: 0.11082741975784302
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 1.3211027693748474
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 6127.633341312408
  time_this_iter_s: 333.198787689209
  time_total_s: 6127.633341312408
  timestamp: 1631178280
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 24.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+--------+----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |   loss |      err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+--------+----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     19 |          6127.63 | 1.3211 | 0.110827 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |        |          |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |        |          |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |        |          |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |        |          |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |        |          |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |        |          |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |        |          |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |        |          |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |        |          |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |        |          |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |        |          |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |        |          |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |        |          |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |        |          |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |        |          |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |        |          |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |        |          |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |        |          |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |        |          |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+--------+----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 55, 
[2m[36m(pid=13132)[0m  train loss: 1.7641567774116993
[2m[36m(pid=13132)[0m  eval loss: 1.2643626713752747, eval err: 0.10370055437088013
[2m[36m(pid=13132)[0m 56, 
[2m[36m(pid=13132)[0m  train loss: 1.662064902484417
[2m[36m(pid=13132)[0m  eval loss: 1.2256925892829895, eval err: 0.10237741231918335
[2m[36m(pid=13132)[0m 57, 
[2m[36m(pid=13132)[0m  train loss: 1.790525572374463
[2m[36m(pid=13132)[0m  eval loss: 1.203709750175476, eval err: 0.0968836236000061
Result for train_b1c14_00000:
  date: 2021-09-09_11-10-14
  done: false
  err: 0.0968836236000061
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 1.203709750175476
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 6461.474345445633
  time_this_iter_s: 333.8410041332245
  time_total_s: 6461.474345445633
  timestamp: 1631178614
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 24.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     20 |          6461.47 | 1.20371 | 0.0968836 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 58, 
[2m[36m(pid=13132)[0m  train loss: 1.7784479521214962
[2m[36m(pid=13132)[0m  eval loss: 1.1810995221138, eval err: 0.09468826055526733
[2m[36m(pid=13132)[0m 59, 
[2m[36m(pid=13132)[0m  train loss: 1.7819800786674023
[2m[36m(pid=13132)[0m  eval loss: 1.1699592804908752, eval err: 0.09166475772857666
[2m[36m(pid=13132)[0m 60, 
[2m[36m(pid=13132)[0m  train loss: 1.7437714003026485
[2m[36m(pid=13132)[0m  eval loss: 1.146309766769409, eval err: 0.09064336061477661
Result for train_b1c14_00000:
  date: 2021-09-09_11-15-47
  done: false
  err: 0.09064336061477661
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 1.146309766769409
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 6794.946654319763
  time_this_iter_s: 333.47230887413025
  time_total_s: 6794.946654319763
  timestamp: 1631178947
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 24.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     21 |          6794.95 | 1.14631 | 0.0906434 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 61, 
[2m[36m(pid=13132)[0m  train loss: 1.6452417485415936
[2m[36m(pid=13132)[0m  eval loss: 1.1169777250289916, eval err: 0.08894317865371704
[2m[36m(pid=13132)[0m 62, 
[2m[36m(pid=13132)[0m  train loss: 1.6908230371773243
[2m[36m(pid=13132)[0m  eval loss: 1.1777870488166808, eval err: 0.09456495046615601
[2m[36m(pid=13132)[0m 63, 
[2m[36m(pid=13132)[0m  train loss: 1.6607900615781546
Result for train_b1c14_00000:
  date: 2021-09-09_11-21-21
  done: false
  err: 0.08441866397857666
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 22
  loss: 1.079282648563385
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 7128.872598648071
  time_this_iter_s: 333.9259443283081
  time_total_s: 7128.872598648071
  timestamp: 1631179281
  timesteps_since_restore: 0
  training_iteration: 22
  trial_id: b1c14_00000
  
[2m[36m(pid=13132)[0m  eval loss: 1.079282648563385, eval err: 0.08441866397857666
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     22 |          7128.87 | 1.07928 | 0.0844187 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 64, 
[2m[36m(pid=13132)[0m  train loss: 1.7163585275411606
[2m[36m(pid=13132)[0m  eval loss: 1.0904337573051452, eval err: 0.0859413766860962
[2m[36m(pid=13132)[0m 65, 
[2m[36m(pid=13132)[0m  train loss: 1.5130667462944984
[2m[36m(pid=13132)[0m  eval loss: 1.0591998553276063, eval err: 0.08495928764343262
[2m[36m(pid=13132)[0m 66, 
[2m[36m(pid=13132)[0m  train loss: 1.4904049765318632
Result for train_b1c14_00000:
  date: 2021-09-09_11-26-55
  done: false
  err: 0.08212239980697632
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 23
  loss: 1.0454243659973144
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 7462.608640432358
  time_this_iter_s: 333.7360417842865
  time_total_s: 7462.608640432358
  timestamp: 1631179615
  timesteps_since_restore: 0
  training_iteration: 23
  trial_id: b1c14_00000
  
[2m[36m(pid=13132)[0m  eval loss: 1.0454243659973144, eval err: 0.08212239980697632
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     23 |          7462.61 | 1.04542 | 0.0821224 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 67, 
[2m[36m(pid=13132)[0m  train loss: 1.5339967161417007
[2m[36m(pid=13132)[0m  eval loss: 1.0092745411396027, eval err: 0.07823768615722657
[2m[36m(pid=13132)[0m 68, 
[2m[36m(pid=13132)[0m  train loss: 1.594609947875142
[2m[36m(pid=13132)[0m  eval loss: 1.0273490524291993, eval err: 0.07846727848052978
[2m[36m(pid=13132)[0m 69, 
[2m[36m(pid=13132)[0m  train loss: 1.4778133016079664
[2m[36m(pid=13132)[0m  eval loss: 1.0409842562675475, eval err: 0.08052334785461426
Result for train_b1c14_00000:
  date: 2021-09-09_11-32-29
  done: false
  err: 0.08052334785461426
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 24
  loss: 1.0409842562675475
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 7796.3633761405945
  time_this_iter_s: 333.7547357082367
  time_total_s: 7796.3633761405945
  timestamp: 1631179949
  timesteps_since_restore: 0
  training_iteration: 24
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     24 |          7796.36 | 1.04098 | 0.0805233 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 70, 
[2m[36m(pid=13132)[0m  train loss: 1.5621758438646793
[2m[36m(pid=13132)[0m  eval loss: 1.0076669239997864, eval err: 0.07841050148010253
[2m[36m(pid=13132)[0m 71, 
[2m[36m(pid=13132)[0m  train loss: 1.4449368491768837
[2m[36m(pid=13132)[0m  eval loss: 0.988821439743042, eval err: 0.07597970008850098
[2m[36m(pid=13132)[0m 72, 
[2m[36m(pid=13132)[0m  train loss: 1.565696133300662
[2m[36m(pid=13132)[0m  eval loss: 1.0141161727905272, eval err: 0.07689047813415527
Result for train_b1c14_00000:
  date: 2021-09-09_11-38-03
  done: false
  err: 0.07689047813415527
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 25
  loss: 1.0141161727905272
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 8130.165965080261
  time_this_iter_s: 333.80258893966675
  time_total_s: 8130.165965080261
  timestamp: 1631180283
  timesteps_since_restore: 0
  training_iteration: 25
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 25.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |    loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     25 |          8130.17 | 1.01412 | 0.0768905 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |         |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |         |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |         |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |         |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |         |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |         |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |         |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |         |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |         |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |         |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |         |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |         |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |         |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |         |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |         |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |         |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |         |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |         |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |         |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+---------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 73, 
[2m[36m(pid=13132)[0m  train loss: 1.402339918538928
[2m[36m(pid=13132)[0m  eval loss: 0.985566520690918, eval err: 0.07411677598953247
[2m[36m(pid=13132)[0m 74, 
[2m[36m(pid=13132)[0m  train loss: 1.3104026857763529
[2m[36m(pid=13132)[0m  eval loss: 0.9733109223842621, eval err: 0.07347637414932251
[2m[36m(pid=13132)[0m 75, 
[2m[36m(pid=13132)[0m  train loss: 1.6465964391827583
[2m[36m(pid=13132)[0m  eval loss: 0.9904791510105133, eval err: 0.07507689237594604
Result for train_b1c14_00000:
  date: 2021-09-09_11-43-37
  done: false
  err: 0.07507689237594604
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 26
  loss: 0.9904791510105133
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 8464.089634180069
  time_this_iter_s: 333.92366909980774
  time_total_s: 8464.089634180069
  timestamp: 1631180617
  timesteps_since_restore: 0
  training_iteration: 26
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |     loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     26 |          8464.09 | 0.990479 | 0.0750769 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |          |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |          |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |          |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |          |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |          |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |          |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |          |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |          |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |          |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |          |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |          |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |          |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |          |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |          |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |          |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |          |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |          |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |          |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |          |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 76, 
[2m[36m(pid=13132)[0m  train loss: 1.4232729766517878
[2m[36m(pid=13132)[0m  eval loss: 0.9812555193901062, eval err: 0.07403444528579711
[2m[36m(pid=13132)[0m 77, 
[2m[36m(pid=13132)[0m  train loss: 1.3909378796815872
[2m[36m(pid=13132)[0m  eval loss: 0.9483229887485504, eval err: 0.07209230422973632
[2m[36m(pid=13132)[0m 78, 
[2m[36m(pid=13132)[0m  train loss: 1.2681515496224165
[2m[36m(pid=13132)[0m  eval loss: 0.943326905965805, eval err: 0.07150837659835815
Result for train_b1c14_00000:
  date: 2021-09-09_11-49-11
  done: false
  err: 0.07150837659835815
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 27
  loss: 0.943326905965805
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 8798.777361869812
  time_this_iter_s: 334.68772768974304
  time_total_s: 8798.777361869812
  timestamp: 1631180951
  timesteps_since_restore: 0
  training_iteration: 27
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |     loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     27 |          8798.78 | 0.943327 | 0.0715084 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |          |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |          |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |          |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |          |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |          |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |          |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |          |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |          |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |          |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |          |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |          |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |          |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |          |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |          |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |          |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |          |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |          |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |          |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |          |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 79, 
[2m[36m(pid=13132)[0m  train loss: 1.2948236353695393
[2m[36m(pid=13132)[0m  eval loss: 0.9273906338214875, eval err: 0.06916773080825805
[2m[36m(pid=13132)[0m 80, 
[2m[36m(pid=13132)[0m  train loss: 1.2645949218422174
[2m[36m(pid=13132)[0m  eval loss: 0.92713161110878, eval err: 0.06930992126464844
[2m[36m(pid=13132)[0m 81, 
[2m[36m(pid=13132)[0m  train loss: 1.2895820494741201
[2m[36m(pid=13132)[0m  eval loss: 0.9214539873600006, eval err: 0.06975986957550048
Result for train_b1c14_00000:
  date: 2021-09-09_11-54-45
  done: false
  err: 0.06975986957550048
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 28
  loss: 0.9214539873600006
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 9132.916188240051
  time_this_iter_s: 334.13882637023926
  time_total_s: 9132.916188240051
  timestamp: 1631181285
  timesteps_since_restore: 0
  training_iteration: 28
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 25.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |     loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     28 |          9132.92 | 0.921454 | 0.0697599 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |          |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |          |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |          |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |          |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |          |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |          |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |          |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |          |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |          |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |          |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |          |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |          |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |          |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |          |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |          |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |          |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |          |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |          |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |          |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 82, 
[2m[36m(pid=13132)[0m  train loss: 1.2612325139343739
[2m[36m(pid=13132)[0m  eval loss: 0.9072219371795655, eval err: 0.06793407917022705
[2m[36m(pid=13132)[0m 83, 
[2m[36m(pid=13132)[0m  train loss: 1.207614492624998
[2m[36m(pid=13132)[0m  eval loss: 0.9039285051822662, eval err: 0.06752860307693481
[2m[36m(pid=13132)[0m 84, 
[2m[36m(pid=13132)[0m  train loss: 1.1325864624232054
[2m[36m(pid=13132)[0m  eval loss: 0.9102188169956207, eval err: 0.0680636477470398
Result for train_b1c14_00000:
  date: 2021-09-09_12-00-20
  done: false
  err: 0.0680636477470398
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 29
  loss: 0.9102188169956207
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 9467.847505569458
  time_this_iter_s: 334.93131732940674
  time_total_s: 9467.847505569458
  timestamp: 1631181620
  timesteps_since_restore: 0
  training_iteration: 29
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 25.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |     loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     29 |          9467.85 | 0.910219 | 0.0680636 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |          |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |          |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |          |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |          |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |          |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |          |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |          |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |          |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |          |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |          |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |          |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |          |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |          |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |          |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |          |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |          |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |          |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |          |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |          |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13132)[0m 85, 
[2m[36m(pid=13132)[0m  train loss: 1.1702400371432304
[2m[36m(pid=13132)[0m  eval loss: 0.912948397397995, eval err: 0.06853638410568237
[2m[36m(pid=13132)[0m 86, 
[2m[36m(pid=13132)[0m  train loss: 1.3300200439989567
[2m[36m(pid=13132)[0m  eval loss: 0.9025929665565491, eval err: 0.06768081665039062
[2m[36m(pid=13132)[0m 87, 
[2m[36m(pid=13132)[0m  train loss: 1.1917901616543531
[2m[36m(pid=13132)[0m  eval loss: 0.9064109408855439, eval err: 0.0676943302154541
Result for train_b1c14_00000:
  date: 2021-09-09_12-05-55
  done: true
  err: 0.0676943302154541
  experiment_id: 0265ebb1281649e79ab571e2fd806c9b
  hostname: bigcuda4
  iterations_since_restore: 30
  loss: 0.9064109408855439
  node_ip: 131.220.7.54
  pid: 13132
  should_checkpoint: true
  time_since_restore: 9802.533772230148
  time_this_iter_s: 334.6862666606903
  time_total_s: 9802.533772230148
  timestamp: 1631181955
  timesteps_since_restore: 0
  training_iteration: 30
  trial_id: b1c14_00000
  
== Status ==
Memory usage on this node: 26.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (24 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
| Trial name        | status   | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |     loss |       err |
|-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------|
| train_b1c14_00000 | RUNNING  | 131.220.7.54:13132 |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 | 0.906411 | 0.0676943 |
| train_b1c14_00001 | PENDING  |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |        |                  |          |           |
| train_b1c14_00002 | PENDING  |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |          |           |
| train_b1c14_00003 | PENDING  |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |          |           |
| train_b1c14_00004 | PENDING  |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |          |           |
| train_b1c14_00005 | PENDING  |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |          |           |
| train_b1c14_00006 | PENDING  |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |          |           |
| train_b1c14_00007 | PENDING  |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |          |           |
| train_b1c14_00008 | PENDING  |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |          |           |
| train_b1c14_00009 | PENDING  |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |          |           |
| train_b1c14_00010 | PENDING  |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |          |           |
| train_b1c14_00011 | PENDING  |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |          |           |
| train_b1c14_00012 | PENDING  |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |          |           |
| train_b1c14_00013 | PENDING  |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |          |           |
| train_b1c14_00014 | PENDING  |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |          |           |
| train_b1c14_00015 | PENDING  |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |          |           |
| train_b1c14_00016 | PENDING  |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |          |           |
| train_b1c14_00017 | PENDING  |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |          |           |
| train_b1c14_00018 | PENDING  |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |          |           |
| train_b1c14_00019 | PENDING  |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |          |           |
+-------------------+----------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m Epoch:
[2m[36m(pid=13138)[0m 0, 
[2m[36m(pid=13138)[0m  train loss: 28.53168874979019
[2m[36m(pid=13138)[0m  eval loss: 117.76420532226562, eval err: 0.9969483828544616
Result for train_b1c14_00001:
  date: 2021-09-09_12-07-48
  done: false
  err: 0.9969483828544616
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 117.76420532226562
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 110.65235996246338
  time_this_iter_s: 110.65235996246338
  time_total_s: 110.65235996246338
  timestamp: 1631182068
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |      1 |          110.652 | 117.764    | 0.996948  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |            |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |            |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |            |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |            |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |            |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |            |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |            |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |            |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |            |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |            |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |            |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 1, 
[2m[36m(pid=13138)[0m  train loss: 27.40804874897003
[2m[36m(pid=13138)[0m  eval loss: 113.41638610839844, eval err: 0.9953855705261231
[2m[36m(pid=13138)[0m 2, 
[2m[36m(pid=13138)[0m  train loss: 20.468175530433655
[2m[36m(pid=13138)[0m  eval loss: 97.10633255004883, eval err: 0.9933581209182739
[2m[36m(pid=13138)[0m 3, 
[2m[36m(pid=13138)[0m  train loss: 14.35688516497612
[2m[36m(pid=13138)[0m  eval loss: 37.80296989440918, eval err: 0.9428443455696106
Result for train_b1c14_00001:
  date: 2021-09-09_12-13-12
  done: false
  err: 0.9428443455696106
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 37.80296989440918
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 434.59597420692444
  time_this_iter_s: 323.94361424446106
  time_total_s: 434.59597420692444
  timestamp: 1631182392
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |      2 |          434.596 | 37.803    | 0.942844  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 4, 
[2m[36m(pid=13138)[0m  train loss: 13.227368593215942
[2m[36m(pid=13138)[0m  eval loss: 29.035215072631836, eval err: 0.9448412561416626
[2m[36m(pid=13138)[0m 5, 
[2m[36m(pid=13138)[0m  train loss: 11.98219096660614
[2m[36m(pid=13138)[0m  eval loss: 29.659614944458006, eval err: 0.953244993686676
[2m[36m(pid=13138)[0m 6, 
[2m[36m(pid=13138)[0m  train loss: 11.451997369527817
[2m[36m(pid=13138)[0m  eval loss: 19.91108699798584, eval err: 0.9175205326080322
Result for train_b1c14_00001:
  date: 2021-09-09_12-18-35
  done: false
  err: 0.9175205326080322
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 19.91108699798584
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 758.0587294101715
  time_this_iter_s: 323.46275520324707
  time_total_s: 758.0587294101715
  timestamp: 1631182715
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |      3 |          758.059 | 19.9111   | 0.917521  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 7, 
[2m[36m(pid=13138)[0m  train loss: 11.8283112347126
[2m[36m(pid=13138)[0m  eval loss: 19.419539794921874, eval err: 0.909017448425293
[2m[36m(pid=13138)[0m 8, 
[2m[36m(pid=13138)[0m  train loss: 11.697336554527283
[2m[36m(pid=13138)[0m  eval loss: 33.375486297607424, eval err: 0.968237361907959
[2m[36m(pid=13138)[0m 9, 
[2m[36m(pid=13138)[0m  train loss: 11.218269526958466
[2m[36m(pid=13138)[0m  eval loss: 22.768558502197266, eval err: 0.9406158471107483
Result for train_b1c14_00001:
  date: 2021-09-09_12-23-58
  done: false
  err: 0.9406158471107483
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 22.768558502197266
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 1080.9733731746674
  time_this_iter_s: 322.91464376449585
  time_total_s: 1080.9733731746674
  timestamp: 1631183038
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |      4 |          1080.97 | 22.7686   | 0.940616  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 10, 
[2m[36m(pid=13138)[0m  train loss: 10.735224530100822
[2m[36m(pid=13138)[0m  eval loss: 34.13185554504395, eval err: 0.9652524757385254
[2m[36m(pid=13138)[0m 11, 
[2m[36m(pid=13138)[0m  train loss: 10.687318921089172
[2m[36m(pid=13138)[0m  eval loss: 31.325117568969727, eval err: 0.9675704002380371
[2m[36m(pid=13138)[0m 12, 
[2m[36m(pid=13138)[0m  train loss: 10.990015774965286
Result for train_b1c14_00001:
  date: 2021-09-09_12-29-21
  done: false
  err: 0.8975599908828735
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 18.869909324645995
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 1404.2600152492523
  time_this_iter_s: 323.28664207458496
  time_total_s: 1404.2600152492523
  timestamp: 1631183361
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |      5 |          1404.26 | 18.8699   | 0.89756   |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m  eval loss: 18.869909324645995, eval err: 0.8975599908828735
[2m[36m(pid=13138)[0m 13, 
[2m[36m(pid=13138)[0m  train loss: 10.283849954605103
[2m[36m(pid=13138)[0m  eval loss: 16.372551918029785, eval err: 0.8711402034759521
[2m[36m(pid=13138)[0m 14, 
[2m[36m(pid=13138)[0m  train loss: 10.528831034898758
[2m[36m(pid=13138)[0m  eval loss: 28.148583602905273, eval err: 0.9576641678810119
[2m[36m(pid=13138)[0m 15, 
[2m[36m(pid=13138)[0m  train loss: 10.439511358737946
Result for train_b1c14_00001:
  date: 2021-09-09_12-34-45
  done: false
  err: 0.9635172724723816
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 31.91938903808594
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 1727.8841960430145
  time_this_iter_s: 323.6241807937622
  time_total_s: 1727.8841960430145
  timestamp: 1631183685
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00001
  
[2m[36m(pid=13138)[0m  eval loss: 31.91938903808594, eval err: 0.9635172724723816
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |      6 |          1727.88 | 31.9194   | 0.963517  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 16, 
[2m[36m(pid=13138)[0m  train loss: 10.997338324785233
[2m[36m(pid=13138)[0m  eval loss: 18.101177368164063, eval err: 0.887600121498108
[2m[36m(pid=13138)[0m 17, 
[2m[36m(pid=13138)[0m  train loss: 10.155234411358833
[2m[36m(pid=13138)[0m  eval loss: 20.297981491088866, eval err: 0.930691909790039
[2m[36m(pid=13138)[0m 18, 
[2m[36m(pid=13138)[0m  train loss: 10.169131740927696
[2m[36m(pid=13138)[0m  eval loss: 24.142780227661135, eval err: 0.953877682685852
Result for train_b1c14_00001:
  date: 2021-09-09_12-40-09
  done: false
  err: 0.953877682685852
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 24.142780227661135
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 2051.9211707115173
  time_this_iter_s: 324.0369746685028
  time_total_s: 2051.9211707115173
  timestamp: 1631184009
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |      7 |          2051.92 | 24.1428   | 0.953878  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 19, 
[2m[36m(pid=13138)[0m  train loss: 10.786377489566803
[2m[36m(pid=13138)[0m  eval loss: 28.75603202819824, eval err: 0.9670705723762513
[2m[36m(pid=13138)[0m 20, 
[2m[36m(pid=13138)[0m  train loss: 10.878451600670815
[2m[36m(pid=13138)[0m  eval loss: 22.51337661743164, eval err: 0.9406579256057739
[2m[36m(pid=13138)[0m 21, 
[2m[36m(pid=13138)[0m  train loss: 10.059025794267654
Result for train_b1c14_00001:
  date: 2021-09-09_12-45-33
  done: false
  err: 0.9440427374839783
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 24.652437591552733
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 2375.7418162822723
  time_this_iter_s: 323.820645570755
  time_total_s: 2375.7418162822723
  timestamp: 1631184333
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00001
  
[2m[36m(pid=13138)[0m  eval loss: 24.652437591552733, eval err: 0.9440427374839783
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |      8 |          2375.74 | 24.6524   | 0.944043  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 22, 
[2m[36m(pid=13138)[0m  train loss: 9.607308492064476
[2m[36m(pid=13138)[0m  eval loss: 30.444214782714845, eval err: 0.9662376427650452
[2m[36m(pid=13138)[0m 23, 
[2m[36m(pid=13138)[0m  train loss: 10.307339891791344
[2m[36m(pid=13138)[0m  eval loss: 17.563864288330077, eval err: 0.8976536083221436
[2m[36m(pid=13138)[0m 24, 
[2m[36m(pid=13138)[0m  train loss: 10.482393220067024
[2m[36m(pid=13138)[0m  eval loss: 24.751649322509767, eval err: 0.9473799657821655
Result for train_b1c14_00001:
  date: 2021-09-09_12-50-56
  done: false
  err: 0.9473799657821655
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 24.751649322509767
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 2699.2352681159973
  time_this_iter_s: 323.493451833725
  time_total_s: 2699.2352681159973
  timestamp: 1631184656
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |      9 |          2699.24 | 24.7516   | 0.94738   |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 25, 
[2m[36m(pid=13138)[0m  train loss: 10.207272693514824
[2m[36m(pid=13138)[0m  eval loss: 19.714928703308104, eval err: 0.919196674823761
[2m[36m(pid=13138)[0m 26, 
[2m[36m(pid=13138)[0m  train loss: 10.173679023981094
[2m[36m(pid=13138)[0m  eval loss: 21.865704650878907, eval err: 0.9207677006721496
[2m[36m(pid=13138)[0m 27, 
[2m[36m(pid=13138)[0m  train loss: 10.10896460711956
Result for train_b1c14_00001:
  date: 2021-09-09_12-56-19
  done: false
  err: 0.8799003100395203
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 15.902966384887696
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 3022.0084342956543
  time_this_iter_s: 322.773166179657
  time_total_s: 3022.0084342956543
  timestamp: 1631184979
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00001
  
[2m[36m(pid=13138)[0m  eval loss: 15.902966384887696, eval err: 0.8799003100395203
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     10 |          3022.01 | 15.903    | 0.8799    |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 28, 
[2m[36m(pid=13138)[0m  train loss: 10.372225865721703
[2m[36m(pid=13138)[0m  eval loss: 13.851877136230469, eval err: 0.8660949039459228
[2m[36m(pid=13138)[0m 29, 
[2m[36m(pid=13138)[0m  train loss: 9.84369507431984
[2m[36m(pid=13138)[0m  eval loss: 16.781948318481444, eval err: 0.8816207957267761
[2m[36m(pid=13138)[0m 30, 
[2m[36m(pid=13138)[0m  train loss: 9.80377821624279
[2m[36m(pid=13138)[0m  eval loss: 12.51304058074951, eval err: 0.8553769969940186
Result for train_b1c14_00001:
  date: 2021-09-09_13-01-42
  done: false
  err: 0.8553769969940186
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 12.51304058074951
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 3344.8724296092987
  time_this_iter_s: 322.8639953136444
  time_total_s: 3344.8724296092987
  timestamp: 1631185302
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     11 |          3344.87 | 12.513    | 0.855377  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 31, 
[2m[36m(pid=13138)[0m  train loss: 10.399843081831932
[2m[36m(pid=13138)[0m  eval loss: 14.651458473205567, eval err: 0.8725999641418457
[2m[36m(pid=13138)[0m 32, 
[2m[36m(pid=13138)[0m  train loss: 10.139193743467331
[2m[36m(pid=13138)[0m  eval loss: 22.14958724975586, eval err: 0.9282181572914123
[2m[36m(pid=13138)[0m 33, 
[2m[36m(pid=13138)[0m  train loss: 9.964468002319336
[2m[36m(pid=13138)[0m  eval loss: 15.616769485473633, eval err: 0.8680481076240539
Result for train_b1c14_00001:
  date: 2021-09-09_13-07-06
  done: false
  err: 0.8680481076240539
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 15.616769485473633
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 3668.6754422187805
  time_this_iter_s: 323.8030126094818
  time_total_s: 3668.6754422187805
  timestamp: 1631185626
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     12 |          3668.68 | 15.6168   | 0.868048  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 34, 
[2m[36m(pid=13138)[0m  train loss: 10.678148657083511
[2m[36m(pid=13138)[0m  eval loss: 13.422377548217774, eval err: 0.8598197603225708
[2m[36m(pid=13138)[0m 35, 
[2m[36m(pid=13138)[0m  train loss: 9.645074605941772
[2m[36m(pid=13138)[0m  eval loss: 22.91262550354004, eval err: 0.9342811298370362
[2m[36m(pid=13138)[0m 36, 
[2m[36m(pid=13138)[0m  train loss: 9.645260870456696
Result for train_b1c14_00001:
  date: 2021-09-09_13-12-29
  done: false
  err: 0.8812869906425476
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 13.70656852722168
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 3991.628664970398
  time_this_iter_s: 322.95322275161743
  time_total_s: 3991.628664970398
  timestamp: 1631185949
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00001
  
[2m[36m(pid=13138)[0m  eval loss: 13.70656852722168, eval err: 0.8812869906425476
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     13 |          3991.63 | 13.7066   | 0.881287  |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13138)[0m 37, 
[2m[36m(pid=13138)[0m  train loss: 9.4259003251791
[2m[36m(pid=13138)[0m  eval loss: 18.98073360443115, eval err: 0.9216225957870483
[2m[36m(pid=13138)[0m 38, 
[2m[36m(pid=13138)[0m  train loss: 9.235671266913414
[2m[36m(pid=13138)[0m  eval loss: 15.308829498291015, eval err: 0.8843671798706054
[2m[36m(pid=13138)[0m 39, 
[2m[36m(pid=13138)[0m  train loss: 9.743798106908798
[2m[36m(pid=13138)[0m  eval loss: 14.127507972717286, eval err: 0.8575198245048523
Result for train_b1c14_00001:
  date: 2021-09-09_13-17-51
  done: true
  err: 0.8575198245048523
  experiment_id: f94ce42069684efcb5402af7f0dd0f50
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 14.127507972717286
  node_ip: 131.220.7.54
  pid: 13138
  should_checkpoint: true
  time_since_restore: 4314.169899702072
  time_this_iter_s: 322.5412347316742
  time_total_s: 4314.169899702072
  timestamp: 1631186271
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00001
  
== Status ==
Memory usage on this node: 26.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (23 PENDING, 1 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00001 | RUNNING    | 131.220.7.54:13138 |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)


[2m[36m(pid=13140)[0m Epoch:
[2m[36m(pid=13140)[0m 0, 
Result for train_b1c14_00002:
  {}
  
== Status ==
Memory usage on this node: 26.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 ERROR, 22 PENDING, 2 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00003 | PENDING    |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 1
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13131)[0m Epoch:
[2m[36m(pid=13131)[0m 0, 
Result for train_b1c14_00003:
  {}
  
== Status ==
Memory usage on this node: 25.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 ERROR, 21 PENDING, 2 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00004 | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 2
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13135)[0m Epoch:
[2m[36m(pid=13135)[0m 0, 
Result for train_b1c14_00004:
  {}
  
== Status ==
Memory usage on this node: 26.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 ERROR, 20 PENDING, 2 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00005 | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00006 | PENDING    |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 3
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13125)[0m Epoch:
[2m[36m(pid=13125)[0m 0, 
Result for train_b1c14_00005:
  {}
  
== Status ==
Memory usage on this node: 25.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 19 PENDING, 2 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | PENDING    |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |        |                  |           |           |
| train_b1c14_00007 | PENDING    |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m Epoch:
[2m[36m(pid=13139)[0m 0, 
[2m[36m(pid=13139)[0m  train loss: 35.254825949668884
[2m[36m(pid=13139)[0m  eval loss: 118.22102081298829, eval err: 0.9998982524871827
Result for train_b1c14_00006:
  date: 2021-09-09_13-20-24
  done: false
  err: 0.9998982524871827
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 118.22102081298829
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 112.03961968421936
  time_this_iter_s: 112.03961968421936
  time_total_s: 112.03961968421936
  timestamp: 1631186424
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 25.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |      1 |           112.04 | 118.221    | 0.999898  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |            |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |            |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |            |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |            |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |            |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |            |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |            |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |            |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |            |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |            |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |   0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 |  14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 1, 
[2m[36m(pid=13139)[0m  train loss: 20.378124594688416
[2m[36m(pid=13139)[0m  eval loss: 91.9246841430664, eval err: 0.9978459739685058
[2m[36m(pid=13139)[0m 2, 
[2m[36m(pid=13139)[0m  train loss: 12.86637195944786
[2m[36m(pid=13139)[0m  eval loss: 18.94440029144287, eval err: 0.8958335208892823
[2m[36m(pid=13139)[0m 3, 
[2m[36m(pid=13139)[0m  train loss: 12.64374428987503
[2m[36m(pid=13139)[0m  eval loss: 18.58610118865967, eval err: 0.909531455039978
Result for train_b1c14_00006:
  date: 2021-09-09_13-25-50
  done: false
  err: 0.909531455039978
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 18.58610118865967
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 438.0316569805145
  time_this_iter_s: 325.99203729629517
  time_total_s: 438.0316569805145
  timestamp: 1631186750
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 14.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |      2 |          438.032 | 18.5861   | 0.909531  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 4, 
[2m[36m(pid=13139)[0m  train loss: 10.990473538637161
[2m[36m(pid=13139)[0m  eval loss: 46.77391372680664, eval err: 0.983134753704071
[2m[36m(pid=13139)[0m 5, 
[2m[36m(pid=13139)[0m  train loss: 12.256051748991013
[2m[36m(pid=13139)[0m  eval loss: 17.746449089050294, eval err: 0.8904907011985779
[2m[36m(pid=13139)[0m 6, 
[2m[36m(pid=13139)[0m  train loss: 10.949036091566086
Result for train_b1c14_00006:
  date: 2021-09-09_13-31-10
  done: false
  err: 0.8834451651573181
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 15.397129592895508
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 757.7566957473755
  time_this_iter_s: 319.72503876686096
  time_total_s: 757.7566957473755
  timestamp: 1631187070
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00006
  
[2m[36m(pid=13139)[0m  eval loss: 15.397129592895508, eval err: 0.8834451651573181
== Status ==
Memory usage on this node: 14.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |      3 |          757.757 | 15.3971   | 0.883445  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 7, 
[2m[36m(pid=13139)[0m  train loss: 10.936568826436996
[2m[36m(pid=13139)[0m  eval loss: 11.423100128173829, eval err: 0.8472158765792847
[2m[36m(pid=13139)[0m 8, 
[2m[36m(pid=13139)[0m  train loss: 11.651148647069931
[2m[36m(pid=13139)[0m  eval loss: 15.685950355529785, eval err: 0.8867600679397583
[2m[36m(pid=13139)[0m 9, 
[2m[36m(pid=13139)[0m  train loss: 10.31063587963581
[2m[36m(pid=13139)[0m  eval loss: 37.12671051025391, eval err: 0.9877492117881775
Result for train_b1c14_00006:
  date: 2021-09-09_13-36-29
  done: false
  err: 0.9877492117881775
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 37.12671051025391
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 1077.0638930797577
  time_this_iter_s: 319.3071973323822
  time_total_s: 1077.0638930797577
  timestamp: 1631187389
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 14.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |      4 |          1077.06 | 37.1267   | 0.987749  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 10, 
[2m[36m(pid=13139)[0m  train loss: 9.607264369726181
[2m[36m(pid=13139)[0m  eval loss: 12.047370491027833, eval err: 0.8820634412765503
[2m[36m(pid=13139)[0m 11, 
[2m[36m(pid=13139)[0m  train loss: 9.992750644683838
[2m[36m(pid=13139)[0m  eval loss: 11.480259017944336, eval err: 0.8617624497413635
[2m[36m(pid=13139)[0m 12, 
[2m[36m(pid=13139)[0m  train loss: 10.074604466557503
[2m[36m(pid=13139)[0m  eval loss: 10.627307662963867, eval err: 0.8595369100570679
Result for train_b1c14_00006:
  date: 2021-09-09_13-41-49
  done: false
  err: 0.8595369100570679
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 10.627307662963867
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 1397.0561289787292
  time_this_iter_s: 319.99223589897156
  time_total_s: 1397.0561289787292
  timestamp: 1631187709
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 14.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |      5 |          1397.06 | 10.6273   | 0.859537  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 13, 
[2m[36m(pid=13139)[0m  train loss: 10.304177016019821
[2m[36m(pid=13139)[0m  eval loss: 14.555641860961915, eval err: 0.8826647138595581
[2m[36m(pid=13139)[0m 14, 
[2m[36m(pid=13139)[0m  train loss: 9.618788212537766
[2m[36m(pid=13139)[0m  eval loss: 11.28587303161621, eval err: 0.871472749710083
[2m[36m(pid=13139)[0m 15, 
[2m[36m(pid=13139)[0m  train loss: 9.429823964834213
[2m[36m(pid=13139)[0m  eval loss: 13.99993522644043, eval err: 0.8610106158256531
Result for train_b1c14_00006:
  date: 2021-09-09_13-47-09
  done: false
  err: 0.8610106158256531
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 13.99993522644043
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 1717.0364210605621
  time_this_iter_s: 319.9802920818329
  time_total_s: 1717.0364210605621
  timestamp: 1631188029
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 14.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |      6 |          1717.04 | 13.9999   | 0.861011  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 16, 
[2m[36m(pid=13139)[0m  train loss: 10.129159286618233
[2m[36m(pid=13139)[0m  eval loss: 13.9958540725708, eval err: 0.8551774334907531
[2m[36m(pid=13139)[0m 17, 
[2m[36m(pid=13139)[0m  train loss: 9.260298520326614
[2m[36m(pid=13139)[0m  eval loss: 11.448355293273925, eval err: 0.848405408859253
[2m[36m(pid=13139)[0m 18, 
[2m[36m(pid=13139)[0m  train loss: 9.473615437746048
[2m[36m(pid=13139)[0m  eval loss: 12.863797607421875, eval err: 0.8482431221008301
Result for train_b1c14_00006:
  date: 2021-09-09_13-52-29
  done: false
  err: 0.8482431221008301
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 12.863797607421875
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 2036.8319709300995
  time_this_iter_s: 319.79554986953735
  time_total_s: 2036.8319709300995
  timestamp: 1631188349
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 14.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |      7 |          2036.83 | 12.8638   | 0.848243  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 19, 
[2m[36m(pid=13139)[0m  train loss: 8.837010696530342
[2m[36m(pid=13139)[0m  eval loss: 10.905588073730469, eval err: 0.8611136317253113
[2m[36m(pid=13139)[0m 20, 
[2m[36m(pid=13139)[0m  train loss: 9.515149384737015
[2m[36m(pid=13139)[0m  eval loss: 11.450938186645509, eval err: 0.8591546106338501
[2m[36m(pid=13139)[0m 21, 
[2m[36m(pid=13139)[0m  train loss: 9.30223274230957
[2m[36m(pid=13139)[0m  eval loss: 12.724534454345703, eval err: 0.8428577327728272
Result for train_b1c14_00006:
  date: 2021-09-09_13-57-49
  done: false
  err: 0.8428577327728272
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 12.724534454345703
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 2356.289986371994
  time_this_iter_s: 319.45801544189453
  time_total_s: 2356.289986371994
  timestamp: 1631188669
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 14.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |      8 |          2356.29 | 12.7245   | 0.842858  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 22, 
[2m[36m(pid=13139)[0m  train loss: 10.12972278892994
[2m[36m(pid=13139)[0m  eval loss: 11.218698425292969, eval err: 0.8427724146842956
[2m[36m(pid=13139)[0m 23, 
[2m[36m(pid=13139)[0m  train loss: 9.615414246916771
[2m[36m(pid=13139)[0m  eval loss: 10.978673706054687, eval err: 0.8413597965240478
[2m[36m(pid=13139)[0m 24, 
[2m[36m(pid=13139)[0m  train loss: 9.94391480088234
Result for train_b1c14_00006:
  date: 2021-09-09_14-03-08
  done: false
  err: 0.8458940839767456
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 10.968833465576171
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 2675.254721879959
  time_this_iter_s: 318.9647355079651
  time_total_s: 2675.254721879959
  timestamp: 1631188988
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00006
  
[2m[36m(pid=13139)[0m  eval loss: 10.968833465576171, eval err: 0.8458940839767456
== Status ==
Memory usage on this node: 14.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |      9 |          2675.25 | 10.9688   | 0.845894  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 25, 
[2m[36m(pid=13139)[0m  train loss: 9.859127625823021
[2m[36m(pid=13139)[0m  eval loss: 11.88135871887207, eval err: 0.842137155532837
[2m[36m(pid=13139)[0m 26, 
[2m[36m(pid=13139)[0m  train loss: 8.829700842499733
[2m[36m(pid=13139)[0m  eval loss: 11.359054565429688, eval err: 0.8361631679534912
[2m[36m(pid=13139)[0m 27, 
[2m[36m(pid=13139)[0m  train loss: 8.998440980911255
[2m[36m(pid=13139)[0m  eval loss: 13.267139129638672, eval err: 0.8487829184532165
Result for train_b1c14_00006:
  date: 2021-09-09_14-08-27
  done: false
  err: 0.8487829184532165
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 13.267139129638672
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 2994.3794791698456
  time_this_iter_s: 319.1247572898865
  time_total_s: 2994.3794791698456
  timestamp: 1631189307
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 14.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     10 |          2994.38 | 13.2671   | 0.848783  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 28, 
[2m[36m(pid=13139)[0m  train loss: 8.767589032649994
[2m[36m(pid=13139)[0m  eval loss: 12.117945709228515, eval err: 0.8360785007476806
[2m[36m(pid=13139)[0m 29, 
[2m[36m(pid=13139)[0m  train loss: 8.994087472558022
[2m[36m(pid=13139)[0m  eval loss: 12.987811241149902, eval err: 0.8484241414070129
[2m[36m(pid=13139)[0m 30, 
[2m[36m(pid=13139)[0m  train loss: 9.42655748128891
[2m[36m(pid=13139)[0m  eval loss: 11.09234401702881, eval err: 0.8439992117881775
Result for train_b1c14_00006:
  date: 2021-09-09_14-13-46
  done: false
  err: 0.8439992117881775
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 11.09234401702881
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 3313.8046641349792
  time_this_iter_s: 319.42518496513367
  time_total_s: 3313.8046641349792
  timestamp: 1631189626
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 14.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     11 |          3313.8  | 11.0923   | 0.843999  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 31, 
[2m[36m(pid=13139)[0m  train loss: 8.67925076186657
[2m[36m(pid=13139)[0m  eval loss: 11.813138999938964, eval err: 0.845421495437622
[2m[36m(pid=13139)[0m 32, 
[2m[36m(pid=13139)[0m  train loss: 8.916619598865509
[2m[36m(pid=13139)[0m  eval loss: 12.242449836730957, eval err: 0.8385731983184814
[2m[36m(pid=13139)[0m 33, 
[2m[36m(pid=13139)[0m  train loss: 8.562575846910477
Result for train_b1c14_00006:
  date: 2021-09-09_14-19-08
  done: false
  err: 0.841789493560791
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 10.485961380004882
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 3635.695559024811
  time_this_iter_s: 321.89089488983154
  time_total_s: 3635.695559024811
  timestamp: 1631189948
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     12 |          3635.7  | 10.486    | 0.841789  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m  eval loss: 10.485961380004882, eval err: 0.841789493560791
[2m[36m(pid=13139)[0m 34, 
[2m[36m(pid=13139)[0m  train loss: 8.784413956105709
[2m[36m(pid=13139)[0m  eval loss: 12.048808937072755, eval err: 0.8282961273193359
[2m[36m(pid=13139)[0m 35, 
[2m[36m(pid=13139)[0m  train loss: 8.681035444140434
[2m[36m(pid=13139)[0m  eval loss: 11.81159107208252, eval err: 0.83228506565094
[2m[36m(pid=13139)[0m 36, 
[2m[36m(pid=13139)[0m  train loss: 8.579431563615799
[2m[36m(pid=13139)[0m  eval loss: 11.631233329772948, eval err: 0.8309227633476257
Result for train_b1c14_00006:
  date: 2021-09-09_14-24-31
  done: false
  err: 0.8309227633476257
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 11.631233329772948
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 3959.09326338768
  time_this_iter_s: 323.39770436286926
  time_total_s: 3959.09326338768
  timestamp: 1631190271
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00006
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     13 |          3959.09 | 11.6312   | 0.830923  |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13139)[0m 37, 
[2m[36m(pid=13139)[0m  train loss: 7.914706692099571
[2m[36m(pid=13139)[0m  eval loss: 10.978127365112305, eval err: 0.8592198967933655
[2m[36m(pid=13139)[0m 38, 
[2m[36m(pid=13139)[0m  train loss: 8.61579743027687
[2m[36m(pid=13139)[0m  eval loss: 10.908937950134277, eval err: 0.8468244481086731
[2m[36m(pid=13139)[0m 39, 
[2m[36m(pid=13139)[0m  train loss: 8.474561139941216
Result for train_b1c14_00006:
  date: 2021-09-09_14-29-54
  done: true
  err: 0.8410897326469421
  experiment_id: 34b4e945c35a4563835ddc32c6b98093
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 10.859393005371095
  node_ip: 131.220.7.54
  pid: 13139
  should_checkpoint: true
  time_since_restore: 4282.0261108875275
  time_this_iter_s: 322.9328474998474
  time_total_s: 4282.0261108875275
  timestamp: 1631190594
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00006
  
[2m[36m(pid=13139)[0m  eval loss: 10.859393005371095, eval err: 0.8410897326469421
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 18 PENDING, 1 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00006 | RUNNING    | 131.220.7.54:13139 |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00007 | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 4
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13137)[0m Epoch:
[2m[36m(pid=13137)[0m 0, 
Result for train_b1c14_00007:
  {}
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 ERROR, 17 PENDING, 3 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00008 | PENDING    |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 5
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt  |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13141)[0m Epoch:
[2m[36m(pid=13141)[0m 0, 
Result for train_b1c14_00008:
  {}
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 ERROR, 16 PENDING, 3 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00009 | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | PENDING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 6
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                             |
|-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt  |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt   |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt  |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt |
+-------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13143)[0m Epoch:
[2m[36m(pid=13143)[0m 0, 
Result for train_b1c14_00009:
  {}
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 ERROR, 15 PENDING, 3 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00010 | PENDING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 7
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13142)[0m Epoch:
[2m[36m(pid=13142)[0m 0, 
Result for train_b1c14_00010:
  {}
  
== Status ==
Memory usage on this node: 16.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 ERROR, 14 PENDING, 3 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00011 | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00012 | PENDING    |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 PENDING)
Number of errored trials: 8
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13129)[0m Epoch:
[2m[36m(pid=13129)[0m 0, 
Result for train_b1c14_00011:
  {}
  
== Status ==
Memory usage on this node: 16.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 13 PENDING, 3 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | PENDING    |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |        |                  |           |           |
| train_b1c14_00013 | PENDING    |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m Epoch:
[2m[36m(pid=13136)[0m 0, 
[2m[36m(pid=13136)[0m  train loss: 29.893816590309143
[2m[36m(pid=13136)[0m  eval loss: 90.7231524658203, eval err: 0.9839991688728332
Result for train_b1c14_00012:
  date: 2021-09-09_14-32-27
  done: false
  err: 0.9839991688728332
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 90.7231524658203
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 101.80287051200867
  time_this_iter_s: 101.80287051200867
  time_total_s: 101.80287051200867
  timestamp: 1631190747
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 16.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |      1 |          101.803 | 90.7232   | 0.983999  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 1, 
[2m[36m(pid=13136)[0m  train loss: 20.106884598731995
[2m[36m(pid=13136)[0m  eval loss: 25.180651321411133, eval err: 0.9133609485626221
[2m[36m(pid=13136)[0m 2, 
[2m[36m(pid=13136)[0m  train loss: 12.338519185781479
[2m[36m(pid=13136)[0m  eval loss: 20.02687225341797, eval err: 0.909681339263916
[2m[36m(pid=13136)[0m 3, 
[2m[36m(pid=13136)[0m  train loss: 10.763293161988258
[2m[36m(pid=13136)[0m  eval loss: 14.364789085388184, eval err: 0.8734896612167359
Result for train_b1c14_00012:
  date: 2021-09-09_14-37-23
  done: false
  err: 0.8734896612167359
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 14.364789085388184
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 397.8299517631531
  time_this_iter_s: 296.0270812511444
  time_total_s: 397.8299517631531
  timestamp: 1631191043
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 16.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |      2 |           397.83 | 14.3648   | 0.87349   |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 4, 
[2m[36m(pid=13136)[0m  train loss: 9.714397102594376
[2m[36m(pid=13136)[0m  eval loss: 12.870994606018066, eval err: 0.8649055242538453
[2m[36m(pid=13136)[0m 5, 
[2m[36m(pid=13136)[0m  train loss: 9.500445902347565
[2m[36m(pid=13136)[0m  eval loss: 11.597867546081543, eval err: 0.8564032101631165
[2m[36m(pid=13136)[0m 6, 
[2m[36m(pid=13136)[0m  train loss: 10.097983837127686
[2m[36m(pid=13136)[0m  eval loss: 10.848600425720214, eval err: 0.833838210105896
Result for train_b1c14_00012:
  date: 2021-09-09_14-42-16
  done: false
  err: 0.833838210105896
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 10.848600425720214
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 690.7339866161346
  time_this_iter_s: 292.90403485298157
  time_total_s: 690.7339866161346
  timestamp: 1631191336
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 16.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |      3 |          690.734 | 10.8486   | 0.833838  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 7, 
[2m[36m(pid=13136)[0m  train loss: 9.936342984437943
[2m[36m(pid=13136)[0m  eval loss: 11.508023681640625, eval err: 0.8556209397315979
[2m[36m(pid=13136)[0m 8, 
[2m[36m(pid=13136)[0m  train loss: 9.05629849433899
[2m[36m(pid=13136)[0m  eval loss: 10.43830509185791, eval err: 0.8225776410102844
[2m[36m(pid=13136)[0m 9, 
[2m[36m(pid=13136)[0m  train loss: 9.324789747595787
Result for train_b1c14_00012:
  date: 2021-09-09_14-47-08
  done: false
  err: 0.8226903533935547
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 10.316286087036133
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 982.5859091281891
  time_this_iter_s: 291.85192251205444
  time_total_s: 982.5859091281891
  timestamp: 1631191628
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00012
  
[2m[36m(pid=13136)[0m  eval loss: 10.316286087036133, eval err: 0.8226903533935547
== Status ==
Memory usage on this node: 16.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |      4 |          982.586 | 10.3163   | 0.82269   |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 10, 
[2m[36m(pid=13136)[0m  train loss: 8.915545150637627
[2m[36m(pid=13136)[0m  eval loss: 10.738390197753906, eval err: 0.8362048935890197
[2m[36m(pid=13136)[0m 11, 
[2m[36m(pid=13136)[0m  train loss: 9.523981302976608
[2m[36m(pid=13136)[0m  eval loss: 9.810263557434082, eval err: 0.8358411884307861
[2m[36m(pid=13136)[0m 12, 
[2m[36m(pid=13136)[0m  train loss: 8.855263277888298
[2m[36m(pid=13136)[0m  eval loss: 8.929254760742188, eval err: 0.7967315673828125
Result for train_b1c14_00012:
  date: 2021-09-09_14-51-59
  done: false
  err: 0.7967315673828125
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 8.929254760742188
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 1273.7366786003113
  time_this_iter_s: 291.1507694721222
  time_total_s: 1273.7366786003113
  timestamp: 1631191919
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 16.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |      5 |          1273.74 |  8.92925  | 0.796732  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 13, 
[2m[36m(pid=13136)[0m  train loss: 9.26017215847969
[2m[36m(pid=13136)[0m  eval loss: 9.945127906799316, eval err: 0.8324202322959899
[2m[36m(pid=13136)[0m 14, 
[2m[36m(pid=13136)[0m  train loss: 8.388668894767761
[2m[36m(pid=13136)[0m  eval loss: 10.935615425109864, eval err: 0.8197901678085328
[2m[36m(pid=13136)[0m 15, 
[2m[36m(pid=13136)[0m  train loss: 9.57702349126339
[2m[36m(pid=13136)[0m  eval loss: 8.925341911315918, eval err: 0.7880640077590942
Result for train_b1c14_00012:
  date: 2021-09-09_14-56-51
  done: false
  err: 0.7880640077590942
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 8.925341911315918
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 1565.3838922977448
  time_this_iter_s: 291.6472136974335
  time_total_s: 1565.3838922977448
  timestamp: 1631192211
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 16.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |      6 |          1565.38 |  8.92534  | 0.788064  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 16, 
[2m[36m(pid=13136)[0m  train loss: 8.291826039552689
[2m[36m(pid=13136)[0m  eval loss: 8.675073928833008, eval err: 0.8060790061950683
[2m[36m(pid=13136)[0m 17, 
[2m[36m(pid=13136)[0m  train loss: 9.087107315659523
[2m[36m(pid=13136)[0m  eval loss: 9.421801662445068, eval err: 0.8156600403785705
[2m[36m(pid=13136)[0m 18, 
[2m[36m(pid=13136)[0m  train loss: 8.41979743540287
[2m[36m(pid=13136)[0m  eval loss: 8.929877128601074, eval err: 0.7995135116577149
Result for train_b1c14_00012:
  date: 2021-09-09_15-01-42
  done: false
  err: 0.7995135116577149
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 8.929877128601074
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 1856.322098016739
  time_this_iter_s: 290.93820571899414
  time_total_s: 1856.322098016739
  timestamp: 1631192502
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 16.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |      7 |          1856.32 |  8.92988  | 0.799514  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 19, 
[2m[36m(pid=13136)[0m  train loss: 8.137856483459473
[2m[36m(pid=13136)[0m  eval loss: 9.01792869567871, eval err: 0.8159637689590454
[2m[36m(pid=13136)[0m 20, 
[2m[36m(pid=13136)[0m  train loss: 7.900758549571037
[2m[36m(pid=13136)[0m  eval loss: 8.1296195602417, eval err: 0.7873057436943054
[2m[36m(pid=13136)[0m 21, 
[2m[36m(pid=13136)[0m  train loss: 8.363261491060257
[2m[36m(pid=13136)[0m  eval loss: 10.25723762512207, eval err: 0.8150966501235962
Result for train_b1c14_00012:
  date: 2021-09-09_15-06-33
  done: false
  err: 0.8150966501235962
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 10.25723762512207
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 2147.779468536377
  time_this_iter_s: 291.45737051963806
  time_total_s: 2147.779468536377
  timestamp: 1631192793
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 17.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |      8 |          2147.78 | 10.2572   | 0.815097  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 22, 
[2m[36m(pid=13136)[0m  train loss: 8.6177369505167
[2m[36m(pid=13136)[0m  eval loss: 9.148104267120361, eval err: 0.7977513027191162
[2m[36m(pid=13136)[0m 23, 
[2m[36m(pid=13136)[0m  train loss: 8.470449790358543
[2m[36m(pid=13136)[0m  eval loss: 8.112828769683837, eval err: 0.7822771120071411
[2m[36m(pid=13136)[0m 24, 
[2m[36m(pid=13136)[0m  train loss: 8.138472750782967
[2m[36m(pid=13136)[0m  eval loss: 8.286054496765138, eval err: 0.7921803736686707
Result for train_b1c14_00012:
  date: 2021-09-09_15-11-25
  done: false
  err: 0.7921803736686707
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 8.286054496765138
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 2439.143007040024
  time_this_iter_s: 291.36353850364685
  time_total_s: 2439.143007040024
  timestamp: 1631193085
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 16.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |      9 |          2439.14 |  8.28605  | 0.79218   |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 25, 
[2m[36m(pid=13136)[0m  train loss: 8.17949566245079
[2m[36m(pid=13136)[0m  eval loss: 9.570456352233887, eval err: 0.7934015250205994
[2m[36m(pid=13136)[0m 26, 
[2m[36m(pid=13136)[0m  train loss: 7.817466288805008
[2m[36m(pid=13136)[0m  eval loss: 8.638819694519043, eval err: 0.7945853018760681
[2m[36m(pid=13136)[0m 27, 
[2m[36m(pid=13136)[0m  train loss: 7.925259485840797
[2m[36m(pid=13136)[0m  eval loss: 7.758566951751709, eval err: 0.7619386792182923
Result for train_b1c14_00012:
  date: 2021-09-09_15-16-17
  done: false
  err: 0.7619386792182923
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 7.758566951751709
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 2731.260877609253
  time_this_iter_s: 292.1178705692291
  time_total_s: 2731.260877609253
  timestamp: 1631193377
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 18.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     10 |          2731.26 |  7.75857  | 0.761939  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 28, 
[2m[36m(pid=13136)[0m  train loss: 7.948981016874313
[2m[36m(pid=13136)[0m  eval loss: 7.993167209625244, eval err: 0.7378065133094788
[2m[36m(pid=13136)[0m 29, 
[2m[36m(pid=13136)[0m  train loss: 8.122471258044243
[2m[36m(pid=13136)[0m  eval loss: 9.667591152191163, eval err: 0.7890203166007995
[2m[36m(pid=13136)[0m 30, 
[2m[36m(pid=13136)[0m  train loss: 7.543063834309578
[2m[36m(pid=13136)[0m  eval loss: 7.446789054870606, eval err: 0.7576550459861755
Result for train_b1c14_00012:
  date: 2021-09-09_15-21-08
  done: false
  err: 0.7576550459861755
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 7.446789054870606
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 3022.6036405563354
  time_this_iter_s: 291.3427629470825
  time_total_s: 3022.6036405563354
  timestamp: 1631193668
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 18.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     11 |          3022.6  |  7.44679  | 0.757655  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 31, 
[2m[36m(pid=13136)[0m  train loss: 7.813902243971825
[2m[36m(pid=13136)[0m  eval loss: 8.827415180206298, eval err: 0.7706159472465515
[2m[36m(pid=13136)[0m 32, 
[2m[36m(pid=13136)[0m  train loss: 7.747662156820297
[2m[36m(pid=13136)[0m  eval loss: 7.303130683898925, eval err: 0.745278115272522
[2m[36m(pid=13136)[0m 33, 
[2m[36m(pid=13136)[0m  train loss: 7.273849248886108
[2m[36m(pid=13136)[0m  eval loss: 8.277245826721192, eval err: 0.7604874610900879
Result for train_b1c14_00012:
  date: 2021-09-09_15-26-00
  done: false
  err: 0.7604874610900879
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 8.277245826721192
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 3314.0548191070557
  time_this_iter_s: 291.4511785507202
  time_total_s: 3314.0548191070557
  timestamp: 1631193960
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 18.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     12 |          3314.05 |  8.27725  | 0.760487  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 34, 
[2m[36m(pid=13136)[0m  train loss: 7.335485816001892
[2m[36m(pid=13136)[0m  eval loss: 7.426133785247803, eval err: 0.7455903768539429
[2m[36m(pid=13136)[0m 35, 
[2m[36m(pid=13136)[0m  train loss: 7.0895091742277145
[2m[36m(pid=13136)[0m  eval loss: 8.372995738983155, eval err: 0.7633816719055175
[2m[36m(pid=13136)[0m 36, 
[2m[36m(pid=13136)[0m  train loss: 6.821989864110947
[2m[36m(pid=13136)[0m  eval loss: 8.174982109069823, eval err: 0.7460163450241089
Result for train_b1c14_00012:
  date: 2021-09-09_15-30-51
  done: false
  err: 0.7460163450241089
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 8.174982109069823
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 3605.8655343055725
  time_this_iter_s: 291.81071519851685
  time_total_s: 3605.8655343055725
  timestamp: 1631194251
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 18.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     13 |          3605.87 |  8.17498  | 0.746016  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13136)[0m 37, 
[2m[36m(pid=13136)[0m  train loss: 6.869257360696793
[2m[36m(pid=13136)[0m  eval loss: 6.89110876083374, eval err: 0.7307098388671875
[2m[36m(pid=13136)[0m 38, 
[2m[36m(pid=13136)[0m  train loss: 7.080586031079292
[2m[36m(pid=13136)[0m  eval loss: 7.451413745880127, eval err: 0.7227472829818725
[2m[36m(pid=13136)[0m 39, 
[2m[36m(pid=13136)[0m  train loss: 6.844925224781036
[2m[36m(pid=13136)[0m  eval loss: 7.545136604309082, eval err: 0.7285831665992737
Result for train_b1c14_00012:
  date: 2021-09-09_15-35-43
  done: true
  err: 0.7285831665992737
  experiment_id: 22accb41d8d0404ca707ddd971072ea6
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 7.545136604309082
  node_ip: 131.220.7.54
  pid: 13136
  should_checkpoint: true
  time_since_restore: 3897.674440383911
  time_this_iter_s: 291.8089060783386
  time_total_s: 3897.674440383911
  timestamp: 1631194543
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00012
  
== Status ==
Memory usage on this node: 18.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 12 PENDING, 1 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00012 | RUNNING    | 131.220.7.54:13136 |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | PENDING    |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |        |                  |           |           |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m Epoch:
[2m[36m(pid=13134)[0m 0, 
[2m[36m(pid=13134)[0m  train loss: 36.885270833969116
[2m[36m(pid=13134)[0m  eval loss: 115.92756652832031, eval err: 0.999890296459198
Result for train_b1c14_00013:
  date: 2021-09-09_15-37-36
  done: false
  err: 0.999890296459198
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 115.92756652832031
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 110.77297782897949
  time_this_iter_s: 110.77297782897949
  time_total_s: 110.77297782897949
  timestamp: 1631194656
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00013
  
== Status ==
Memory usage on this node: 18.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |      1 |          110.773 | 115.928    | 0.99989   |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |            |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |            |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |            |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |            |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |            |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |            |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |            |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |            |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |            |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 1, 
[2m[36m(pid=13134)[0m  train loss: 17.809364289045334
[2m[36m(pid=13134)[0m  eval loss: 36.602208557128904, eval err: 0.974667739868164
[2m[36m(pid=13134)[0m 2, 
[2m[36m(pid=13134)[0m  train loss: 12.308718472719193
[2m[36m(pid=13134)[0m  eval loss: 40.6721337890625, eval err: 0.9786842536926269
[2m[36m(pid=13134)[0m 3, 
[2m[36m(pid=13134)[0m  train loss: 11.681519627571106
[2m[36m(pid=13134)[0m  eval loss: 26.948405075073243, eval err: 0.9267726707458496
Result for train_b1c14_00013:
  date: 2021-09-09_15-43-00
  done: false
  err: 0.9267726707458496
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 26.948405075073243
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 434.5523250102997
  time_this_iter_s: 323.7793471813202
  time_total_s: 434.5523250102997
  timestamp: 1631194980
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00013
  
== Status ==
Memory usage on this node: 18.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |      2 |          434.552 | 26.9484   | 0.926773  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 4, 
[2m[36m(pid=13134)[0m  train loss: 12.055431067943573
[2m[36m(pid=13134)[0m  eval loss: 12.589598617553712, eval err: 0.880089077949524
[2m[36m(pid=13134)[0m 5, 
[2m[36m(pid=13134)[0m  train loss: 10.976958274841309
[2m[36m(pid=13134)[0m  eval loss: 36.77447174072265, eval err: 0.979333438873291
[2m[36m(pid=13134)[0m 6, 
[2m[36m(pid=13134)[0m  train loss: 11.367308035492897
Result for train_b1c14_00013:
  date: 2021-09-09_15-48-23
  done: false
  err: 0.8465779638290405
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 10.63653392791748
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 758.2198467254639
  time_this_iter_s: 323.6675217151642
  time_total_s: 758.2198467254639
  timestamp: 1631195303
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00013
  
[2m[36m(pid=13134)[0m  eval loss: 10.63653392791748, eval err: 0.8465779638290405
== Status ==
Memory usage on this node: 18.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |      3 |           758.22 | 10.6365   | 0.846578  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 7, 
[2m[36m(pid=13134)[0m  train loss: 10.612284421920776
[2m[36m(pid=13134)[0m  eval loss: 16.240853843688964, eval err: 0.9049543380737305
[2m[36m(pid=13134)[0m 8, 
[2m[36m(pid=13134)[0m  train loss: 9.866151973605156
[2m[36m(pid=13134)[0m  eval loss: 12.914519386291504, eval err: 0.8562613558769226
[2m[36m(pid=13134)[0m 9, 
[2m[36m(pid=13134)[0m  train loss: 9.765880703926086
Result for train_b1c14_00013:
  date: 2021-09-09_15-53-47
  done: false
  err: 0.859804961681366
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 13.434206733703613
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 1081.980665922165
  time_this_iter_s: 323.76081919670105
  time_total_s: 1081.980665922165
  timestamp: 1631195627
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00013
  
[2m[36m(pid=13134)[0m  eval loss: 13.434206733703613, eval err: 0.859804961681366
== Status ==
Memory usage on this node: 18.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |      4 |          1081.98 | 13.4342   | 0.859805  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 10, 
[2m[36m(pid=13134)[0m  train loss: 9.841947227716446
[2m[36m(pid=13134)[0m  eval loss: 12.172498931884766, eval err: 0.8658588004112243
[2m[36m(pid=13134)[0m 11, 
[2m[36m(pid=13134)[0m  train loss: 10.060732558369637
[2m[36m(pid=13134)[0m  eval loss: 10.532613067626952, eval err: 0.850152850151062
[2m[36m(pid=13134)[0m 12, 
[2m[36m(pid=13134)[0m  train loss: 9.735825434327126
Result for train_b1c14_00013:
  date: 2021-09-09_15-59-10
  done: false
  err: 0.8708018612861633
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 11.173590087890625
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 1405.2240312099457
  time_this_iter_s: 323.24336528778076
  time_total_s: 1405.2240312099457
  timestamp: 1631195950
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00013
  
[2m[36m(pid=13134)[0m  eval loss: 11.173590087890625, eval err: 0.8708018612861633
== Status ==
Memory usage on this node: 18.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |      5 |          1405.22 | 11.1736   | 0.870802  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 13, 
[2m[36m(pid=13134)[0m  train loss: 10.045464754104614
[2m[36m(pid=13134)[0m  eval loss: 13.258811073303223, eval err: 0.8574236297607422
[2m[36m(pid=13134)[0m 14, 
[2m[36m(pid=13134)[0m  train loss: 9.511496350169182
[2m[36m(pid=13134)[0m  eval loss: 13.405289649963379, eval err: 0.8551815581321717
[2m[36m(pid=13134)[0m 15, 
[2m[36m(pid=13134)[0m  train loss: 9.461904898285866
[2m[36m(pid=13134)[0m  eval loss: 10.710910606384278, eval err: 0.8635117197036744
Result for train_b1c14_00013:
  date: 2021-09-09_16-04-34
  done: false
  err: 0.8635117197036744
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 10.710910606384278
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 1728.5143356323242
  time_this_iter_s: 323.29030442237854
  time_total_s: 1728.5143356323242
  timestamp: 1631196274
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00013
  
== Status ==
Memory usage on this node: 18.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |      6 |          1728.51 | 10.7109   | 0.863512  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 16, 
[2m[36m(pid=13134)[0m  train loss: 9.59981121122837
[2m[36m(pid=13134)[0m  eval loss: 11.809856605529784, eval err: 0.8650293278694153
[2m[36m(pid=13134)[0m 17, 
[2m[36m(pid=13134)[0m  train loss: 9.524589702486992
[2m[36m(pid=13134)[0m  eval loss: 11.319082260131836, eval err: 0.855705714225769
[2m[36m(pid=13134)[0m 18, 
[2m[36m(pid=13134)[0m  train loss: 9.782332941889763
[2m[36m(pid=13134)[0m  eval loss: 12.869286842346192, eval err: 0.8418666124343872
Result for train_b1c14_00013:
  date: 2021-09-09_16-09-57
  done: false
  err: 0.8418666124343872
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 12.869286842346192
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 2051.7143952846527
  time_this_iter_s: 323.2000596523285
  time_total_s: 2051.7143952846527
  timestamp: 1631196597
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00013
  
== Status ==
Memory usage on this node: 18.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |      7 |          2051.71 | 12.8693   | 0.841867  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 19, 
[2m[36m(pid=13134)[0m  train loss: 9.568592816591263
[2m[36m(pid=13134)[0m  eval loss: 13.248453102111817, eval err: 0.8490769505500794
[2m[36m(pid=13134)[0m 20, 
[2m[36m(pid=13134)[0m  train loss: 9.609860301017761
[2m[36m(pid=13134)[0m  eval loss: 12.412186584472657, eval err: 0.8462040209770203
[2m[36m(pid=13134)[0m 21, 
[2m[36m(pid=13134)[0m  train loss: 9.048048362135887
[2m[36m(pid=13134)[0m  eval loss: 11.856800956726074, eval err: 0.8768385505676269
Result for train_b1c14_00013:
  date: 2021-09-09_16-15-20
  done: false
  err: 0.8768385505676269
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 11.856800956726074
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 2374.61624956131
  time_this_iter_s: 322.9018542766571
  time_total_s: 2374.61624956131
  timestamp: 1631196920
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00013
  
== Status ==
Memory usage on this node: 18.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |      8 |          2374.62 | 11.8568   | 0.876839  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 22, 
[2m[36m(pid=13134)[0m  train loss: 8.918552175164223
[2m[36m(pid=13134)[0m  eval loss: 10.43251537322998, eval err: 0.8298155426979065
[2m[36m(pid=13134)[0m 23, 
[2m[36m(pid=13134)[0m  train loss: 8.809570670127869
[2m[36m(pid=13134)[0m  eval loss: 10.901981163024903, eval err: 0.8573144412040711
[2m[36m(pid=13134)[0m 24, 
[2m[36m(pid=13134)[0m  train loss: 9.09098856151104
[2m[36m(pid=13134)[0m  eval loss: 9.812424087524414, eval err: 0.8416002297401428
Result for train_b1c14_00013:
  date: 2021-09-09_16-20-43
  done: false
  err: 0.8416002297401428
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 9.812424087524414
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 2697.9250280857086
  time_this_iter_s: 323.3087785243988
  time_total_s: 2697.9250280857086
  timestamp: 1631197243
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00013
  
== Status ==
Memory usage on this node: 18.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |      9 |          2697.93 |  9.81242  | 0.8416    |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 25, 
[2m[36m(pid=13134)[0m  train loss: 9.588927522301674
[2m[36m(pid=13134)[0m  eval loss: 11.974296035766601, eval err: 0.8348197913169861
[2m[36m(pid=13134)[0m 26, 
[2m[36m(pid=13134)[0m  train loss: 9.111478313803673
[2m[36m(pid=13134)[0m  eval loss: 11.868304557800293, eval err: 0.8754460883140563
[2m[36m(pid=13134)[0m 27, 
[2m[36m(pid=13134)[0m  train loss: 9.067235857248306
[2m[36m(pid=13134)[0m  eval loss: 12.877029609680175, eval err: 0.8515947794914246
Result for train_b1c14_00013:
  date: 2021-09-09_16-26-07
  done: false
  err: 0.8515947794914246
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 12.877029609680175
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 3021.9119350910187
  time_this_iter_s: 323.98690700531006
  time_total_s: 3021.9119350910187
  timestamp: 1631197567
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00013
  
== Status ==
Memory usage on this node: 18.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     10 |          3021.91 | 12.877    | 0.851595  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 28, 
[2m[36m(pid=13134)[0m  train loss: 8.635553136467934
[2m[36m(pid=13134)[0m  eval loss: 10.367332572937011, eval err: 0.8532160425186157
[2m[36m(pid=13134)[0m 29, 
[2m[36m(pid=13134)[0m  train loss: 8.493125915527344
[2m[36m(pid=13134)[0m  eval loss: 12.437971229553222, eval err: 0.8325029873847961
[2m[36m(pid=13134)[0m 30, 
[2m[36m(pid=13134)[0m  train loss: 8.894771233201027
Result for train_b1c14_00013:
  date: 2021-09-09_16-31-31
  done: false
  err: 0.8400366139411927
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 10.716185607910155
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 3345.369485616684
  time_this_iter_s: 323.4575505256653
  time_total_s: 3345.369485616684
  timestamp: 1631197891
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00013
  
[2m[36m(pid=13134)[0m  eval loss: 10.716185607910155, eval err: 0.8400366139411927
== Status ==
Memory usage on this node: 19.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     11 |          3345.37 | 10.7162   | 0.840037  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 31, 
[2m[36m(pid=13134)[0m  train loss: 8.782583981752396
[2m[36m(pid=13134)[0m  eval loss: 10.63197364807129, eval err: 0.8343727493286133
[2m[36m(pid=13134)[0m 32, 
[2m[36m(pid=13134)[0m  train loss: 8.689641162753105
[2m[36m(pid=13134)[0m  eval loss: 10.127621803283692, eval err: 0.8425968360900878
[2m[36m(pid=13134)[0m 33, 
[2m[36m(pid=13134)[0m  train loss: 9.259575203061104
Result for train_b1c14_00013:
  date: 2021-09-09_16-36-54
  done: false
  err: 0.8403892588615417
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 10.836190452575684
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 3668.936815738678
  time_this_iter_s: 323.567330121994
  time_total_s: 3668.936815738678
  timestamp: 1631198214
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00013
  
[2m[36m(pid=13134)[0m  eval loss: 10.836190452575684, eval err: 0.8403892588615417
== Status ==
Memory usage on this node: 19.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     12 |          3668.94 | 10.8362   | 0.840389  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 34, 
[2m[36m(pid=13134)[0m  train loss: 8.458329692482948
[2m[36m(pid=13134)[0m  eval loss: 10.967762832641602, eval err: 0.8377366590499878
[2m[36m(pid=13134)[0m 35, 
[2m[36m(pid=13134)[0m  train loss: 8.453192725777626
[2m[36m(pid=13134)[0m  eval loss: 10.15227466583252, eval err: 0.836285400390625
[2m[36m(pid=13134)[0m 36, 
[2m[36m(pid=13134)[0m  train loss: 8.658178105950356
Result for train_b1c14_00013:
  date: 2021-09-09_16-42-18
  done: false
  err: 0.8482940697669983
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 10.408970336914063
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 3992.7948236465454
  time_this_iter_s: 323.85800790786743
  time_total_s: 3992.7948236465454
  timestamp: 1631198538
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00013
  
[2m[36m(pid=13134)[0m  eval loss: 10.408970336914063, eval err: 0.8482940697669983
== Status ==
Memory usage on this node: 19.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     13 |          3992.79 | 10.409    | 0.848294  |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13134)[0m 37, 
[2m[36m(pid=13134)[0m  train loss: 8.55774974822998
[2m[36m(pid=13134)[0m  eval loss: 9.785990009307861, eval err: 0.8325970244407653
[2m[36m(pid=13134)[0m 38, 
[2m[36m(pid=13134)[0m  train loss: 8.564268365502357
[2m[36m(pid=13134)[0m  eval loss: 11.717501525878907, eval err: 0.8582921171188355
[2m[36m(pid=13134)[0m 39, 
[2m[36m(pid=13134)[0m  train loss: 8.39556808769703
[2m[36m(pid=13134)[0m  eval loss: 9.757204132080078, eval err: 0.829930248260498
Result for train_b1c14_00013:
  date: 2021-09-09_16-47-41
  done: true
  err: 0.829930248260498
  experiment_id: 6e604615f8184d689890c85b4560eb93
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 9.757204132080078
  node_ip: 131.220.7.54
  pid: 13134
  should_checkpoint: true
  time_since_restore: 4316.2303466796875
  time_this_iter_s: 323.4355230331421
  time_total_s: 4316.2303466796875
  timestamp: 1631198861
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00013
  
== Status ==
Memory usage on this node: 19.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 ERROR, 11 PENDING, 1 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00013 | RUNNING    | 131.220.7.54:13134 |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00014 | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00015 | PENDING    |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 ERROR)
Number of errored trials: 9
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13122)[0m Epoch:
[2m[36m(pid=13122)[0m 0, 
Result for train_b1c14_00014:
  {}
  
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 10 PENDING, 5 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | PENDING    |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |        |                  |           |           |
| train_b1c14_00016 | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 2 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m Epoch:
[2m[36m(pid=13133)[0m 0, 
[2m[36m(pid=13133)[0m  train loss: 32.97186529636383
Result for train_b1c14_00015:
  date: 2021-09-09_16-49-44
  done: false
  err: 0.9647745990753174
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 67.04834762573242
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 110.94919037818909
  time_this_iter_s: 110.94919037818909
  time_total_s: 110.94919037818909
  timestamp: 1631198984
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00015
  
[2m[36m(pid=13133)[0m  eval loss: 67.04834762573242, eval err: 0.9647745990753174
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |      1 |          110.949 | 67.0483   | 0.964775  |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 1, 
[2m[36m(pid=13133)[0m  train loss: 14.266096860170364
[2m[36m(pid=13133)[0m  eval loss: 13.979458847045898, eval err: 0.8637885975837708
[2m[36m(pid=13133)[0m 2, 
[2m[36m(pid=13133)[0m  train loss: 11.313324943184853
[2m[36m(pid=13133)[0m  eval loss: 16.814652805328368, eval err: 0.875258891582489
[2m[36m(pid=13133)[0m 3, 
[2m[36m(pid=13133)[0m  train loss: 11.036239087581635
[2m[36m(pid=13133)[0m  eval loss: 10.705186824798584, eval err: 0.8180483102798461
Result for train_b1c14_00015:
  date: 2021-09-09_16-55-09
  done: false
  err: 0.8180483102798461
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 10.705186824798584
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 436.3054242134094
  time_this_iter_s: 325.35623383522034
  time_total_s: 436.3054242134094
  timestamp: 1631199309
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |      2 |          436.305 | 10.7052   | 0.818048  |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 4, 
[2m[36m(pid=13133)[0m  train loss: 10.139320775866508
[2m[36m(pid=13133)[0m  eval loss: 12.90908390045166, eval err: 0.8602700328826904
[2m[36m(pid=13133)[0m 5, 
[2m[36m(pid=13133)[0m  train loss: 9.899027198553085
[2m[36m(pid=13133)[0m  eval loss: 10.943040466308593, eval err: 0.8320106792449952
[2m[36m(pid=13133)[0m 6, 
[2m[36m(pid=13133)[0m  train loss: 9.00048053264618
Result for train_b1c14_00015:
  date: 2021-09-09_17-00-34
  done: false
  err: 0.8023199367523194
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 8.630916194915772
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 760.5345118045807
  time_this_iter_s: 324.22908759117126
  time_total_s: 760.5345118045807
  timestamp: 1631199634
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00015
  
[2m[36m(pid=13133)[0m  eval loss: 8.630916194915772, eval err: 0.8023199367523194
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |      3 |          760.535 |  8.63092  | 0.80232   |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 7, 
[2m[36m(pid=13133)[0m  train loss: 8.893139436841011
[2m[36m(pid=13133)[0m  eval loss: 11.173319778442384, eval err: 0.8375569820404053
[2m[36m(pid=13133)[0m 8, 
[2m[36m(pid=13133)[0m  train loss: 8.565571546554565
[2m[36m(pid=13133)[0m  eval loss: 9.285921840667724, eval err: 0.8053487753868103
[2m[36m(pid=13133)[0m 9, 
[2m[36m(pid=13133)[0m  train loss: 8.945945858955383
[2m[36m(pid=13133)[0m  eval loss: 11.175642471313477, eval err: 0.8276243853569031
Result for train_b1c14_00015:
  date: 2021-09-09_17-05-57
  done: false
  err: 0.8276243853569031
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 11.175642471313477
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 1084.2511429786682
  time_this_iter_s: 323.7166311740875
  time_total_s: 1084.2511429786682
  timestamp: 1631199957
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |      4 |          1084.25 | 11.1756   | 0.827624  |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 10, 
[2m[36m(pid=13133)[0m  train loss: 7.20078906416893
[2m[36m(pid=13133)[0m  eval loss: 8.588420181274413, eval err: 0.7886634564399719
[2m[36m(pid=13133)[0m 11, 
[2m[36m(pid=13133)[0m  train loss: 7.0959037989377975
[2m[36m(pid=13133)[0m  eval loss: 9.193803215026856, eval err: 0.7773407030105591
[2m[36m(pid=13133)[0m 12, 
[2m[36m(pid=13133)[0m  train loss: 7.616230338811874
[2m[36m(pid=13133)[0m  eval loss: 6.813111038208008, eval err: 0.6936753177642823
Result for train_b1c14_00015:
  date: 2021-09-09_17-11-22
  done: false
  err: 0.6936753177642823
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 6.813111038208008
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 1408.6577112674713
  time_this_iter_s: 324.4065682888031
  time_total_s: 1408.6577112674713
  timestamp: 1631200282
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |      5 |          1408.66 |  6.81311  | 0.693675  |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 13, 
[2m[36m(pid=13133)[0m  train loss: 5.780459240078926
[2m[36m(pid=13133)[0m  eval loss: 6.501629257202149, eval err: 0.5653924131393433
[2m[36m(pid=13133)[0m 14, 
[2m[36m(pid=13133)[0m  train loss: 4.982568368315697
[2m[36m(pid=13133)[0m  eval loss: 3.681800241470337, eval err: 0.39383683204650877
[2m[36m(pid=13133)[0m 15, 
[2m[36m(pid=13133)[0m  train loss: 3.8184860795736313
[2m[36m(pid=13133)[0m  eval loss: 2.8145552253723145, eval err: 0.28756041049957276
Result for train_b1c14_00015:
  date: 2021-09-09_17-16-46
  done: false
  err: 0.28756041049957276
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 2.8145552253723145
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 1732.727784395218
  time_this_iter_s: 324.0700731277466
  time_total_s: 1732.727784395218
  timestamp: 1631200606
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 19.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |      6 |          1732.73 |  2.81456  | 0.28756   |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 16, 
[2m[36m(pid=13133)[0m  train loss: 2.8735245652496815
[2m[36m(pid=13133)[0m  eval loss: 2.2357615518569944, eval err: 0.22297513723373413
[2m[36m(pid=13133)[0m 17, 
[2m[36m(pid=13133)[0m  train loss: 2.1757090613245964
[2m[36m(pid=13133)[0m  eval loss: 1.6080182242393493, eval err: 0.15396274089813233
[2m[36m(pid=13133)[0m 18, 
[2m[36m(pid=13133)[0m  train loss: 2.0936331395059824
Result for train_b1c14_00015:
  date: 2021-09-09_17-22-10
  done: false
  err: 0.1281459403038025
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 1.4229207706451417
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 2057.2716886997223
  time_this_iter_s: 324.5439043045044
  time_total_s: 2057.2716886997223
  timestamp: 1631200930
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00015
  
[2m[36m(pid=13133)[0m  eval loss: 1.4229207706451417, eval err: 0.1281459403038025
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |      7 |          2057.27 |  1.42292  | 0.128146  |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 19, 
[2m[36m(pid=13133)[0m  train loss: 1.7750264033675194
[2m[36m(pid=13133)[0m  eval loss: 1.3377716445922851, eval err: 0.11381453514099121
[2m[36m(pid=13133)[0m 20, 
[2m[36m(pid=13133)[0m  train loss: 1.607643524184823
[2m[36m(pid=13133)[0m  eval loss: 1.2205165195465089, eval err: 0.09957956314086915
[2m[36m(pid=13133)[0m 21, 
[2m[36m(pid=13133)[0m  train loss: 1.5591610074043274
Result for train_b1c14_00015:
  date: 2021-09-09_17-27-35
  done: false
  err: 0.09132433414459229
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 1.114738962650299
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 2381.809044122696
  time_this_iter_s: 324.53735542297363
  time_total_s: 2381.809044122696
  timestamp: 1631201255
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00015
  
[2m[36m(pid=13133)[0m  eval loss: 1.114738962650299, eval err: 0.09132433414459229
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |      8 |          2381.81 |  1.11474  | 0.0913243 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 22, 
[2m[36m(pid=13133)[0m  train loss: 1.517916103824973
[2m[36m(pid=13133)[0m  eval loss: 1.0935923647880554, eval err: 0.08901419878005981
[2m[36m(pid=13133)[0m 23, 
[2m[36m(pid=13133)[0m  train loss: 1.639631712809205
[2m[36m(pid=13133)[0m  eval loss: 1.1632625222206117, eval err: 0.09930238008499145
[2m[36m(pid=13133)[0m 24, 
[2m[36m(pid=13133)[0m  train loss: 1.5883144456893206
Result for train_b1c14_00015:
  date: 2021-09-09_17-32-57
  done: false
  err: 0.0845176076889038
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 1.0690840363502503
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 2704.136817932129
  time_this_iter_s: 322.327773809433
  time_total_s: 2704.136817932129
  timestamp: 1631201577
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00015
  
[2m[36m(pid=13133)[0m  eval loss: 1.0690840363502503, eval err: 0.0845176076889038
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |      9 |          2704.14 |  1.06908  | 0.0845176 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 25, 
[2m[36m(pid=13133)[0m  train loss: 1.5848660785704851
[2m[36m(pid=13133)[0m  eval loss: 1.1462540483474732, eval err: 0.09092718124389648
[2m[36m(pid=13133)[0m 26, 
[2m[36m(pid=13133)[0m  train loss: 1.7220470160245895
[2m[36m(pid=13133)[0m  eval loss: 1.0988811612129212, eval err: 0.08594950437545776
[2m[36m(pid=13133)[0m 27, 
[2m[36m(pid=13133)[0m  train loss: 1.911470802500844
[2m[36m(pid=13133)[0m  eval loss: 1.0929981088638305, eval err: 0.08637856483459473
Result for train_b1c14_00015:
  date: 2021-09-09_17-38-18
  done: false
  err: 0.08637856483459473
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 1.0929981088638305
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 3024.805923938751
  time_this_iter_s: 320.6691060066223
  time_total_s: 3024.805923938751
  timestamp: 1631201898
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     10 |          3024.81 |  1.093    | 0.0863786 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 28, 
[2m[36m(pid=13133)[0m  train loss: 1.4485033582895994
[2m[36m(pid=13133)[0m  eval loss: 1.043895547389984, eval err: 0.0809227728843689
[2m[36m(pid=13133)[0m 29, 
[2m[36m(pid=13133)[0m  train loss: 1.3847389444708824
[2m[36m(pid=13133)[0m  eval loss: 1.0221499037742614, eval err: 0.07848871469497681
[2m[36m(pid=13133)[0m 30, 
[2m[36m(pid=13133)[0m  train loss: 1.3961302917450666
[2m[36m(pid=13133)[0m  eval loss: 0.9543859374523163, eval err: 0.07234306573867798
Result for train_b1c14_00015:
  date: 2021-09-09_17-43-38
  done: false
  err: 0.07234306573867798
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 0.9543859374523163
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 3344.670445203781
  time_this_iter_s: 319.8645212650299
  time_total_s: 3344.670445203781
  timestamp: 1631202218
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     11 |          3344.67 |  0.954386 | 0.0723431 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 31, 
[2m[36m(pid=13133)[0m  train loss: 1.496464990079403
[2m[36m(pid=13133)[0m  eval loss: 0.9042142426967621, eval err: 0.06801501750946044
[2m[36m(pid=13133)[0m 32, 
[2m[36m(pid=13133)[0m  train loss: 1.2403786778450012
[2m[36m(pid=13133)[0m  eval loss: 0.8825613641738892, eval err: 0.06557325601577758
[2m[36m(pid=13133)[0m 33, 
[2m[36m(pid=13133)[0m  train loss: 1.1487639732658863
[2m[36m(pid=13133)[0m  eval loss: 0.8471724200248718, eval err: 0.06278611660003662
Result for train_b1c14_00015:
  date: 2021-09-09_17-48-58
  done: false
  err: 0.06278611660003662
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 0.8471724200248718
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 3665.142465353012
  time_this_iter_s: 320.47202014923096
  time_total_s: 3665.142465353012
  timestamp: 1631202538
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     12 |          3665.14 |  0.847172 | 0.0627861 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 34, 
[2m[36m(pid=13133)[0m  train loss: 0.9371346458792686
[2m[36m(pid=13133)[0m  eval loss: 0.8300519740581512, eval err: 0.06146220922470093
[2m[36m(pid=13133)[0m 35, 
[2m[36m(pid=13133)[0m  train loss: 0.9691379945725203
[2m[36m(pid=13133)[0m  eval loss: 0.8484588193893433, eval err: 0.0636479115486145
[2m[36m(pid=13133)[0m 36, 
[2m[36m(pid=13133)[0m  train loss: 1.2474183812737465
[2m[36m(pid=13133)[0m  eval loss: 1.020461962223053, eval err: 0.07568863153457642
Result for train_b1c14_00015:
  date: 2021-09-09_17-54-20
  done: false
  err: 0.07568863153457642
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 1.020461962223053
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 3986.527997970581
  time_this_iter_s: 321.38553261756897
  time_total_s: 3986.527997970581
  timestamp: 1631202860
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     13 |          3986.53 |  1.02046  | 0.0756886 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 37, 
[2m[36m(pid=13133)[0m  train loss: 1.2695500925183296
[2m[36m(pid=13133)[0m  eval loss: 0.9109568309783935, eval err: 0.06820205450057984
[2m[36m(pid=13133)[0m 38, 
[2m[36m(pid=13133)[0m  train loss: 0.9345622286200523
[2m[36m(pid=13133)[0m  eval loss: 0.8251437187194824, eval err: 0.05929331302642822
[2m[36m(pid=13133)[0m 39, 
[2m[36m(pid=13133)[0m  train loss: 0.9747827732935548
[2m[36m(pid=13133)[0m  eval loss: 0.8240160119533538, eval err: 0.05913946390151978
Result for train_b1c14_00015:
  date: 2021-09-09_17-59-41
  done: false
  err: 0.05913946390151978
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 0.8240160119533538
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 4307.95436668396
  time_this_iter_s: 321.4263687133789
  time_total_s: 4307.95436668396
  timestamp: 1631203181
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     14 |          4307.95 |  0.824016 | 0.0591395 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 40, 
[2m[36m(pid=13133)[0m  train loss: 0.9590599043294787
[2m[36m(pid=13133)[0m  eval loss: 0.7780910170078278, eval err: 0.05468005895614624
[2m[36m(pid=13133)[0m 41, 
[2m[36m(pid=13133)[0m  train loss: 0.9593372279778123
[2m[36m(pid=13133)[0m  eval loss: 0.9533189105987548, eval err: 0.062057275772094724
[2m[36m(pid=13133)[0m 42, 
[2m[36m(pid=13133)[0m  train loss: 0.915142523124814
[2m[36m(pid=13133)[0m  eval loss: 0.7596026623249054, eval err: 0.05478663682937622
Result for train_b1c14_00015:
  date: 2021-09-09_18-05-02
  done: false
  err: 0.05478663682937622
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 0.7596026623249054
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 4629.130308151245
  time_this_iter_s: 321.17594146728516
  time_total_s: 4629.130308151245
  timestamp: 1631203502
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     15 |          4629.13 |  0.759603 | 0.0547866 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 43, 
[2m[36m(pid=13133)[0m  train loss: 1.2951185647398233
[2m[36m(pid=13133)[0m  eval loss: 0.9248493790626526, eval err: 0.06217053413391113
[2m[36m(pid=13133)[0m 44, 
[2m[36m(pid=13133)[0m  train loss: 0.9933404177427292
[2m[36m(pid=13133)[0m  eval loss: 0.8117005133628845, eval err: 0.05723843812942505
[2m[36m(pid=13133)[0m 45, 
[2m[36m(pid=13133)[0m  train loss: 1.142443811520934
[2m[36m(pid=13133)[0m  eval loss: 0.7993560183048248, eval err: 0.05625425815582275
Result for train_b1c14_00015:
  date: 2021-09-09_18-10-24
  done: false
  err: 0.05625425815582275
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 0.7993560183048248
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 4951.008071422577
  time_this_iter_s: 321.8777632713318
  time_total_s: 4951.008071422577
  timestamp: 1631203824
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     16 |          4951.01 |  0.799356 | 0.0562543 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 46, 
[2m[36m(pid=13133)[0m  train loss: 1.0401582103222609
[2m[36m(pid=13133)[0m  eval loss: 0.7892039394378663, eval err: 0.05631558179855347
[2m[36m(pid=13133)[0m 47, 
[2m[36m(pid=13133)[0m  train loss: 1.1371937049552798
[2m[36m(pid=13133)[0m  eval loss: 0.7844628036022187, eval err: 0.05605863809585571
[2m[36m(pid=13133)[0m 48, 
[2m[36m(pid=13133)[0m  train loss: 1.0793151091784239
[2m[36m(pid=13133)[0m  eval loss: 0.7825877487659454, eval err: 0.05558070421218872
Result for train_b1c14_00015:
  date: 2021-09-09_18-15-45
  done: false
  err: 0.05558070421218872
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 0.7825877487659454
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 5272.148420095444
  time_this_iter_s: 321.1403486728668
  time_total_s: 5272.148420095444
  timestamp: 1631204145
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     17 |          5272.15 |  0.782588 | 0.0555807 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 49, 
[2m[36m(pid=13133)[0m  train loss: 1.0218263044953346
[2m[36m(pid=13133)[0m  eval loss: 0.780763064622879, eval err: 0.05484233856201172
[2m[36m(pid=13133)[0m 50, 
[2m[36m(pid=13133)[0m  train loss: 0.9591220263391733
[2m[36m(pid=13133)[0m  eval loss: 0.7735064971446991, eval err: 0.05420866012573242
[2m[36m(pid=13133)[0m 51, 
[2m[36m(pid=13133)[0m  train loss: 0.956283314153552
[2m[36m(pid=13133)[0m  eval loss: 0.7649737858772278, eval err: 0.054083797931671146
Result for train_b1c14_00015:
  date: 2021-09-09_18-21-06
  done: false
  err: 0.054083797931671146
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 0.7649737858772278
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 5593.11144900322
  time_this_iter_s: 320.9630289077759
  time_total_s: 5593.11144900322
  timestamp: 1631204466
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     18 |          5593.11 |  0.764974 | 0.0540838 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m 52, 
[2m[36m(pid=13133)[0m  train loss: 0.9179394142702222
[2m[36m(pid=13133)[0m  eval loss: 0.7630699753761292, eval err: 0.05431264877319336
[2m[36m(pid=13133)[0m 53, 
[2m[36m(pid=13133)[0m  train loss: 0.9373683799058199
[2m[36m(pid=13133)[0m  eval loss: 0.7535916376113891, eval err: 0.05323471307754517
[2m[36m(pid=13133)[0m 54
[2m[36m(pid=13133)[0m , 
[2m[36m(pid=13133)[0m  train loss: 0.9435900365933776
Result for train_b1c14_00015:
  date: 2021-09-09_18-26-27
  done: false
  err: 0.05335970640182495
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 0.7555648863315583
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 5913.766588449478
  time_this_iter_s: 320.65513944625854
  time_total_s: 5913.766588449478
  timestamp: 1631204787
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: b1c14_00015
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     19 |          5913.77 |  0.755565 | 0.0533597 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13133)[0m  eval loss: 0.7555648863315583, eval err: 0.05335970640182495
[2m[36m(pid=13133)[0m 55, 
[2m[36m(pid=13133)[0m  train loss: 0.8673520302399993
[2m[36m(pid=13133)[0m  eval loss: 0.7547160398960113, eval err: 0.05315837860107422
[2m[36m(pid=13133)[0m 56, 
[2m[36m(pid=13133)[0m  train loss: 0.9344225833192468
[2m[36m(pid=13133)[0m  eval loss: 0.7464834666252136, eval err: 0.05278380870819092
[2m[36m(pid=13133)[0m 57, 
[2m[36m(pid=13133)[0m  train loss: 0.8399009676650167
Result for train_b1c14_00015:
  date: 2021-09-09_18-31-47
  done: true
  err: 0.05325212717056274
  experiment_id: a2d38f120d09461cbe7279f2467ce559
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 0.7500478219985962
  node_ip: 131.220.7.54
  pid: 13133
  should_checkpoint: true
  time_since_restore: 6234.390158653259
  time_this_iter_s: 320.6235702037811
  time_total_s: 6234.390158653259
  timestamp: 1631205107
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: b1c14_00015
  
[2m[36m(pid=13133)[0m  eval loss: 0.7500478219985962, eval err: 0.05325212717056274
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 ERROR, 9 PENDING, 1 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00015 | RUNNING    | 131.220.7.54:13133 |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00016 | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 ERROR)
Number of errored trials: 10
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13102)[0m Epoch:
[2m[36m(pid=13102)[0m 0, 
Result for train_b1c14_00016:
  {}
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 ERROR, 8 PENDING, 6 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00017 | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 PENDING, 4 ERROR)
Number of errored trials: 11
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13092)[0m Epoch:
[2m[36m(pid=13092)[0m 0, 
Result for train_b1c14_00017:
  {}
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 7 PENDING, 6 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | PENDING    |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |        |                  |           |           |
| train_b1c14_00019 | PENDING    |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |       |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m Epoch:
[2m[36m(pid=13099)[0m 0, 
[2m[36m(pid=13099)[0m  train loss: 29.81472736597061
[2m[36m(pid=13099)[0m  eval loss: 111.49086517333984, eval err: 0.9989480376243591
Result for train_b1c14_00018:
  date: 2021-09-09_18-33-49
  done: false
  err: 0.9989480376243591
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 111.49086517333984
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 99.87391757965088
  time_this_iter_s: 99.87391757965088
  time_total_s: 99.87391757965088
  timestamp: 1631205229
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |      1 |          99.8739 | 111.491    | 0.998948  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |            |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |            |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |            |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |            |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |            |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |            |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |        9802.53   |   0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |        4314.17   |  14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |        4282.03   |  10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |        3897.67   |   7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |        4316.23   |   9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |        6234.39   |   0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 1, 
[2m[36m(pid=13099)[0m  train loss: 30.518508076667786
[2m[36m(pid=13099)[0m  eval loss: 111.38762023925781, eval err: 0.9997433519363403
[2m[36m(pid=13099)[0m 2, 
[2m[36m(pid=13099)[0m  train loss: 24.055918276309967
[2m[36m(pid=13099)[0m  eval loss: 107.86838348388672, eval err: 0.9984283661842346
[2m[36m(pid=13099)[0m 3, 
[2m[36m(pid=13099)[0m  train loss: 17.22489419579506
[2m[36m(pid=13099)[0m  eval loss: 93.0406298828125, eval err: 0.9971662449836731
Result for train_b1c14_00018:
  date: 2021-09-09_18-38-39
  done: false
  err: 0.9971662449836731
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 93.0406298828125
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 390.08975291252136
  time_this_iter_s: 290.2158353328705
  time_total_s: 390.08975291252136
  timestamp: 1631205519
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |      2 |           390.09 | 93.0406   | 0.997166  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 4, 
[2m[36m(pid=13099)[0m  train loss: 14.741352885961533
[2m[36m(pid=13099)[0m  eval loss: 64.16636993408203, eval err: 0.9982923603057862
[2m[36m(pid=13099)[0m 5, 
[2m[36m(pid=13099)[0m  train loss: 13.35935115814209
[2m[36m(pid=13099)[0m  eval loss: 57.57236358642578, eval err: 0.9935349202156067
[2m[36m(pid=13099)[0m 6, 
[2m[36m(pid=13099)[0m  train loss: 13.527293294668198
[2m[36m(pid=13099)[0m  eval loss: 49.470605010986326, eval err: 0.9852635335922241
Result for train_b1c14_00018:
  date: 2021-09-09_18-43-28
  done: false
  err: 0.9852635335922241
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 49.470605010986326
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 678.9748091697693
  time_this_iter_s: 288.8850562572479
  time_total_s: 678.9748091697693
  timestamp: 1631205808
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |      3 |          678.975 | 49.4706   | 0.985264  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 7, 
[2m[36m(pid=13099)[0m  train loss: 12.897343248128891
[2m[36m(pid=13099)[0m  eval loss: 46.1428759765625, eval err: 0.9825714039802551
[2m[36m(pid=13099)[0m 8, 
[2m[36m(pid=13099)[0m  train loss: 12.128657817840576
[2m[36m(pid=13099)[0m  eval loss: 43.484160919189456, eval err: 0.980984377861023
[2m[36m(pid=13099)[0m 9, 
[2m[36m(pid=13099)[0m  train loss: 12.128222733736038
Result for train_b1c14_00018:
  date: 2021-09-09_18-48-17
  done: false
  err: 0.9882808160781861
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 51.72958648681641
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 967.742466211319
  time_this_iter_s: 288.7676570415497
  time_total_s: 967.742466211319
  timestamp: 1631206097
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00018
  
[2m[36m(pid=13099)[0m  eval loss: 51.72958648681641, eval err: 0.9882808160781861
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |      4 |          967.742 | 51.7296   | 0.988281  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 10, 
[2m[36m(pid=13099)[0m  train loss: 12.010345041751862
[2m[36m(pid=13099)[0m  eval loss: 29.604540939331056, eval err: 0.9427695608139038
[2m[36m(pid=13099)[0m 11, 
[2m[36m(pid=13099)[0m  train loss: 11.651684015989304
[2m[36m(pid=13099)[0m  eval loss: 33.123117218017576, eval err: 0.9714575743675232
[2m[36m(pid=13099)[0m 12, 
[2m[36m(pid=13099)[0m  train loss: 11.236975729465485
[2m[36m(pid=13099)[0m  eval loss: 37.775677871704104, eval err: 0.9751071977615356
Result for train_b1c14_00018:
  date: 2021-09-09_18-53-05
  done: false
  err: 0.9751071977615356
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 37.775677871704104
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 1256.017226934433
  time_this_iter_s: 288.274760723114
  time_total_s: 1256.017226934433
  timestamp: 1631206385
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |      5 |          1256.02 | 37.7757   | 0.975107  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 13, 
[2m[36m(pid=13099)[0m  train loss: 11.74110621213913
[2m[36m(pid=13099)[0m  eval loss: 40.58076950073242, eval err: 0.981960473060608
[2m[36m(pid=13099)[0m 14, 
[2m[36m(pid=13099)[0m  train loss: 11.633781045675278
[2m[36m(pid=13099)[0m  eval loss: 32.32337921142578, eval err: 0.9534565806388855
[2m[36m(pid=13099)[0m 15, 
[2m[36m(pid=13099)[0m  train loss: 11.244817227125168
[2m[36m(pid=13099)[0m  eval loss: 38.82068450927734, eval err: 0.9784892463684082
Result for train_b1c14_00018:
  date: 2021-09-09_18-57-54
  done: false
  err: 0.9784892463684082
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 38.82068450927734
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 1544.718586921692
  time_this_iter_s: 288.7013599872589
  time_total_s: 1544.718586921692
  timestamp: 1631206674
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |      6 |          1544.72 | 38.8207   | 0.978489  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 16, 
[2m[36m(pid=13099)[0m  train loss: 11.046179354190826
[2m[36m(pid=13099)[0m  eval loss: 36.95445556640625, eval err: 0.9793868947029114
[2m[36m(pid=13099)[0m 17, 
[2m[36m(pid=13099)[0m  train loss: 11.446302682161331
[2m[36m(pid=13099)[0m  eval loss: 33.74263023376465, eval err: 0.9521623563766479
[2m[36m(pid=13099)[0m 18, 
[2m[36m(pid=13099)[0m  train loss: 11.6793632209301
Result for train_b1c14_00018:
  date: 2021-09-09_19-02-42
  done: false
  err: 0.9349398803710938
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 30.166564254760743
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 1833.5398898124695
  time_this_iter_s: 288.8213028907776
  time_total_s: 1833.5398898124695
  timestamp: 1631206962
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00018
  
[2m[36m(pid=13099)[0m  eval loss: 30.166564254760743, eval err: 0.9349398803710938
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |      7 |          1833.54 | 30.1666   | 0.93494   |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 19, 
[2m[36m(pid=13099)[0m  train loss: 11.444324746727943
[2m[36m(pid=13099)[0m  eval loss: 31.31495635986328, eval err: 0.9453066945075989
[2m[36m(pid=13099)[0m 20, 
[2m[36m(pid=13099)[0m  train loss: 11.053746521472931
[2m[36m(pid=13099)[0m  eval loss: 29.46523712158203, eval err: 0.9355463075637818
[2m[36m(pid=13099)[0m 21, 
[2m[36m(pid=13099)[0m  train loss: 11.003883048892021
[2m[36m(pid=13099)[0m  eval loss: 29.715748443603516, eval err: 0.9348720383644104
Result for train_b1c14_00018:
  date: 2021-09-09_19-07-32
  done: false
  err: 0.9348720383644104
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 29.715748443603516
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 2122.9033122062683
  time_this_iter_s: 289.3634223937988
  time_total_s: 2122.9033122062683
  timestamp: 1631207252
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |      8 |          2122.9  | 29.7157   | 0.934872  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 22, 
[2m[36m(pid=13099)[0m  train loss: 11.388517677783966
[2m[36m(pid=13099)[0m  eval loss: 28.590349349975586, eval err: 0.9425907707214356
[2m[36m(pid=13099)[0m 23, 
[2m[36m(pid=13099)[0m  train loss: 10.68141183257103
[2m[36m(pid=13099)[0m  eval loss: 36.47912231445312, eval err: 0.960968599319458
[2m[36m(pid=13099)[0m 24, 
[2m[36m(pid=13099)[0m  train loss: 11.101862788200378
[2m[36m(pid=13099)[0m  eval loss: 38.57944808959961, eval err: 0.9775140833854675
Result for train_b1c14_00018:
  date: 2021-09-09_19-12-21
  done: false
  err: 0.9775140833854675
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 38.57944808959961
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 2411.769323348999
  time_this_iter_s: 288.8660111427307
  time_total_s: 2411.769323348999
  timestamp: 1631207541
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |      9 |          2411.77 | 38.5794   | 0.977514  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 25, 
[2m[36m(pid=13099)[0m  train loss: 11.294972881674767
[2m[36m(pid=13099)[0m  eval loss: 29.35283401489258, eval err: 0.9412483859062195
[2m[36m(pid=13099)[0m 26, 
[2m[36m(pid=13099)[0m  train loss: 10.79653862118721
[2m[36m(pid=13099)[0m  eval loss: 26.359052658081055, eval err: 0.9231489181518555
[2m[36m(pid=13099)[0m 27, 
[2m[36m(pid=13099)[0m  train loss: 11.38161726295948
[2m[36m(pid=13099)[0m  eval loss: 32.85561233520508, eval err: 0.9520745515823364
Result for train_b1c14_00018:
  date: 2021-09-09_19-17-10
  done: false
  err: 0.9520745515823364
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 32.85561233520508
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 2701.0766730308533
  time_this_iter_s: 289.30734968185425
  time_total_s: 2701.0766730308533
  timestamp: 1631207830
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     10 |          2701.08 | 32.8556   | 0.952075  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 28, 
[2m[36m(pid=13099)[0m  train loss: 10.882755100727081
[2m[36m(pid=13099)[0m  eval loss: 33.66354827880859, eval err: 0.9621121764183045
[2m[36m(pid=13099)[0m 29, 
[2m[36m(pid=13099)[0m  train loss: 10.880207389593124
[2m[36m(pid=13099)[0m  eval loss: 24.60282783508301, eval err: 0.9172975015640259
[2m[36m(pid=13099)[0m 30, 
[2m[36m(pid=13099)[0m  train loss: 10.920484974980354
[2m[36m(pid=13099)[0m  eval loss: 23.145556182861327, eval err: 0.9176291632652283
Result for train_b1c14_00018:
  date: 2021-09-09_19-21-59
  done: false
  err: 0.9176291632652283
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 23.145556182861327
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 2990.281017065048
  time_this_iter_s: 289.20434403419495
  time_total_s: 2990.281017065048
  timestamp: 1631208119
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     11 |          2990.28 | 23.1456   | 0.917629  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 31, 
[2m[36m(pid=13099)[0m  train loss: 10.26428073644638
[2m[36m(pid=13099)[0m  eval loss: 21.61201591491699, eval err: 0.9224236750602722
[2m[36m(pid=13099)[0m 32, 
[2m[36m(pid=13099)[0m  train loss: 11.20441497862339
[2m[36m(pid=13099)[0m  eval loss: 30.77667251586914, eval err: 0.9616429567337036
[2m[36m(pid=13099)[0m 33, 
[2m[36m(pid=13099)[0m  train loss: 10.956894621253014
[2m[36m(pid=13099)[0m  eval loss: 31.808596954345703, eval err: 0.9536662530899048
Result for train_b1c14_00018:
  date: 2021-09-09_19-26-48
  done: false
  err: 0.9536662530899048
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 31.808596954345703
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 3279.5007450580597
  time_this_iter_s: 289.2197279930115
  time_total_s: 3279.5007450580597
  timestamp: 1631208408
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     12 |          3279.5  | 31.8086   | 0.953666  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 34, 
[2m[36m(pid=13099)[0m  train loss: 11.134675860404968
[2m[36m(pid=13099)[0m  eval loss: 30.235276260375976, eval err: 0.9535695695877076
[2m[36m(pid=13099)[0m 35, 
[2m[36m(pid=13099)[0m  train loss: 10.869693800807
[2m[36m(pid=13099)[0m  eval loss: 32.644586410522464, eval err: 0.9717446422576904
[2m[36m(pid=13099)[0m 36, 
[2m[36m(pid=13099)[0m  train loss: 10.655127882957458
[2m[36m(pid=13099)[0m  eval loss: 23.234893646240234, eval err: 0.9136829280853271
Result for train_b1c14_00018:
  date: 2021-09-09_19-31-37
  done: false
  err: 0.9136829280853271
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 23.234893646240234
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 3568.6129655838013
  time_this_iter_s: 289.1122205257416
  time_total_s: 3568.6129655838013
  timestamp: 1631208697
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     13 |          3568.61 | 23.2349   | 0.913683  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13099)[0m 37, 
[2m[36m(pid=13099)[0m  train loss: 10.412142246961594
[2m[36m(pid=13099)[0m  eval loss: 20.77096305847168, eval err: 0.9142610359191895
[2m[36m(pid=13099)[0m 38, 
[2m[36m(pid=13099)[0m  train loss: 10.70931538939476
[2m[36m(pid=13099)[0m  eval loss: 23.08539680480957, eval err: 0.92331960439682
[2m[36m(pid=13099)[0m 39, 
[2m[36m(pid=13099)[0m  train loss: 10.404549047350883
[2m[36m(pid=13099)[0m  eval loss: 20.90916130065918, eval err: 0.9179365444183349
Result for train_b1c14_00018:
  date: 2021-09-09_19-36-27
  done: true
  err: 0.9179365444183349
  experiment_id: 7018ec2029a3421e8a6c162290bd81bf
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 20.90916130065918
  node_ip: 131.220.7.54
  pid: 13099
  should_checkpoint: true
  time_since_restore: 3857.94593667984
  time_this_iter_s: 289.3329710960388
  time_total_s: 3857.94593667984
  timestamp: 1631208987
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00018
  
== Status ==
Memory usage on this node: 14.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 6 PENDING, 1 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00018 | RUNNING    | 131.220.7.54:13099 |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | PENDING    |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |        |                  |           |           |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m Epoch:
[2m[36m(pid=13088)[0m 0, 
[2m[36m(pid=13088)[0m  train loss: 30.376674711704254
[2m[36m(pid=13088)[0m  eval loss: 60.37044387817383, eval err: 0.9584088182449341
Result for train_b1c14_00019:
  date: 2021-09-09_19-38-22
  done: false
  err: 0.9584088182449341
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 60.37044387817383
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 113.77314949035645
  time_this_iter_s: 113.77314949035645
  time_total_s: 113.77314949035645
  timestamp: 1631209102
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |      1 |          113.773 | 60.3704   | 0.958409  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 1, 
[2m[36m(pid=13088)[0m  train loss: 15.605668783187866
[2m[36m(pid=13088)[0m  eval loss: 11.999242210388184, eval err: 0.8398467564582824
[2m[36m(pid=13088)[0m 2, 
[2m[36m(pid=13088)[0m  train loss: 11.635084837675095
[2m[36m(pid=13088)[0m  eval loss: 12.556420097351074, eval err: 0.8525468826293945
[2m[36m(pid=13088)[0m 3, 
[2m[36m(pid=13088)[0m  train loss: 10.899559795856476
[2m[36m(pid=13088)[0m  eval loss: 10.16799165725708, eval err: 0.8116376113891601
Result for train_b1c14_00019:
  date: 2021-09-09_19-43-55
  done: false
  err: 0.8116376113891601
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 10.16799165725708
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 445.95233249664307
  time_this_iter_s: 332.1791830062866
  time_total_s: 445.95233249664307
  timestamp: 1631209435
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |      2 |          445.952 | 10.168    | 0.811638  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 4, 
[2m[36m(pid=13088)[0m  train loss: 10.035555392503738
[2m[36m(pid=13088)[0m  eval loss: 10.679884243011475, eval err: 0.8267030906677246
[2m[36m(pid=13088)[0m 5, 
[2m[36m(pid=13088)[0m  train loss: 8.697357952594757
[2m[36m(pid=13088)[0m  eval loss: 11.215800514221192, eval err: 0.8385664391517639
[2m[36m(pid=13088)[0m 6, 
[2m[36m(pid=13088)[0m  train loss: 8.612608954310417
Result for train_b1c14_00019:
  date: 2021-09-09_19-49-27
  done: false
  err: 0.8282208752632141
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 11.210313110351562
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 777.9489481449127
  time_this_iter_s: 331.99661564826965
  time_total_s: 777.9489481449127
  timestamp: 1631209767
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00019
  
[2m[36m(pid=13088)[0m  eval loss: 11.210313110351562, eval err: 0.8282208752632141
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |      3 |          777.949 | 11.2103   | 0.828221  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 7, 
[2m[36m(pid=13088)[0m  train loss: 9.159569665789604
[2m[36m(pid=13088)[0m  eval loss: 8.385333709716797, eval err: 0.7810523653030396
[2m[36m(pid=13088)[0m 8, 
[2m[36m(pid=13088)[0m  train loss: 9.091041654348373
[2m[36m(pid=13088)[0m  eval loss: 8.761308994293213, eval err: 0.7734552574157715
[2m[36m(pid=13088)[0m 9, 
[2m[36m(pid=13088)[0m  train loss: 8.306758493185043
[2m[36m(pid=13088)[0m  eval loss: 9.292299251556397, eval err: 0.8073476719856262
Result for train_b1c14_00019:
  date: 2021-09-09_19-54-58
  done: false
  err: 0.8073476719856262
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 9.292299251556397
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 1109.754585981369
  time_this_iter_s: 331.8056378364563
  time_total_s: 1109.754585981369
  timestamp: 1631210098
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |      4 |          1109.75 |  9.2923   | 0.807348  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 10, 
[2m[36m(pid=13088)[0m  train loss: 7.601921573281288
[2m[36m(pid=13088)[0m  eval loss: 8.725020751953124, eval err: 0.7750730419158935
[2m[36m(pid=13088)[0m 11, 
[2m[36m(pid=13088)[0m  train loss: 7.820710718631744
[2m[36m(pid=13088)[0m  eval loss: 8.906289253234863, eval err: 0.7758697700500489
[2m[36m(pid=13088)[0m 12, 
[2m[36m(pid=13088)[0m  train loss: 7.506268724799156
Result for train_b1c14_00019:
  date: 2021-09-09_20-00-30
  done: false
  err: 0.7460583281517029
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 7.913345832824707
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 1441.268601179123
  time_this_iter_s: 331.5140151977539
  time_total_s: 1441.268601179123
  timestamp: 1631210430
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00019
  
[2m[36m(pid=13088)[0m  eval loss: 7.913345832824707, eval err: 0.7460583281517029
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |      5 |          1441.27 |  7.91335  | 0.746058  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 13, 
[2m[36m(pid=13088)[0m  train loss: 7.424058705568314
[2m[36m(pid=13088)[0m  eval loss: 8.04786678314209, eval err: 0.7406614089012146
[2m[36m(pid=13088)[0m 14, 
[2m[36m(pid=13088)[0m  train loss: 6.925053372979164
[2m[36m(pid=13088)[0m  eval loss: 7.057281017303467, eval err: 0.6910689663887024
[2m[36m(pid=13088)[0m 15, 
[2m[36m(pid=13088)[0m  train loss: 6.028387784957886
[2m[36m(pid=13088)[0m  eval loss: 7.0296662902832034, eval err: 0.6622112202644348
Result for train_b1c14_00019:
  date: 2021-09-09_20-06-01
  done: false
  err: 0.6622112202644348
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 7.0296662902832034
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 1772.632973432541
  time_this_iter_s: 331.36437225341797
  time_total_s: 1772.632973432541
  timestamp: 1631210761
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |      6 |          1772.63 |  7.02967  | 0.662211  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 16, 
[2m[36m(pid=13088)[0m  train loss: 5.422400660812855
[2m[36m(pid=13088)[0m  eval loss: 5.611956081390381, eval err: 0.5642143201828003
[2m[36m(pid=13088)[0m 17, 
[2m[36m(pid=13088)[0m  train loss: 4.906050011515617
[2m[36m(pid=13088)[0m  eval loss: 5.310110168457031, eval err: 0.5106177997589111
[2m[36m(pid=13088)[0m 18, 
[2m[36m(pid=13088)[0m  train loss: 4.209558539092541
[2m[36m(pid=13088)[0m  eval loss: 3.2961447429656983, eval err: 0.3615432286262512
Result for train_b1c14_00019:
  date: 2021-09-09_20-11-33
  done: false
  err: 0.3615432286262512
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 3.2961447429656983
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 2104.4705963134766
  time_this_iter_s: 331.83762288093567
  time_total_s: 2104.4705963134766
  timestamp: 1631211093
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |      7 |          2104.47 |  3.29614  | 0.361543  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 19, 
[2m[36m(pid=13088)[0m  train loss: 4.060912609100342
[2m[36m(pid=13088)[0m  eval loss: 3.5880033016204833, eval err: 0.33723578214645383
[2m[36m(pid=13088)[0m 20, 
[2m[36m(pid=13088)[0m  train loss: 3.2560972943902016
[2m[36m(pid=13088)[0m  eval loss: 3.345993595123291, eval err: 0.3158699989318848
[2m[36m(pid=13088)[0m 21, 
[2m[36m(pid=13088)[0m  train loss: 3.061600662767887
[2m[36m(pid=13088)[0m  eval loss: 2.4488107538223267, eval err: 0.24641944885253905
Result for train_b1c14_00019:
  date: 2021-09-09_20-17-05
  done: false
  err: 0.24641944885253905
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 2.4488107538223267
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 2436.211454629898
  time_this_iter_s: 331.7408583164215
  time_total_s: 2436.211454629898
  timestamp: 1631211425
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |      8 |          2436.21 |  2.44881  | 0.246419  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 22, 
[2m[36m(pid=13088)[0m  train loss: 2.904983937740326
[2m[36m(pid=13088)[0m  eval loss: 2.216097536087036, eval err: 0.2240304946899414
[2m[36m(pid=13088)[0m 23, 
[2m[36m(pid=13088)[0m  train loss: 2.447756841778755
[2m[36m(pid=13088)[0m  eval loss: 2.091107382774353, eval err: 0.20824442386627198
[2m[36m(pid=13088)[0m 24, 
[2m[36m(pid=13088)[0m  train loss: 2.224537953734398
[2m[36m(pid=13088)[0m  eval loss: 1.7626907753944396, eval err: 0.16542326927185058
Result for train_b1c14_00019:
  date: 2021-09-09_20-22-37
  done: false
  err: 0.16542326927185058
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 1.7626907753944396
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 2768.396238565445
  time_this_iter_s: 332.1847839355469
  time_total_s: 2768.396238565445
  timestamp: 1631211757
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |      9 |          2768.4  |  1.76269  | 0.165423  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 25, 
[2m[36m(pid=13088)[0m  train loss: 1.9310657866299152
[2m[36m(pid=13088)[0m  eval loss: 1.7330895137786866, eval err: 0.1642276382446289
[2m[36m(pid=13088)[0m 26, 
[2m[36m(pid=13088)[0m  train loss: 2.0364455468952656
[2m[36m(pid=13088)[0m  eval loss: 1.5792638874053955, eval err: 0.14604451656341552
[2m[36m(pid=13088)[0m 27, 
[2m[36m(pid=13088)[0m  train loss: 1.9417353384196758
[2m[36m(pid=13088)[0m  eval loss: 1.607437288761139, eval err: 0.14514821290969848
Result for train_b1c14_00019:
  date: 2021-09-09_20-28-09
  done: false
  err: 0.14514821290969848
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 1.607437288761139
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 3100.695969104767
  time_this_iter_s: 332.2997305393219
  time_total_s: 3100.695969104767
  timestamp: 1631212089
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     10 |          3100.7  |  1.60744  | 0.145148  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 28, 
[2m[36m(pid=13088)[0m  train loss: 1.8595172204077244
[2m[36m(pid=13088)[0m  eval loss: 1.4492104148864746, eval err: 0.12422869205474854
[2m[36m(pid=13088)[0m 29, 
[2m[36m(pid=13088)[0m  train loss: 1.7280128374695778
[2m[36m(pid=13088)[0m  eval loss: 1.289241406917572, eval err: 0.112305748462677
[2m[36m(pid=13088)[0m 30, 
[2m[36m(pid=13088)[0m  train loss: 1.6810167226940393
Result for train_b1c14_00019:
  date: 2021-09-09_20-33-42
  done: false
  err: 0.1095937442779541
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 1.3015339994430541
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 3433.047470331192
  time_this_iter_s: 332.35150122642517
  time_total_s: 3433.047470331192
  timestamp: 1631212422
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00019
  
[2m[36m(pid=13088)[0m  eval loss: 1.3015339994430541, eval err: 0.1095937442779541
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     11 |          3433.05 |  1.30153  | 0.109594  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 31, 
[2m[36m(pid=13088)[0m  train loss: 1.7715109717100859
[2m[36m(pid=13088)[0m  eval loss: 1.2313294672966004, eval err: 0.10183279037475586
[2m[36m(pid=13088)[0m 32, 
[2m[36m(pid=13088)[0m  train loss: 1.7438293807208538
[2m[36m(pid=13088)[0m  eval loss: 1.1828094005584717, eval err: 0.09728366851806641
[2m[36m(pid=13088)[0m 33, 
[2m[36m(pid=13088)[0m  train loss: 1.7048883847892284
[2m[36m(pid=13088)[0m  eval loss: 1.2498008227348327, eval err: 0.10551640033721923
Result for train_b1c14_00019:
  date: 2021-09-09_20-39-14
  done: false
  err: 0.10551640033721923
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 1.2498008227348327
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 3765.538993358612
  time_this_iter_s: 332.49152302742004
  time_total_s: 3765.538993358612
  timestamp: 1631212754
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     12 |          3765.54 |  1.2498   | 0.105516  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 34, 
[2m[36m(pid=13088)[0m  train loss: 1.5875945314764977
[2m[36m(pid=13088)[0m  eval loss: 1.1547556471824647, eval err: 0.09531898260116577
[2m[36m(pid=13088)[0m 35, 
[2m[36m(pid=13088)[0m  train loss: 1.5231421049684286
[2m[36m(pid=13088)[0m  eval loss: 1.068100345134735, eval err: 0.08493858814239502
[2m[36m(pid=13088)[0m 36, 
[2m[36m(pid=13088)[0m  train loss: 1.3672893531620502
Result for train_b1c14_00019:
  date: 2021-09-09_20-44-46
  done: false
  err: 0.08790306091308593
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 1.0906539154052735
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 4097.762840986252
  time_this_iter_s: 332.22384762763977
  time_total_s: 4097.762840986252
  timestamp: 1631213086
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00019
  
[2m[36m(pid=13088)[0m  eval loss: 1.0906539154052735, eval err: 0.08790306091308593
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     13 |          4097.76 |  1.09065  | 0.0879031 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 37, 
[2m[36m(pid=13088)[0m  train loss: 1.424432722851634
[2m[36m(pid=13088)[0m  eval loss: 1.0849349188804627, eval err: 0.08330347299575806
[2m[36m(pid=13088)[0m 38, 
[2m[36m(pid=13088)[0m  train loss: 1.4269603714346886
[2m[36m(pid=13088)[0m  eval loss: 1.1252817606925964, eval err: 0.08938270092010497
[2m[36m(pid=13088)[0m 39, 
[2m[36m(pid=13088)[0m  train loss: 1.3402977082878351
[2m[36m(pid=13088)[0m  eval loss: 1.0634569311141968, eval err: 0.08416853666305542
Result for train_b1c14_00019:
  date: 2021-09-09_20-50-19
  done: false
  err: 0.08416853666305542
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 1.0634569311141968
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 4430.224825143814
  time_this_iter_s: 332.46198415756226
  time_total_s: 4430.224825143814
  timestamp: 1631213419
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     14 |          4430.22 |  1.06346  | 0.0841685 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 40, 
[2m[36m(pid=13088)[0m  train loss: 1.4726844299584627
[2m[36m(pid=13088)[0m  eval loss: 1.0902927470207215, eval err: 0.08369731664657593
[2m[36m(pid=13088)[0m 41, 
[2m[36m(pid=13088)[0m  train loss: 1.3219769150018692
[2m[36m(pid=13088)[0m  eval loss: 0.9964780926704406, eval err: 0.07695249319076539
[2m[36m(pid=13088)[0m 42, 
[2m[36m(pid=13088)[0m  train loss: 1.176905645057559
[2m[36m(pid=13088)[0m  eval loss: 0.9766618251800537, eval err: 0.0762856936454773
Result for train_b1c14_00019:
  date: 2021-09-09_20-55-51
  done: false
  err: 0.0762856936454773
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 0.9766618251800537
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 4762.212003946304
  time_this_iter_s: 331.98717880249023
  time_total_s: 4762.212003946304
  timestamp: 1631213751
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     15 |          4762.21 |  0.976662 | 0.0762857 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 43, 
[2m[36m(pid=13088)[0m  train loss: 1.2442414853721857
[2m[36m(pid=13088)[0m  eval loss: 0.9769858360290528, eval err: 0.07489838361740113
[2m[36m(pid=13088)[0m 44, 
[2m[36m(pid=13088)[0m  train loss: 1.3238398507237434
[2m[36m(pid=13088)[0m  eval loss: 1.2807174921035767, eval err: 0.09693063020706177
[2m[36m(pid=13088)[0m 45, 
[2m[36m(pid=13088)[0m  train loss: 1.453597605228424
[2m[36m(pid=13088)[0m  eval loss: 0.9474288856983185, eval err: 0.07336302757263184
Result for train_b1c14_00019:
  date: 2021-09-09_21-01-23
  done: false
  err: 0.07336302757263184
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 0.9474288856983185
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 5094.485580444336
  time_this_iter_s: 332.2735764980316
  time_total_s: 5094.485580444336
  timestamp: 1631214083
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     16 |          5094.49 |  0.947429 | 0.073363  |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 46, 
[2m[36m(pid=13088)[0m  train loss: 1.2315687872469425
[2m[36m(pid=13088)[0m  eval loss: 0.9771575284004211, eval err: 0.07571776866912842
[2m[36m(pid=13088)[0m 47, 
[2m[36m(pid=13088)[0m  train loss: 1.2069422081112862
[2m[36m(pid=13088)[0m  eval loss: 0.9192976844310761, eval err: 0.06843062162399292
[2m[36m(pid=13088)[0m 48, 
[2m[36m(pid=13088)[0m  train loss: 1.2992141917347908
[2m[36m(pid=13088)[0m  eval loss: 0.9302568972110749, eval err: 0.07089529991149902
Result for train_b1c14_00019:
  date: 2021-09-09_21-06-55
  done: false
  err: 0.07089529991149902
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 0.9302568972110749
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 5426.730342626572
  time_this_iter_s: 332.2447621822357
  time_total_s: 5426.730342626572
  timestamp: 1631214415
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     17 |          5426.73 |  0.930257 | 0.0708953 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 49, 
[2m[36m(pid=13088)[0m  train loss: 1.1052028695121408
[2m[36m(pid=13088)[0m  eval loss: 0.9440734732151032, eval err: 0.07077936887741089
[2m[36m(pid=13088)[0m 50, 
[2m[36m(pid=13088)[0m  train loss: 1.3482878971844912
[2m[36m(pid=13088)[0m  eval loss: 0.9221517288684845, eval err: 0.06907641410827636
[2m[36m(pid=13088)[0m 51, 
[2m[36m(pid=13088)[0m  train loss: 1.1289418619126081
[2m[36m(pid=13088)[0m  eval loss: 0.8796086776256561, eval err: 0.06475012540817261
Result for train_b1c14_00019:
  date: 2021-09-09_21-12-28
  done: false
  err: 0.06475012540817261
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 0.8796086776256561
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 5758.9471299648285
  time_this_iter_s: 332.21678733825684
  time_total_s: 5758.9471299648285
  timestamp: 1631214748
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     18 |          5758.95 |  0.879609 | 0.0647501 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 52, 
[2m[36m(pid=13088)[0m  train loss: 1.1827198658138514
[2m[36m(pid=13088)[0m  eval loss: 0.9181684362888336, eval err: 0.06839402675628663
[2m[36m(pid=13088)[0m 53, 
[2m[36m(pid=13088)[0m  train loss: 1.2106808833777905
[2m[36m(pid=13088)[0m  eval loss: 0.9117919373512268, eval err: 0.06548332452774047
[2m[36m(pid=13088)[0m 54, 
[2m[36m(pid=13088)[0m  train loss: 1.1287897862493992
[2m[36m(pid=13088)[0m  eval loss: 0.9139922666549682, eval err: 0.06788886070251465
Result for train_b1c14_00019:
  date: 2021-09-09_21-18-00
  done: false
  err: 0.06788886070251465
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 0.9139922666549682
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 6091.315078735352
  time_this_iter_s: 332.36794877052307
  time_total_s: 6091.315078735352
  timestamp: 1631215080
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     19 |          6091.32 |  0.913992 | 0.0678889 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 55, 
[2m[36m(pid=13088)[0m  train loss: 1.1239856583997607
[2m[36m(pid=13088)[0m  eval loss: 0.9203351676464081, eval err: 0.06901982307434082
[2m[36m(pid=13088)[0m 56, 
[2m[36m(pid=13088)[0m  train loss: 1.172143790870905
[2m[36m(pid=13088)[0m  eval loss: 0.8554611217975616, eval err: 0.06352220296859741
[2m[36m(pid=13088)[0m 57, 
[2m[36m(pid=13088)[0m  train loss: 1.1062619518488646
[2m[36m(pid=13088)[0m  eval loss: 0.8741913485527039, eval err: 0.06412621498107911
Result for train_b1c14_00019:
  date: 2021-09-09_21-23-32
  done: false
  err: 0.06412621498107911
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 0.8741913485527039
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 6423.547481060028
  time_this_iter_s: 332.2324023246765
  time_total_s: 6423.547481060028
  timestamp: 1631215412
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     20 |          6423.55 |  0.874191 | 0.0641262 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 58, 
[2m[36m(pid=13088)[0m  train loss: 1.1366525311022997
[2m[36m(pid=13088)[0m  eval loss: 0.9328756988048553, eval err: 0.06623944044113159
[2m[36m(pid=13088)[0m 59, 
[2m[36m(pid=13088)[0m  train loss: 1.1040345672518015
[2m[36m(pid=13088)[0m  eval loss: 0.8287616348266602, eval err: 0.059881067276000975
[2m[36m(pid=13088)[0m 60, 
[2m[36m(pid=13088)[0m  train loss: 0.9855396170169115
Result for train_b1c14_00019:
  date: 2021-09-09_21-29-04
  done: false
  err: 0.05972462415695191
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 0.81822558760643
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 6755.617343187332
  time_this_iter_s: 332.0698621273041
  time_total_s: 6755.617343187332
  timestamp: 1631215744
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     21 |          6755.62 |  0.818226 | 0.0597246 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m  eval loss: 0.81822558760643, eval err: 0.05972462415695191
[2m[36m(pid=13088)[0m 61, 
[2m[36m(pid=13088)[0m  train loss: 1.032184187322855
[2m[36m(pid=13088)[0m  eval loss: 0.8022223639488221, eval err: 0.05788062334060669
[2m[36m(pid=13088)[0m 62, 
[2m[36m(pid=13088)[0m  train loss: 1.0232999557629228
[2m[36m(pid=13088)[0m  eval loss: 0.8032651591300964, eval err: 0.05898822546005249
[2m[36m(pid=13088)[0m 63, 
[2m[36m(pid=13088)[0m  train loss: 1.096496518701315
[2m[36m(pid=13088)[0m  eval loss: 0.8022121977806091, eval err: 0.05882785081863404
Result for train_b1c14_00019:
  date: 2021-09-09_21-34-37
  done: false
  err: 0.05882785081863404
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 22
  loss: 0.8022121977806091
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 7088.023168325424
  time_this_iter_s: 332.40582513809204
  time_total_s: 7088.023168325424
  timestamp: 1631216077
  timesteps_since_restore: 0
  training_iteration: 22
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     22 |          7088.02 |  0.802212 | 0.0588279 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 64, 
[2m[36m(pid=13088)[0m  train loss: 1.0429242122918367
[2m[36m(pid=13088)[0m  eval loss: 0.7882461607456207, eval err: 0.05718762397766113
[2m[36m(pid=13088)[0m 65, 
[2m[36m(pid=13088)[0m  train loss: 1.008617701008916
[2m[36m(pid=13088)[0m  eval loss: 0.7856633496284485, eval err: 0.05722109079360962
[2m[36m(pid=13088)[0m 66, 
[2m[36m(pid=13088)[0m  train loss: 0.9354961160570383
[2m[36m(pid=13088)[0m  eval loss: 0.7783208727836609, eval err: 0.055964434146881105
Result for train_b1c14_00019:
  date: 2021-09-09_21-40-09
  done: false
  err: 0.055964434146881105
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 23
  loss: 0.7783208727836609
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 7420.664395570755
  time_this_iter_s: 332.6412272453308
  time_total_s: 7420.664395570755
  timestamp: 1631216409
  timesteps_since_restore: 0
  training_iteration: 23
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     23 |          7420.66 |  0.778321 | 0.0559644 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13088)[0m 67, 
[2m[36m(pid=13088)[0m  train loss: 1.0178877795115113
[2m[36m(pid=13088)[0m  eval loss: 0.7941834473609924, eval err: 0.05754302024841309
[2m[36m(pid=13088)[0m 68, 
[2m[36m(pid=13088)[0m  train loss: 0.8699250686913729
[2m[36m(pid=13088)[0m  eval loss: 0.7728399229049683, eval err: 0.05568053722381592
[2m[36m(pid=13088)[0m 69, 
[2m[36m(pid=13088)[0m  train loss: 1.0257086884230375
[2m[36m(pid=13088)[0m  eval loss: 0.790129725933075, eval err: 0.05723456382751465
Result for train_b1c14_00019:
  date: 2021-09-09_21-45-42
  done: true
  err: 0.05723456382751465
  experiment_id: 8ccbd3d8d0bd4f4391dd72db386709fa
  hostname: bigcuda4
  iterations_since_restore: 24
  loss: 0.790129725933075
  node_ip: 131.220.7.54
  pid: 13088
  should_checkpoint: true
  time_since_restore: 7753.022480249405
  time_this_iter_s: 332.3580846786499
  time_total_s: 7753.022480249405
  timestamp: 1631216742
  timesteps_since_restore: 0
  training_iteration: 24
  trial_id: b1c14_00019
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 5 PENDING, 1 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00019 | RUNNING    | 131.220.7.54:13088 |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | PENDING    |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |        |                  |           |           |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (5 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m Epoch:
[2m[36m(pid=13124)[0m 0, 
[2m[36m(pid=13124)[0m  train loss: 33.8620588183403
[2m[36m(pid=13124)[0m  eval loss: 119.04199981689453, eval err: 0.9998529291152954
Result for train_b1c14_00020:
  date: 2021-09-09_21-47-33
  done: false
  err: 0.9998529291152954
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 119.04199981689453
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 109.80207252502441
  time_this_iter_s: 109.80207252502441
  time_total_s: 109.80207252502441
  timestamp: 1631216853
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |      1 |          109.802 | 119.042    | 0.999853  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |            |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |            |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |            |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |            |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  |  20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |   0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |            |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 1, 
[2m[36m(pid=13124)[0m  train loss: 32.084947764873505
[2m[36m(pid=13124)[0m  eval loss: 117.18903137207032, eval err: 0.9997829985618591
[2m[36m(pid=13124)[0m 2, 
[2m[36m(pid=13124)[0m  train loss: 28.39795309305191
[2m[36m(pid=13124)[0m  eval loss: 119.92041778564453, eval err: 0.9997867655754089
[2m[36m(pid=13124)[0m 3, 
[2m[36m(pid=13124)[0m  train loss: 22.789308845996857
[2m[36m(pid=13124)[0m  eval loss: 119.16016510009766, eval err: 0.9996634268760681
Result for train_b1c14_00020:
  date: 2021-09-09_21-52-54
  done: false
  err: 0.9996634268760681
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 119.16016510009766
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 430.18454694747925
  time_this_iter_s: 320.38247442245483
  time_total_s: 430.18454694747925
  timestamp: 1631217174
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |      2 |          430.185 | 119.16     | 0.999663  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |            |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |            |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |            |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |            |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  |  20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |   0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |            |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 4, 
[2m[36m(pid=13124)[0m  train loss: 15.636810541152954
[2m[36m(pid=13124)[0m  eval loss: 104.36221252441406, eval err: 0.9954667329788208
[2m[36m(pid=13124)[0m 5, 
[2m[36m(pid=13124)[0m  train loss: 13.317977786064148
[2m[36m(pid=13124)[0m  eval loss: 71.99308044433593, eval err: 0.996354877948761
[2m[36m(pid=13124)[0m 6, 
[2m[36m(pid=13124)[0m  train loss: 13.532226085662842
[2m[36m(pid=13124)[0m  eval loss: 44.918756561279295, eval err: 0.9792516732215881
Result for train_b1c14_00020:
  date: 2021-09-09_21-58-14
  done: false
  err: 0.9792516732215881
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 44.918756561279295
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 750.3327984809875
  time_this_iter_s: 320.1482515335083
  time_total_s: 750.3327984809875
  timestamp: 1631217494
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |      3 |          750.333 | 44.9188   | 0.979252  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 7, 
[2m[36m(pid=13124)[0m  train loss: 13.052468866109848
[2m[36m(pid=13124)[0m  eval loss: 44.67962127685547, eval err: 0.9849698185920716
[2m[36m(pid=13124)[0m 8, 
[2m[36m(pid=13124)[0m  train loss: 12.241209357976913
[2m[36m(pid=13124)[0m  eval loss: 33.415594329833986, eval err: 0.9495266008377076
[2m[36m(pid=13124)[0m 9, 
[2m[36m(pid=13124)[0m  train loss: 12.344096690416336
[2m[36m(pid=13124)[0m  eval loss: 40.48694946289063, eval err: 0.982501609325409
Result for train_b1c14_00020:
  date: 2021-09-09_22-03-34
  done: false
  err: 0.982501609325409
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 40.48694946289063
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 1070.5918669700623
  time_this_iter_s: 320.2590684890747
  time_total_s: 1070.5918669700623
  timestamp: 1631217814
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |      4 |          1070.59 | 40.4869   | 0.982502  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 10, 
[2m[36m(pid=13124)[0m  train loss: 12.670752763748169
[2m[36m(pid=13124)[0m  eval loss: 45.72563552856445, eval err: 0.9864106154441834
[2m[36m(pid=13124)[0m 11, 
[2m[36m(pid=13124)[0m  train loss: 12.689877986907959
[2m[36m(pid=13124)[0m  eval loss: 48.79335723876953, eval err: 0.9872170257568359
[2m[36m(pid=13124)[0m 12, 
[2m[36m(pid=13124)[0m  train loss: 12.432657927274704
Result for train_b1c14_00020:
  date: 2021-09-09_22-08-54
  done: false
  err: 0.978420524597168
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 36.845916290283206
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 1390.5151226520538
  time_this_iter_s: 319.9232556819916
  time_total_s: 1390.5151226520538
  timestamp: 1631218134
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00020
  
[2m[36m(pid=13124)[0m  eval loss: 36.845916290283206, eval err: 0.978420524597168
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |      5 |          1390.52 | 36.8459   | 0.978421  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 13, 
[2m[36m(pid=13124)[0m  train loss: 11.948428213596344
[2m[36m(pid=13124)[0m  eval loss: 37.58751388549805, eval err: 0.9836708760261536
[2m[36m(pid=13124)[0m 14, 
[2m[36m(pid=13124)[0m  train loss: 12.318626314401627
[2m[36m(pid=13124)[0m  eval loss: 42.02937973022461, eval err: 0.9846532154083252
[2m[36m(pid=13124)[0m 15, 
[2m[36m(pid=13124)[0m  train loss: 12.017699509859085
[2m[36m(pid=13124)[0m  eval loss: 28.789726028442384, eval err: 0.9629188632965088
Result for train_b1c14_00020:
  date: 2021-09-09_22-14-14
  done: false
  err: 0.9629188632965088
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 28.789726028442384
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 1710.3576731681824
  time_this_iter_s: 319.84255051612854
  time_total_s: 1710.3576731681824
  timestamp: 1631218454
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |      6 |          1710.36 | 28.7897   | 0.962919  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 16, 
[2m[36m(pid=13124)[0m  train loss: 11.472646117210388
[2m[36m(pid=13124)[0m  eval loss: 38.014019622802735, eval err: 0.9832697105407715
[2m[36m(pid=13124)[0m 17, 
[2m[36m(pid=13124)[0m  train loss: 11.791401028633118
[2m[36m(pid=13124)[0m  eval loss: 23.88456283569336, eval err: 0.9269695901870727
[2m[36m(pid=13124)[0m 18, 
[2m[36m(pid=13124)[0m  train loss: 11.077509820461273
[2m[36m(pid=13124)[0m  eval loss: 26.07651252746582, eval err: 0.9344678497314454
Result for train_b1c14_00020:
  date: 2021-09-09_22-19-34
  done: false
  err: 0.9344678497314454
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 26.07651252746582
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 2030.0050909519196
  time_this_iter_s: 319.6474177837372
  time_total_s: 2030.0050909519196
  timestamp: 1631218774
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |      7 |          2030.01 | 26.0765   | 0.934468  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 19, 
[2m[36m(pid=13124)[0m  train loss: 11.489851832389832
[2m[36m(pid=13124)[0m  eval loss: 29.402958602905272, eval err: 0.9738320946693421
[2m[36m(pid=13124)[0m 20, 
[2m[36m(pid=13124)[0m  train loss: 11.729137778282166
[2m[36m(pid=13124)[0m  eval loss: 34.50472190856934, eval err: 0.9819209551811219
[2m[36m(pid=13124)[0m 21, 
[2m[36m(pid=13124)[0m  train loss: 11.60877376794815
[2m[36m(pid=13124)[0m  eval loss: 34.110680465698245, eval err: 0.9812960720062256
Result for train_b1c14_00020:
  date: 2021-09-09_22-24-53
  done: false
  err: 0.9812960720062256
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 34.110680465698245
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 2349.5746257305145
  time_this_iter_s: 319.56953477859497
  time_total_s: 2349.5746257305145
  timestamp: 1631219093
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |      8 |          2349.57 | 34.1107   | 0.981296  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 22, 
[2m[36m(pid=13124)[0m  train loss: 11.278320133686066
[2m[36m(pid=13124)[0m  eval loss: 44.255565032958984, eval err: 0.9870957326889038
[2m[36m(pid=13124)[0m 23, 
[2m[36m(pid=13124)[0m  train loss: 11.626901268959045
[2m[36m(pid=13124)[0m  eval loss: 42.918687133789064, eval err: 0.985217297077179
[2m[36m(pid=13124)[0m 24, 
[2m[36m(pid=13124)[0m  train loss: 11.88746526837349
Result for train_b1c14_00020:
  date: 2021-09-09_22-30-13
  done: false
  err: 0.984156391620636
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 38.407391662597654
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 2669.5544188022614
  time_this_iter_s: 319.9797930717468
  time_total_s: 2669.5544188022614
  timestamp: 1631219413
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00020
  
[2m[36m(pid=13124)[0m  eval loss: 38.407391662597654, eval err: 0.984156391620636
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |      9 |          2669.55 | 38.4074   | 0.984156  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 25, 
[2m[36m(pid=13124)[0m  train loss: 10.90468543767929
[2m[36m(pid=13124)[0m  eval loss: 32.60762855529785, eval err: 0.979259831905365
[2m[36m(pid=13124)[0m 26, 
[2m[36m(pid=13124)[0m  train loss: 11.329772248864174
[2m[36m(pid=13124)[0m  eval loss: 35.23821578979492, eval err: 0.9834106159210205
[2m[36m(pid=13124)[0m 27, 
[2m[36m(pid=13124)[0m  train loss: 11.741595476865768
[2m[36m(pid=13124)[0m  eval loss: 31.77376609802246, eval err: 0.9789085865020752
Result for train_b1c14_00020:
  date: 2021-09-09_22-35-33
  done: false
  err: 0.9789085865020752
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 31.77376609802246
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 2989.931878566742
  time_this_iter_s: 320.3774597644806
  time_total_s: 2989.931878566742
  timestamp: 1631219733
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     10 |          2989.93 | 31.7738   | 0.978909  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 28, 
[2m[36m(pid=13124)[0m  train loss: 11.132036954164505
[2m[36m(pid=13124)[0m  eval loss: 32.32761863708496, eval err: 0.9808302998542786
[2m[36m(pid=13124)[0m 29, 
[2m[36m(pid=13124)[0m  train loss: 11.067244723439217
[2m[36m(pid=13124)[0m  eval loss: 26.424658279418946, eval err: 0.9619595336914063
[2m[36m(pid=13124)[0m 30, 
[2m[36m(pid=13124)[0m  train loss: 11.09780502319336
Result for train_b1c14_00020:
  date: 2021-09-09_22-40-54
  done: false
  err: 0.9850789713859558
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 35.96300956726074
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 3310.1204657554626
  time_this_iter_s: 320.1885871887207
  time_total_s: 3310.1204657554626
  timestamp: 1631220054
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00020
  
[2m[36m(pid=13124)[0m  eval loss: 35.96300956726074, eval err: 0.9850789713859558
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     11 |          3310.12 | 35.963    | 0.985079  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 31, 
[2m[36m(pid=13124)[0m  train loss: 11.348109304904938
[2m[36m(pid=13124)[0m  eval loss: 34.644073486328125, eval err: 0.9830519986152649
[2m[36m(pid=13124)[0m 32, 
[2m[36m(pid=13124)[0m  train loss: 11.104266121983528
[2m[36m(pid=13124)[0m  eval loss: 31.43197883605957, eval err: 0.9800504946708679
[2m[36m(pid=13124)[0m 33, 
[2m[36m(pid=13124)[0m  train loss: 11.017960637807846
[2m[36m(pid=13124)[0m  eval loss: 26.61573028564453, eval err: 0.9737139630317688
Result for train_b1c14_00020:
  date: 2021-09-09_22-46-14
  done: false
  err: 0.9737139630317688
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 26.61573028564453
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 3630.0795130729675
  time_this_iter_s: 319.9590473175049
  time_total_s: 3630.0795130729675
  timestamp: 1631220374
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     12 |          3630.08 | 26.6157   | 0.973714  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 34, 
[2m[36m(pid=13124)[0m  train loss: 10.930913895368576
[2m[36m(pid=13124)[0m  eval loss: 29.385007095336913, eval err: 0.9735236287117004
[2m[36m(pid=13124)[0m 35, 
[2m[36m(pid=13124)[0m  train loss: 10.92999678850174
[2m[36m(pid=13124)[0m  eval loss: 28.79848762512207, eval err: 0.9764504384994507
[2m[36m(pid=13124)[0m 36, 
[2m[36m(pid=13124)[0m  train loss: 10.936194315552711
[2m[36m(pid=13124)[0m  eval loss: 26.09336929321289, eval err: 0.958115303516388
Result for train_b1c14_00020:
  date: 2021-09-09_22-51-33
  done: false
  err: 0.958115303516388
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 26.09336929321289
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 3949.75043964386
  time_this_iter_s: 319.67092657089233
  time_total_s: 3949.75043964386
  timestamp: 1631220693
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     13 |          3949.75 | 26.0934   | 0.958115  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13124)[0m 37, 
[2m[36m(pid=13124)[0m  train loss: 11.618815556168556
[2m[36m(pid=13124)[0m  eval loss: 35.55922241210938, eval err: 0.982679169178009
[2m[36m(pid=13124)[0m 38, 
[2m[36m(pid=13124)[0m  train loss: 10.30726508796215
[2m[36m(pid=13124)[0m  eval loss: 32.65933937072754, eval err: 0.981858389377594
[2m[36m(pid=13124)[0m 39, 
[2m[36m(pid=13124)[0m  train loss: 10.340032815933228
[2m[36m(pid=13124)[0m  eval loss: 32.679269561767576, eval err: 0.9798323535919189
Result for train_b1c14_00020:
  date: 2021-09-09_22-56-53
  done: true
  err: 0.9798323535919189
  experiment_id: 1229a8b88a6141989dba6a170bd3c9da
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 32.679269561767576
  node_ip: 131.220.7.54
  pid: 13124
  should_checkpoint: true
  time_since_restore: 4269.560344219208
  time_this_iter_s: 319.8099045753479
  time_total_s: 4269.560344219208
  timestamp: 1631221013
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00020
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 4 PENDING, 1 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00020 | RUNNING    | 131.220.7.54:13124 |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | PENDING    |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |        |                  |           |           |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m Epoch:
[2m[36m(pid=13084)[0m 0, 
[2m[36m(pid=13084)[0m  train loss: 30.764653980731964
[2m[36m(pid=13084)[0m  eval loss: 55.44164772033692, eval err: 0.9525714421272278
Result for train_b1c14_00021:
  date: 2021-09-09_22-58-45
  done: false
  err: 0.9525714421272278
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 55.44164772033692
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 110.38703393936157
  time_this_iter_s: 110.38703393936157
  time_total_s: 110.38703393936157
  timestamp: 1631221125
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00021
  
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |      1 |          110.387 | 55.4416   | 0.952571  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 1, 
[2m[36m(pid=13084)[0m  train loss: 23.577728390693665
[2m[36m(pid=13084)[0m  eval loss: 24.79878921508789, eval err: 0.9051471328735352
[2m[36m(pid=13084)[0m 2, 
[2m[36m(pid=13084)[0m  train loss: 14.61081650853157
[2m[36m(pid=13084)[0m  eval loss: 12.374319267272949, eval err: 0.8429625821113587
[2m[36m(pid=13084)[0m 3, 
[2m[36m(pid=13084)[0m  train loss: 11.013180032372475
Result for train_b1c14_00021:
  date: 2021-09-09_23-04-08
  done: false
  err: 0.8494025874137878
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 12.667567672729492
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 433.1071491241455
  time_this_iter_s: 322.72011518478394
  time_total_s: 433.1071491241455
  timestamp: 1631221448
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00021
  
[2m[36m(pid=13084)[0m  eval loss: 12.667567672729492, eval err: 0.8494025874137878
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |      2 |          433.107 | 12.6676   | 0.849403  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 4, 
[2m[36m(pid=13084)[0m  train loss: 10.501602828502655
[2m[36m(pid=13084)[0m  eval loss: 11.165617027282714, eval err: 0.8324332427978516
[2m[36m(pid=13084)[0m 5, 
[2m[36m(pid=13084)[0m  train loss: 9.657615840435028
[2m[36m(pid=13084)[0m  eval loss: 11.545348129272462, eval err: 0.833748710155487
[2m[36m(pid=13084)[0m 6, 
[2m[36m(pid=13084)[0m  train loss: 9.708644419908524
Result for train_b1c14_00021:
  date: 2021-09-09_23-09-31
  done: false
  err: 0.8170888662338257
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 10.094791507720947
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 755.9607684612274
  time_this_iter_s: 322.8536193370819
  time_total_s: 755.9607684612274
  timestamp: 1631221771
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00021
  
[2m[36m(pid=13084)[0m  eval loss: 10.094791507720947, eval err: 0.8170888662338257
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |      3 |          755.961 | 10.0948   | 0.817089  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 7, 
[2m[36m(pid=13084)[0m  train loss: 9.352978184819221
[2m[36m(pid=13084)[0m  eval loss: 10.193212852478027, eval err: 0.816904788017273
[2m[36m(pid=13084)[0m 8, 
[2m[36m(pid=13084)[0m  train loss: 9.042422950267792
[2m[36m(pid=13084)[0m  eval loss: 10.341099720001221, eval err: 0.8147393035888671
[2m[36m(pid=13084)[0m 9, 
[2m[36m(pid=13084)[0m  train loss: 8.944314077496529
[2m[36m(pid=13084)[0m  eval loss: 9.681260814666748, eval err: 0.8023323273658752
Result for train_b1c14_00021:
  date: 2021-09-09_23-14-53
  done: false
  err: 0.8023323273658752
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 9.681260814666748
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 1077.9536769390106
  time_this_iter_s: 321.9929084777832
  time_total_s: 1077.9536769390106
  timestamp: 1631222093
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00021
  
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |      4 |          1077.95 |  9.68126  | 0.802332  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 10, 
[2m[36m(pid=13084)[0m  train loss: 8.380965009331703
[2m[36m(pid=13084)[0m  eval loss: 8.779683094024659, eval err: 0.7894984817504883
[2m[36m(pid=13084)[0m 11, 
[2m[36m(pid=13084)[0m  train loss: 8.250967130064964
[2m[36m(pid=13084)[0m  eval loss: 9.647203025817872, eval err: 0.8033770608901978
[2m[36m(pid=13084)[0m 12, 
[2m[36m(pid=13084)[0m  train loss: 8.558857336640358
Result for train_b1c14_00021:
  date: 2021-09-09_23-20-15
  done: false
  err: 0.7940316987037659
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 9.236776390075683
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 1400.0174424648285
  time_this_iter_s: 322.06376552581787
  time_total_s: 1400.0174424648285
  timestamp: 1631222415
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00021
  
[2m[36m(pid=13084)[0m  eval loss: 9.236776390075683, eval err: 0.7940316987037659
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |      5 |          1400.02 |  9.23678  | 0.794032  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 13, 
[2m[36m(pid=13084)[0m  train loss: 8.302337229251862
[2m[36m(pid=13084)[0m  eval loss: 8.714986839294433, eval err: 0.7815412640571594
[2m[36m(pid=13084)[0m 14, 
[2m[36m(pid=13084)[0m  train loss: 7.992261931300163
[2m[36m(pid=13084)[0m  eval loss: 9.763627281188965, eval err: 0.8046853947639465
[2m[36m(pid=13084)[0m 15, 
[2m[36m(pid=13084)[0m  train loss: 7.91069769859314
[2m[36m(pid=13084)[0m  eval loss: 9.165347690582275, eval err: 0.7874192357063293
Result for train_b1c14_00021:
  date: 2021-09-09_23-25-37
  done: false
  err: 0.7874192357063293
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 9.165347690582275
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 1722.1818664073944
  time_this_iter_s: 322.1644239425659
  time_total_s: 1722.1818664073944
  timestamp: 1631222737
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00021
  
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |      6 |          1722.18 |  9.16535  | 0.787419  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 16, 
[2m[36m(pid=13084)[0m  train loss: 7.7275218069553375
[2m[36m(pid=13084)[0m  eval loss: 9.188683910369873, eval err: 0.7844718670845032
[2m[36m(pid=13084)[0m 17, 
[2m[36m(pid=13084)[0m  train loss: 8.191764444112778
[2m[36m(pid=13084)[0m  eval loss: 8.870174808502197, eval err: 0.7914149045944214
[2m[36m(pid=13084)[0m 18, 
[2m[36m(pid=13084)[0m  train loss: 8.032178491353989
[2m[36m(pid=13084)[0m  eval loss: 9.176135883331298, eval err: 0.793534414768219
Result for train_b1c14_00021:
  date: 2021-09-09_23-30-59
  done: false
  err: 0.793534414768219
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 9.176135883331298
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 2044.310958623886
  time_this_iter_s: 322.1290922164917
  time_total_s: 2044.310958623886
  timestamp: 1631223059
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00021
  
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |      7 |          2044.31 |  9.17614  | 0.793534  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 19, 
[2m[36m(pid=13084)[0m  train loss: 7.741263806819916
[2m[36m(pid=13084)[0m  eval loss: 8.469159851074219, eval err: 0.7815414142608642
[2m[36m(pid=13084)[0m 20, 
[2m[36m(pid=13084)[0m  train loss: 7.358182817697525
[2m[36m(pid=13084)[0m  eval loss: 9.46772029876709, eval err: 0.7911795830726623
[2m[36m(pid=13084)[0m 21, 
[2m[36m(pid=13084)[0m  train loss: 7.508749917149544
[2m[36m(pid=13084)[0m  eval loss: 8.489745445251465, eval err: 0.7678338313102722
Result for train_b1c14_00021:
  date: 2021-09-09_23-36-21
  done: false
  err: 0.7678338313102722
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 8.489745445251465
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 2366.473063468933
  time_this_iter_s: 322.162104845047
  time_total_s: 2366.473063468933
  timestamp: 1631223381
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00021
  
== Status ==
Memory usage on this node: 14.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |      8 |          2366.47 |  8.48975  | 0.767834  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 22, 
[2m[36m(pid=13084)[0m  train loss: 7.194344416260719
[2m[36m(pid=13084)[0m  eval loss: 8.531334648132324, eval err: 0.7725557684898376
[2m[36m(pid=13084)[0m 23, 
[2m[36m(pid=13084)[0m  train loss: 7.320958286523819
[2m[36m(pid=13084)[0m  eval loss: 8.56187967300415, eval err: 0.7776811265945435
[2m[36m(pid=13084)[0m 24, 
[2m[36m(pid=13084)[0m  train loss: 7.094273597002029
[2m[36m(pid=13084)[0m  eval loss: 7.975052280426025, eval err: 0.7604178237915039
Result for train_b1c14_00021:
  date: 2021-09-09_23-41-44
  done: false
  err: 0.7604178237915039
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 7.975052280426025
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 2688.557443380356
  time_this_iter_s: 322.08437991142273
  time_total_s: 2688.557443380356
  timestamp: 1631223704
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00021
  
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |      9 |          2688.56 |  7.97505  | 0.760418  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 25, 
[2m[36m(pid=13084)[0m  train loss: 7.606061205267906
[2m[36m(pid=13084)[0m  eval loss: 7.737791557312011, eval err: 0.7502791333198547
[2m[36m(pid=13084)[0m 26, 
[2m[36m(pid=13084)[0m  train loss: 7.3122909516096115
[2m[36m(pid=13084)[0m  eval loss: 8.302949771881103, eval err: 0.7660771751403809
[2m[36m(pid=13084)[0m 27, 
[2m[36m(pid=13084)[0m  train loss: 6.889127224683762
[2m[36m(pid=13084)[0m  eval loss: 8.71650354385376, eval err: 0.7678978228569031
Result for train_b1c14_00021:
  date: 2021-09-09_23-47-06
  done: false
  err: 0.7678978228569031
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 8.71650354385376
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 3010.523946762085
  time_this_iter_s: 321.9665033817291
  time_total_s: 3010.523946762085
  timestamp: 1631224026
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00021
  
== Status ==
Memory usage on this node: 14.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     10 |          3010.52 |  8.7165   | 0.767898  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 28, 
[2m[36m(pid=13084)[0m  train loss: 6.987608343362808
[2m[36m(pid=13084)[0m  eval loss: 7.973358764648437, eval err: 0.7583498859405517
[2m[36m(pid=13084)[0m 29, 
[2m[36m(pid=13084)[0m  train loss: 6.604806989431381
[2m[36m(pid=13084)[0m  eval loss: 8.877442474365234, eval err: 0.7647692847251892
[2m[36m(pid=13084)[0m 30, 
[2m[36m(pid=13084)[0m  train loss: 7.141453102231026
[2m[36m(pid=13084)[0m  eval loss: 7.840423603057861, eval err: 0.7482525324821472
Result for train_b1c14_00021:
  date: 2021-09-09_23-52-27
  done: false
  err: 0.7482525324821472
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 7.840423603057861
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 3332.492683172226
  time_this_iter_s: 321.968736410141
  time_total_s: 3332.492683172226
  timestamp: 1631224347
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00021
  
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     11 |          3332.49 |  7.84042  | 0.748253  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 31, 
[2m[36m(pid=13084)[0m  train loss: 6.94840681552887
[2m[36m(pid=13084)[0m  eval loss: 8.348298854827881, eval err: 0.755963122844696
[2m[36m(pid=13084)[0m 32, 
[2m[36m(pid=13084)[0m  train loss: 6.753740444779396
[2m[36m(pid=13084)[0m  eval loss: 9.266365070343017, eval err: 0.7610975885391236
[2m[36m(pid=13084)[0m 33, 
[2m[36m(pid=13084)[0m  train loss: 6.872866585850716
[2m[36m(pid=13084)[0m  eval loss: 7.623584232330322, eval err: 0.7524250674247742
Result for train_b1c14_00021:
  date: 2021-09-09_23-57-50
  done: false
  err: 0.7524250674247742
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 7.623584232330322
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 3654.58256983757
  time_this_iter_s: 322.08988666534424
  time_total_s: 3654.58256983757
  timestamp: 1631224670
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00021
  
== Status ==
Memory usage on this node: 13.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     12 |          3654.58 |  7.62358  | 0.752425  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 34, 
[2m[36m(pid=13084)[0m  train loss: 7.258302599191666
[2m[36m(pid=13084)[0m  eval loss: 7.216685752868653, eval err: 0.7325752830505371
[2m[36m(pid=13084)[0m 35, 
[2m[36m(pid=13084)[0m  train loss: 6.7824380695819855
[2m[36m(pid=13084)[0m  eval loss: 8.051564178466798, eval err: 0.7427447700500488
[2m[36m(pid=13084)[0m 36, 
[2m[36m(pid=13084)[0m  train loss: 6.476996272802353
Result for train_b1c14_00021:
  date: 2021-09-10_00-03-11
  done: false
  err: 0.7350483202934265
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 7.904734268188476
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 3976.388889312744
  time_this_iter_s: 321.80631947517395
  time_total_s: 3976.388889312744
  timestamp: 1631224991
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00021
  
[2m[36m(pid=13084)[0m  eval loss: 7.904734268188476, eval err: 0.7350483202934265
== Status ==
Memory usage on this node: 13.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     13 |          3976.39 |  7.90473  | 0.735048  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13084)[0m 37, 
[2m[36m(pid=13084)[0m  train loss: 6.485708400607109
[2m[36m(pid=13084)[0m  eval loss: 6.701050281524658, eval err: 0.6948375487327576
[2m[36m(pid=13084)[0m 38, 
[2m[36m(pid=13084)[0m  train loss: 5.927150174975395
[2m[36m(pid=13084)[0m  eval loss: 7.865951709747314, eval err: 0.7202835202217102
[2m[36m(pid=13084)[0m 39, 
[2m[36m(pid=13084)[0m  train loss: 5.612499192357063
Result for train_b1c14_00021:
  date: 2021-09-10_00-08-32
  done: true
  err: 0.7036918711662292
  experiment_id: 7f9161edf65e4765b366aa733734cf4f
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 6.825116539001465
  node_ip: 131.220.7.54
  pid: 13084
  should_checkpoint: true
  time_since_restore: 4297.503732204437
  time_this_iter_s: 321.1148428916931
  time_total_s: 4297.503732204437
  timestamp: 1631225312
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00021
  
[2m[36m(pid=13084)[0m  eval loss: 6.825116539001465, eval err: 0.7036918711662292
== Status ==
Memory usage on this node: 13.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 ERROR, 3 PENDING, 1 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00021 | RUNNING    | 131.220.7.54:13084 |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00022 | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |                    |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 12
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13101)[0m Epoch:
[2m[36m(pid=13101)[0m 0, 
Result for train_b1c14_00022:
  {}
  
== Status ==
Memory usage on this node: 13.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 ERROR, 2 PENDING, 10 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00023 | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_b1c14_00024 | PENDING    |       |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 TERMINATED, 4 ERROR)
Number of errored trials: 13
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13091)[0m Epoch:
[2m[36m(pid=13091)[0m 0, 
Result for train_b1c14_00023:
  {}
  
== Status ==
Memory usage on this node: 13.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 PENDING, 10 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | PENDING    |       |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m Epoch:
[2m[36m(pid=13085)[0m 0, 
[2m[36m(pid=13085)[0m  train loss: 30.513705790042877
[2m[36m(pid=13085)[0m  eval loss: 78.73499855041504, eval err: 0.9784874033927917
Result for train_b1c14_00024:
  date: 2021-09-10_00-10-34
  done: false
  err: 0.9784874033927917
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 78.73499855041504
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 99.81206011772156
  time_this_iter_s: 99.81206011772156
  time_total_s: 99.81206011772156
  timestamp: 1631225434
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |      1 |          99.8121 | 78.735    | 0.978487  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |        9802.53   |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |        4314.17   | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |        4282.03   | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |        3897.67   |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |        4316.23   |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |        6234.39   |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |        3857.95   | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |        7753.02   |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |        4269.56   | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |        4297.5    |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 1, 
[2m[36m(pid=13085)[0m  train loss: 12.611996695399284
[2m[36m(pid=13085)[0m  eval loss: 13.522349166870118, eval err: 0.8481494235992432
[2m[36m(pid=13085)[0m 2, 
[2m[36m(pid=13085)[0m  train loss: 11.339041396975517
[2m[36m(pid=13085)[0m  eval loss: 12.400403213500976, eval err: 0.8540625
[2m[36m(pid=13085)[0m 3, 
[2m[36m(pid=13085)[0m  train loss: 11.145345374941826
[2m[36m(pid=13085)[0m  eval loss: 18.399637832641602, eval err: 0.8782086825370788
Result for train_b1c14_00024:
  date: 2021-09-10_00-15-24
  done: false
  err: 0.8782086825370788
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 18.399637832641602
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 390.014262676239
  time_this_iter_s: 290.20220255851746
  time_total_s: 390.014262676239
  timestamp: 1631225724
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |      2 |          390.014 | 18.3996   | 0.878209  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |         4269.56  | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |         4297.5   |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 4, 
[2m[36m(pid=13085)[0m  train loss: 10.466737747192383
[2m[36m(pid=13085)[0m  eval loss: 10.395463523864747, eval err: 0.8391984057426453
[2m[36m(pid=13085)[0m 5, 
[2m[36m(pid=13085)[0m  train loss: 10.409960880875587
[2m[36m(pid=13085)[0m  eval loss: 13.730658149719238, eval err: 0.8634278106689454
[2m[36m(pid=13085)[0m 6, 
[2m[36m(pid=13085)[0m  train loss: 10.016556650400162
[2m[36m(pid=13085)[0m  eval loss: 11.270836639404298, eval err: 0.8470100355148316
Result for train_b1c14_00024:
  date: 2021-09-10_00-20-13
  done: false
  err: 0.8470100355148316
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 11.270836639404298
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 678.7943317890167
  time_this_iter_s: 288.7800691127777
  time_total_s: 678.7943317890167
  timestamp: 1631226013
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |      3 |          678.794 | 11.2708   | 0.84701   |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |         4269.56  | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |         4297.5   |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 7, 
[2m[36m(pid=13085)[0m  train loss: 9.623007997870445
[2m[36m(pid=13085)[0m  eval loss: 12.483719444274902, eval err: 0.8461547327041626
[2m[36m(pid=13085)[0m 8, 
[2m[36m(pid=13085)[0m  train loss: 8.695418253540993
[2m[36m(pid=13085)[0m  eval loss: 11.022401008605957, eval err: 0.823038055896759
[2m[36m(pid=13085)[0m 9, 
[2m[36m(pid=13085)[0m  train loss: 8.867573022842407
[2m[36m(pid=13085)[0m  eval loss: 9.674402465820313, eval err: 0.8255282926559449
Result for train_b1c14_00024:
  date: 2021-09-10_00-25-02
  done: false
  err: 0.8255282926559449
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 9.674402465820313
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 967.970454454422
  time_this_iter_s: 289.1761226654053
  time_total_s: 967.970454454422
  timestamp: 1631226302
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |      4 |           967.97 |  9.6744   | 0.825528  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 10, 
[2m[36m(pid=13085)[0m  train loss: 9.003582775592804
[2m[36m(pid=13085)[0m  eval loss: 9.879808235168458, eval err: 0.8145896434783936
[2m[36m(pid=13085)[0m 11, 
[2m[36m(pid=13085)[0m  train loss: 7.633733838796616
[2m[36m(pid=13085)[0m  eval loss: 10.779204063415527, eval err: 0.8238501620292663
[2m[36m(pid=13085)[0m 12, 
[2m[36m(pid=13085)[0m  train loss: 8.293631702661514
[2m[36m(pid=13085)[0m  eval loss: 9.206463088989258, eval err: 0.7641482424736022
Result for train_b1c14_00024:
  date: 2021-09-10_00-29-51
  done: false
  err: 0.7641482424736022
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 9.206463088989258
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 1257.128089427948
  time_this_iter_s: 289.157634973526
  time_total_s: 1257.128089427948
  timestamp: 1631226591
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |      5 |          1257.13 |  9.20646  | 0.764148  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 13, 
[2m[36m(pid=13085)[0m  train loss: 7.81324827671051
[2m[36m(pid=13085)[0m  eval loss: 8.091167068481445, eval err: 0.7237051224708557
[2m[36m(pid=13085)[0m 14, 
[2m[36m(pid=13085)[0m  train loss: 6.966861113905907
[2m[36m(pid=13085)[0m  eval loss: 7.242826137542725, eval err: 0.7027903819084167
[2m[36m(pid=13085)[0m 15, 
[2m[36m(pid=13085)[0m  train loss: 5.985025152564049
[2m[36m(pid=13085)[0m  eval loss: 4.508820533752441, eval err: 0.55701669216156
Result for train_b1c14_00024:
  date: 2021-09-10_00-34-41
  done: false
  err: 0.55701669216156
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 4.508820533752441
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 1546.8591141700745
  time_this_iter_s: 289.73102474212646
  time_total_s: 1546.8591141700745
  timestamp: 1631226881
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |      6 |          1546.86 |  4.50882  | 0.557017  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 16, 
[2m[36m(pid=13085)[0m  train loss: 4.58467847853899
[2m[36m(pid=13085)[0m  eval loss: 2.5076685190200805, eval err: 0.2878100371360779
[2m[36m(pid=13085)[0m 17, 
[2m[36m(pid=13085)[0m  train loss: 3.4924487322568893
[2m[36m(pid=13085)[0m  eval loss: 2.624984827041626, eval err: 0.283240385055542
[2m[36m(pid=13085)[0m 18, 
[2m[36m(pid=13085)[0m  train loss: 2.6807564720511436
[2m[36m(pid=13085)[0m  eval loss: 1.7667683935165406, eval err: 0.15633707046508788
Result for train_b1c14_00024:
  date: 2021-09-10_00-39-31
  done: false
  err: 0.15633707046508788
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 1.7667683935165406
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 1836.4690001010895
  time_this_iter_s: 289.609885931015
  time_total_s: 1836.4690001010895
  timestamp: 1631227171
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |      7 |          1836.47 |  1.76677  | 0.156337  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 19, 
[2m[36m(pid=13085)[0m  train loss: 2.4298267513513565
[2m[36m(pid=13085)[0m  eval loss: 1.390751473903656, eval err: 0.11618823051452637
[2m[36m(pid=13085)[0m 20, 
[2m[36m(pid=13085)[0m  train loss: 2.018280617892742
[2m[36m(pid=13085)[0m  eval loss: 1.2447333192825318, eval err: 0.10261648178100585
[2m[36m(pid=13085)[0m 21, 
[2m[36m(pid=13085)[0m  train loss: 2.0671254694461823
[2m[36m(pid=13085)[0m  eval loss: 1.5001578330993652, eval err: 0.12180595636367798
Result for train_b1c14_00024:
  date: 2021-09-10_00-44-20
  done: false
  err: 0.12180595636367798
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 1.5001578330993652
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 2126.0294630527496
  time_this_iter_s: 289.56046295166016
  time_total_s: 2126.0294630527496
  timestamp: 1631227460
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |      8 |          2126.03 |  1.50016  | 0.121806  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 22, 
[2m[36m(pid=13085)[0m  train loss: 2.074852015823126
[2m[36m(pid=13085)[0m  eval loss: 1.1939035224914551, eval err: 0.09495849132537842
[2m[36m(pid=13085)[0m 23, 
[2m[36m(pid=13085)[0m  train loss: 1.8398778289556503
[2m[36m(pid=13085)[0m  eval loss: 1.2460152530670165, eval err: 0.09375491857528687
[2m[36m(pid=13085)[0m 24, 
[2m[36m(pid=13085)[0m  train loss: 2.006400015205145
[2m[36m(pid=13085)[0m  eval loss: 1.1783261799812317, eval err: 0.09065882921218872
Result for train_b1c14_00024:
  date: 2021-09-10_00-49-10
  done: false
  err: 0.09065882921218872
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 1.1783261799812317
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 2415.2971873283386
  time_this_iter_s: 289.267724275589
  time_total_s: 2415.2971873283386
  timestamp: 1631227750
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |      9 |          2415.3  |  1.17833  | 0.0906588 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 25, 
[2m[36m(pid=13085)[0m  train loss: 1.9733050968497992
[2m[36m(pid=13085)[0m  eval loss: 1.2319474983215333, eval err: 0.09553967237472534
[2m[36m(pid=13085)[0m 26, 
[2m[36m(pid=13085)[0m  train loss: 1.7036504074931145
[2m[36m(pid=13085)[0m  eval loss: 1.1906527137756349, eval err: 0.09214133977890014
[2m[36m(pid=13085)[0m 27, 
[2m[36m(pid=13085)[0m  train loss: 1.9256316237151623
[2m[36m(pid=13085)[0m  eval loss: 1.2580859565734863, eval err: 0.08762353658676147
Result for train_b1c14_00024:
  date: 2021-09-10_00-53-59
  done: false
  err: 0.08762353658676147
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 1.2580859565734863
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 2704.4100358486176
  time_this_iter_s: 289.11284852027893
  time_total_s: 2704.4100358486176
  timestamp: 1631228039
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     10 |          2704.41 |  1.25809  | 0.0876235 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 28, 
[2m[36m(pid=13085)[0m  train loss: 1.6318006105720997
[2m[36m(pid=13085)[0m  eval loss: 1.123912365436554, eval err: 0.08850490808486938
[2m[36m(pid=13085)[0m 29, 
[2m[36m(pid=13085)[0m  train loss: 1.557423746213317
[2m[36m(pid=13085)[0m  eval loss: 1.0489859342575074, eval err: 0.07832035541534424
[2m[36m(pid=13085)[0m 30, 
[2m[36m(pid=13085)[0m  train loss: 1.7498705945909023
Result for train_b1c14_00024:
  date: 2021-09-10_00-58-48
  done: false
  err: 0.07966376543045044
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 1.031502810716629
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 2993.3607444763184
  time_this_iter_s: 288.9507086277008
  time_total_s: 2993.3607444763184
  timestamp: 1631228328
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: b1c14_00024
  
[2m[36m(pid=13085)[0m  eval loss: 1.031502810716629, eval err: 0.07966376543045044
== Status ==
Memory usage on this node: 13.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     11 |          2993.36 |  1.0315   | 0.0796638 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 31, 
[2m[36m(pid=13085)[0m  train loss: 1.4884297195822
[2m[36m(pid=13085)[0m  eval loss: 1.0796107864379882, eval err: 0.073094801902771
[2m[36m(pid=13085)[0m 32, 
[2m[36m(pid=13085)[0m  train loss: 1.6502688769251108
[2m[36m(pid=13085)[0m  eval loss: 1.0348544061183929, eval err: 0.0781454586982727
[2m[36m(pid=13085)[0m 33, 
[2m[36m(pid=13085)[0m  train loss: 1.3032724000513554
Result for train_b1c14_00024:
  date: 2021-09-10_01-03-36
  done: false
  err: 0.06970887184143067
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 0.9569528090953827
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 3282.09907913208
  time_this_iter_s: 288.7383346557617
  time_total_s: 3282.09907913208
  timestamp: 1631228616
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: b1c14_00024
  
[2m[36m(pid=13085)[0m  eval loss: 0.9569528090953827, eval err: 0.06970887184143067
== Status ==
Memory usage on this node: 13.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     12 |          3282.1  |  0.956953 | 0.0697089 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 34, 
[2m[36m(pid=13085)[0m  train loss: 1.2725835386663675
[2m[36m(pid=13085)[0m  eval loss: 0.9521337687969208, eval err: 0.06949789524078369
[2m[36m(pid=13085)[0m 35, 
[2m[36m(pid=13085)[0m  train loss: 1.5988618209958076
[2m[36m(pid=13085)[0m  eval loss: 0.9904380464553832, eval err: 0.06872776746749878
[2m[36m(pid=13085)[0m 36, 
[2m[36m(pid=13085)[0m  train loss: 1.436433307826519
[2m[36m(pid=13085)[0m  eval loss: 0.9774096393585205, eval err: 0.06905272006988525
Result for train_b1c14_00024:
  date: 2021-09-10_01-08-25
  done: false
  err: 0.06905272006988525
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 0.9774096393585205
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 3570.956718683243
  time_this_iter_s: 288.8576395511627
  time_total_s: 3570.956718683243
  timestamp: 1631228905
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     13 |          3570.96 |  0.97741  | 0.0690527 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 37, 
[2m[36m(pid=13085)[0m  train loss: 1.4890433810651302
[2m[36m(pid=13085)[0m  eval loss: 0.938124463558197, eval err: 0.06821723222732544
[2m[36m(pid=13085)[0m 38, 
[2m[36m(pid=13085)[0m  train loss: 1.2883339188992977
[2m[36m(pid=13085)[0m  eval loss: 0.9193517684936523, eval err: 0.0677213740348816
[2m[36m(pid=13085)[0m 39, 
[2m[36m(pid=13085)[0m  train loss: 1.5440020374953747
[2m[36m(pid=13085)[0m  eval loss: 1.0669143104553223, eval err: 0.07394092082977295
Result for train_b1c14_00024:
  date: 2021-09-10_01-13-14
  done: false
  err: 0.07394092082977295
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 1.0669143104553223
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 3859.6381483078003
  time_this_iter_s: 288.6814296245575
  time_total_s: 3859.6381483078003
  timestamp: 1631229194
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     14 |          3859.64 |  1.06691  | 0.0739409 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 40, 
[2m[36m(pid=13085)[0m  train loss: 1.5591709520667791
[2m[36m(pid=13085)[0m  eval loss: 1.0978653311729432, eval err: 0.07731320142745972
[2m[36m(pid=13085)[0m 41, 
[2m[36m(pid=13085)[0m  train loss: 1.2675808239728212
[2m[36m(pid=13085)[0m  eval loss: 0.9367407023906708, eval err: 0.06856527090072632
[2m[36m(pid=13085)[0m 42, 
[2m[36m(pid=13085)[0m  train loss: 1.2898851716890931
[2m[36m(pid=13085)[0m  eval loss: 0.9415238273143768, eval err: 0.06719405651092529
Result for train_b1c14_00024:
  date: 2021-09-10_01-18-02
  done: false
  err: 0.06719405651092529
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 0.9415238273143768
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 4148.079703330994
  time_this_iter_s: 288.44155502319336
  time_total_s: 4148.079703330994
  timestamp: 1631229482
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     15 |          4148.08 |  0.941524 | 0.0671941 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 43, 
[2m[36m(pid=13085)[0m  train loss: 1.372904846444726
[2m[36m(pid=13085)[0m  eval loss: 0.9001312637329102, eval err: 0.06596788644790649
[2m[36m(pid=13085)[0m 44, 
[2m[36m(pid=13085)[0m  train loss: 1.234936585649848
[2m[36m(pid=13085)[0m  eval loss: 0.8874810647964477, eval err: 0.06498381614685059
[2m[36m(pid=13085)[0m 45, 
[2m[36m(pid=13085)[0m  train loss: 1.2588703967630863
[2m[36m(pid=13085)[0m  eval loss: 0.8672238326072693, eval err: 0.06201162815093994
Result for train_b1c14_00024:
  date: 2021-09-10_01-22-51
  done: false
  err: 0.06201162815093994
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 0.8672238326072693
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 4436.738608837128
  time_this_iter_s: 288.65890550613403
  time_total_s: 4436.738608837128
  timestamp: 1631229771
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     16 |          4436.74 |  0.867224 | 0.0620116 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 46, 
[2m[36m(pid=13085)[0m  train loss: 1.2435526587069035
[2m[36m(pid=13085)[0m  eval loss: 0.8655099952220917, eval err: 0.06177290916442871
[2m[36m(pid=13085)[0m 47, 
[2m[36m(pid=13085)[0m  train loss: 1.2026471346616745
[2m[36m(pid=13085)[0m  eval loss: 0.8274564898014068, eval err: 0.058093962669372556
[2m[36m(pid=13085)[0m 48, 
[2m[36m(pid=13085)[0m  train loss: 1.0690636616200209
[2m[36m(pid=13085)[0m  eval loss: 0.8471867299079895, eval err: 0.060524256229400636
Result for train_b1c14_00024:
  date: 2021-09-10_01-27-40
  done: false
  err: 0.060524256229400636
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 0.8471867299079895
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 4725.568758249283
  time_this_iter_s: 288.83014941215515
  time_total_s: 4725.568758249283
  timestamp: 1631230060
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     17 |          4725.57 |  0.847187 | 0.0605243 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 49, 
[2m[36m(pid=13085)[0m  train loss: 1.3818796314299107
[2m[36m(pid=13085)[0m  eval loss: 1.0005907118320465, eval err: 0.06645927667617797
[2m[36m(pid=13085)[0m 50, 
[2m[36m(pid=13085)[0m  train loss: 1.0686943959444761
[2m[36m(pid=13085)[0m  eval loss: 0.8112661361694335, eval err: 0.05821230173110962
[2m[36m(pid=13085)[0m 51, 
[2m[36m(pid=13085)[0m  train loss: 1.1056648287922144
[2m[36m(pid=13085)[0m  eval loss: 0.8368825471401214, eval err: 0.059655771255493165
Result for train_b1c14_00024:
  date: 2021-09-10_01-32-29
  done: false
  err: 0.059655771255493165
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 0.8368825471401214
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 5014.60301399231
  time_this_iter_s: 289.03425574302673
  time_total_s: 5014.60301399231
  timestamp: 1631230349
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     18 |          5014.6  |  0.836883 | 0.0596558 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 52, 
[2m[36m(pid=13085)[0m  train loss: 1.1499776933342218
[2m[36m(pid=13085)[0m  eval loss: 0.8027881097793579, eval err: 0.05697591543197632
[2m[36m(pid=13085)[0m 53, 
[2m[36m(pid=13085)[0m  train loss: 1.1279031578451395
[2m[36m(pid=13085)[0m  eval loss: 0.8358254945278167, eval err: 0.05620854377746582
[2m[36m(pid=13085)[0m 54, 
[2m[36m(pid=13085)[0m  train loss: 1.2373267784714699
Result for train_b1c14_00024:
  date: 2021-09-10_01-37-18
  done: false
  err: 0.0648949146270752
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 0.9724235248565674
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 5303.994647502899
  time_this_iter_s: 289.3916335105896
  time_total_s: 5303.994647502899
  timestamp: 1631230638
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: b1c14_00024
  
[2m[36m(pid=13085)[0m  eval loss: 0.9724235248565674, eval err: 0.0648949146270752
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     19 |          5303.99 |  0.972424 | 0.0648949 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 55, 
[2m[36m(pid=13085)[0m  train loss: 1.250449014827609
[2m[36m(pid=13085)[0m  eval loss: 0.8573426795005799, eval err: 0.0579947566986084
[2m[36m(pid=13085)[0m 56, 
[2m[36m(pid=13085)[0m  train loss: 1.047190811485052
[2m[36m(pid=13085)[0m  eval loss: 0.8164592671394348, eval err: 0.05659975290298462
[2m[36m(pid=13085)[0m 57, 
[2m[36m(pid=13085)[0m  train loss: 1.1512935357168317
[2m[36m(pid=13085)[0m  eval loss: 0.7996702527999878, eval err: 0.05626963138580322
Result for train_b1c14_00024:
  date: 2021-09-10_01-42-08
  done: false
  err: 0.05626963138580322
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 0.7996702527999878
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 5593.307987689972
  time_this_iter_s: 289.31334018707275
  time_total_s: 5593.307987689972
  timestamp: 1631230928
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     20 |          5593.31 |  0.79967  | 0.0562696 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 58, 
[2m[36m(pid=13085)[0m  train loss: 1.0144367180764675
[2m[36m(pid=13085)[0m  eval loss: 0.7897253131866455, eval err: 0.054899435043334964
[2m[36m(pid=13085)[0m 59, 
[2m[36m(pid=13085)[0m  train loss: 1.0586119834333658
[2m[36m(pid=13085)[0m  eval loss: 0.7801092886924743, eval err: 0.0544884181022644
[2m[36m(pid=13085)[0m 60, 
[2m[36m(pid=13085)[0m  train loss: 0.9844927042722702
[2m[36m(pid=13085)[0m  eval loss: 0.7729505443572998, eval err: 0.05414253234863281
Result for train_b1c14_00024:
  date: 2021-09-10_01-46-56
  done: false
  err: 0.05414253234863281
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 0.7729505443572998
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 5882.228246688843
  time_this_iter_s: 288.92025899887085
  time_total_s: 5882.228246688843
  timestamp: 1631231216
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     21 |          5882.23 |  0.772951 | 0.0541425 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 61, 
[2m[36m(pid=13085)[0m  train loss: 1.114925310947001
[2m[36m(pid=13085)[0m  eval loss: 0.7693805468082427, eval err: 0.053797962665557864
[2m[36m(pid=13085)[0m 62, 
[2m[36m(pid=13085)[0m  train loss: 1.0244645569473505
[2m[36m(pid=13085)[0m  eval loss: 0.7643120336532593, eval err: 0.05311811208724976
[2m[36m(pid=13085)[0m 63, 
[2m[36m(pid=13085)[0m  train loss: 0.9445481793954968
[2m[36m(pid=13085)[0m  eval loss: 0.7625421690940857, eval err: 0.05304865121841431
Result for train_b1c14_00024:
  date: 2021-09-10_01-51-45
  done: false
  err: 0.05304865121841431
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 22
  loss: 0.7625421690940857
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 6170.871545553207
  time_this_iter_s: 288.6432988643646
  time_total_s: 6170.871545553207
  timestamp: 1631231505
  timesteps_since_restore: 0
  training_iteration: 22
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     22 |          6170.87 |  0.762542 | 0.0530487 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 64, 
[2m[36m(pid=13085)[0m  train loss: 0.9755906099453568
[2m[36m(pid=13085)[0m  eval loss: 0.7550694262981414, eval err: 0.05287593603134155
[2m[36m(pid=13085)[0m 65, 
[2m[36m(pid=13085)[0m  train loss: 0.9661715030670166
[2m[36m(pid=13085)[0m  eval loss: 0.7535672187805176, eval err: 0.052566933631896975
[2m[36m(pid=13085)[0m 66, 
[2m[36m(pid=13085)[0m  train loss: 0.9062044275924563
[2m[36m(pid=13085)[0m  eval loss: 0.7498574757575989, eval err: 0.05273067712783813
Result for train_b1c14_00024:
  date: 2021-09-10_01-56-34
  done: false
  err: 0.05273067712783813
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 23
  loss: 0.7498574757575989
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 6459.512659311295
  time_this_iter_s: 288.64111375808716
  time_total_s: 6459.512659311295
  timestamp: 1631231794
  timesteps_since_restore: 0
  training_iteration: 23
  trial_id: b1c14_00024
  
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     23 |          6459.51 |  0.749857 | 0.0527307 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=13085)[0m 67, 
[2m[36m(pid=13085)[0m  train loss: 0.9665605891495943
[2m[36m(pid=13085)[0m  eval loss: 0.7505202507972717, eval err: 0.05200536251068115
[2m[36m(pid=13085)[0m 68, 
[2m[36m(pid=13085)[0m  train loss: 0.9240325605496764
[2m[36m(pid=13085)[0m  eval loss: 0.7493944787979125, eval err: 0.05198642253875732
[2m[36m(pid=13085)[0m 69, 
[2m[36m(pid=13085)[0m  train loss: 0.9756598928943276
Result for train_b1c14_00024:
  date: 2021-09-10_02-01-22
  done: true
  err: 0.05195723533630371
  experiment_id: d8b9d0c5fa3e4aee9ccd9d5d9ee950cd
  hostname: bigcuda4
  iterations_since_restore: 24
  loss: 0.7498626518249512
  node_ip: 131.220.7.54
  pid: 13085
  should_checkpoint: true
  time_since_restore: 6747.907239437103
  time_this_iter_s: 288.3945801258087
  time_total_s: 6747.907239437103
  timestamp: 1631232082
  timesteps_since_restore: 0
  training_iteration: 24
  trial_id: b1c14_00024
  
[2m[36m(pid=13085)[0m  eval loss: 0.7498626518249512, eval err: 0.05195723533630371
== Status ==
Memory usage on this node: 13.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 1 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00024 | RUNNING    | 131.220.7.54:13085 |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     24 |          6747.91 |  0.749863 | 0.0519572 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00002 | ERROR      |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (4 ERROR)
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

== Status ==
Memory usage on this node: 13.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/154.2 GiB heap, 0.0/70.08 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (14 ERROR, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00024 | TERMINATED |       |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     24 |          6747.91 |  0.749863 | 0.0519572 |
| train_b1c14_00002 | ERROR      |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_b1c14_00003 | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_b1c14_00004 | ERROR      |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_b1c14_00005 | ERROR      |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_b1c14_00007 | ERROR      |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00008 | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_b1c14_00009 | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_b1c14_00010 | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_b1c14_00011 | ERROR      |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00014 | ERROR      |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00016 | ERROR      |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00017 | ERROR      |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00022 | ERROR      |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_b1c14_00023 | ERROR      |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
Number of errored trials: 14
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                                                                                                              |
|-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_b1c14_00002 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00002_2_batch_size=5,dropout_p=0.07273,layers=feat_heavy,lr=6.8909e-06,num_warmup=5.0,step_lr=81.0_2021-09-09_12-05-55/error.txt  |
| train_b1c14_00003 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00003_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-09_13-17-51/error.txt  |
| train_b1c14_00004 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00004_4_batch_size=5,dropout_p=0.18243,layers=feat_heavy,lr=0.0003675,num_warmup=5.0,step_lr=51.0_2021-09-09_13-18-01/error.txt   |
| train_b1c14_00005 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00005_5_batch_size=5,dropout_p=0.01858,layers=cost_heavy,lr=0.001179,num_warmup=6.0,step_lr=63.0_2021-09-09_13-18-11/error.txt    |
| train_b1c14_00007 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00007_7_batch_size=5,dropout_p=0.17606,layers=cost_heavy,lr=0.0013783,num_warmup=7.0,step_lr=40.0_2021-09-09_13-18-30/error.txt   |
| train_b1c14_00008 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00008_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-09_14-29-54/error.txt  |
| train_b1c14_00009 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00009_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-09_14-30-04/error.txt |
| train_b1c14_00010 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00010_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-09_14-30-14/error.txt   |
| train_b1c14_00011 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00011_11_batch_size=5,dropout_p=0.13013,layers=feat_heavy,lr=0.0029565,num_warmup=4.0,step_lr=46.0_2021-09-09_14-30-24/error.txt  |
| train_b1c14_00014 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00014_14_batch_size=5,dropout_p=0.28274,layers=feat_heavy,lr=0.0010808,num_warmup=7.0,step_lr=35.0_2021-09-09_15-35-43/error.txt  |
| train_b1c14_00016 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00016_16_batch_size=5,dropout_p=0.12439,layers=feat_heavy,lr=0.0019912,num_warmup=4.0,step_lr=53.0_2021-09-09_16-47-51/error.txt  |
| train_b1c14_00017 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00017_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-09_18-31-48/error.txt |
| train_b1c14_00022 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00022_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-09_22-56-53/error.txt |
| train_b1c14_00023 |            1 | /home/user/brank/ray_results/experiment_2/train_b1c14_00023_23_batch_size=5,dropout_p=0.21574,layers=feat_heavy,lr=0.0021063,num_warmup=8.0,step_lr=45.0_2021-09-10_00-08-33/error.txt  |
+-------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

==========================================train errored trials====================================== Status ==
Memory usage on this node: 20.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.16 GiB heap, 0.0/70.49 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_2e5e39b2    | RUNNING    |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_2e5e5adc    | PENDING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_2e5e7a08    | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_2e5e9ce0    | PENDING    |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_2e5ebec8    | PENDING    |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_2e5ee57e    | PENDING    |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_2e5f0ed2    | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_2e5f3312    | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_2e5f5310    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_2e5f755c    | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_2e5f955a    | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=10619)[0m Epoch:
[2m[36m(pid=10619)[0m 0, 
== Status ==
Memory usage on this node: 22.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.16 GiB heap, 0.0/70.49 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_2e5e39b2    | RUNNING    |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_2e5e5adc    | PENDING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_2e5e7a08    | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_2e5e9ce0    | PENDING    |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_2e5ebec8    | PENDING    |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_2e5ee57e    | PENDING    |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_2e5f0ed2    | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_2e5f3312    | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_2e5f5310    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_2e5f755c    | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_2e5f955a    | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


Result for train_2e5e39b2:
  {}
  
[2m[36m(pid=10618)[0m Epoch:
[2m[36m(pid=10618)[0m 0, 
Result for train_2e5e5adc:
  {}
  
== Status ==
Memory usage on this node: 22.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/155.16 GiB heap, 0.0/70.49 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 ERROR, 12 PENDING, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_2e5e7a08    | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_2e5e9ce0    | PENDING    |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_2e5ebec8    | PENDING    |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_2e5ee57e    | PENDING    |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_2e5f0ed2    | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_2e5f3312    | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_2e5f5310    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_2e5f755c    | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_2e5f955a    | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_2e5e39b2    | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_2e5e5adc    | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 2 TERMINATED)
Number of errored trials: 2
+----------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name     |   # failures | error file                                                                                                                                                                          |
|----------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_2e5e39b2 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e39b2_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-10_15-37-02/error.txt |
| train_2e5e5adc |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e5adc_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-10_15-37-02/error.txt  |
+----------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=10620)[0m Epoch:
[2m[36m(pid=10620)[0m 0, 
Result for train_2e5e7a08:
  {}
  
== Status ==
Memory usage on this node: 22.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/155.16 GiB heap, 0.0/70.49 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 ERROR, 11 PENDING, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_2e5e9ce0    | PENDING    |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_2e5ebec8    | PENDING    |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_2e5ee57e    | PENDING    |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_2e5f0ed2    | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_2e5f3312    | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_2e5f5310    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_2e5f755c    | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_2e5f955a    | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_2e5fb72e    | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_2e5e39b2    | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_2e5e5adc    | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_2e5e7a08    | ERROR      |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)
Number of errored trials: 3
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name     |   # failures | error file                                                                                                                                                                           |
|----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_2e5e39b2 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e39b2_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-10_15-37-02/error.txt  |
| train_2e5e5adc |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e5adc_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-10_15-37-02/error.txt   |
| train_2e5e7a08 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e7a08_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-10_15-37-12/error.txt |
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=10568)[0m Epoch:
[2m[36m(pid=10568)[0m 0, 
Result for train_2e5e9ce0:
  {}
  
== Status ==
Memory usage on this node: 22.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/155.16 GiB heap, 0.0/70.49 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 ERROR, 10 PENDING, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_2e5ebec8    | PENDING    |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
| train_2e5ee57e    | PENDING    |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_2e5f0ed2    | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_2e5f3312    | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_2e5f5310    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_2e5f755c    | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_2e5f955a    | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_2e5fb72e    | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_2e5e39b2    | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_2e5e5adc    | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_2e5e7a08    | ERROR      |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_2e5e9ce0    | ERROR      |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (2 PENDING, 3 TERMINATED)
Number of errored trials: 4
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name     |   # failures | error file                                                                                                                                                                           |
|----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_2e5e39b2 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e39b2_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-10_15-37-02/error.txt  |
| train_2e5e5adc |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e5adc_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-10_15-37-02/error.txt   |
| train_2e5e7a08 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e7a08_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-10_15-37-12/error.txt |
| train_2e5e9ce0 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e9ce0_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-10_15-37-22/error.txt |
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

[2m[36m(pid=10622)[0m Epoch:
[2m[36m(pid=10622)[0m 0, 
Result for train_2e5ebec8:
  {}
  
== Status ==
Memory usage on this node: 22.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/155.16 GiB heap, 0.0/70.49 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 ERROR, 9 PENDING, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_2e5ee57e    | PENDING    |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_2e5f0ed2    | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_2e5f3312    | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_2e5f5310    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_2e5f755c    | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_2e5f955a    | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_2e5fb72e    | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_2e5fd682    | PENDING    |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_2e5e39b2    | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_2e5e5adc    | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_2e5e7a08    | ERROR      |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_2e5e9ce0    | ERROR      |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_2e5ebec8    | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)
Number of errored trials: 5
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name     |   # failures | error file                                                                                                                                                                           |
|----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_2e5e39b2 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e39b2_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-10_15-37-02/error.txt  |
| train_2e5e5adc |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e5adc_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-10_15-37-02/error.txt   |
| train_2e5e7a08 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e7a08_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-10_15-37-12/error.txt |
| train_2e5e9ce0 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e9ce0_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-10_15-37-22/error.txt |
| train_2e5ebec8 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5ebec8_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-10_15-37-32/error.txt  |
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

== Status ==
Memory usage on this node: 21.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/155.16 GiB heap, 0.0/70.49 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 ERROR, 9 PENDING, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_2e5ee57e    | PENDING    |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_2e5f0ed2    | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_2e5f3312    | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_2e5f5310    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_2e5f755c    | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_2e5f955a    | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_2e5fb72e    | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_2e5fd682    | PENDING    |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_2e5ff572    | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_b1c14_00024 | TERMINATED |       |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     24 |          6747.91 |  0.749863 | 0.0519572 |
| train_b1c14_00021 | TERMINATED |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_2e5e39b2    | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |
| train_2e5e5adc    | ERROR      |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_2e5e7a08    | ERROR      |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_2e5e9ce0    | ERROR      |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |
| train_2e5ebec8    | ERROR      |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
Number of errored trials: 5
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name     |   # failures | error file                                                                                                                                                                           |
|----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_2e5e39b2 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e39b2_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-10_15-37-02/error.txt  |
| train_2e5e5adc |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e5adc_10_batch_size=5,dropout_p=0.23916,layers=feat_heavy,lr=0.004075,num_warmup=8.0,step_lr=87.0_2021-09-10_15-37-02/error.txt   |
| train_2e5e7a08 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e7a08_17_batch_size=5,dropout_p=0.18889,layers=cost_heavy,lr=5.3451e-05,num_warmup=8.0,step_lr=70.0_2021-09-10_15-37-12/error.txt |
| train_2e5e9ce0 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5e9ce0_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-10_15-37-22/error.txt |
| train_2e5ebec8 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5ebec8_3_batch_size=5,dropout_p=0.11649,layers=feat_heavy,lr=2.0599e-05,num_warmup=6.0,step_lr=64.0_2021-09-10_15-37-32/error.txt  |
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Best trial config: {'lr': 0.0013694229386422441, 'layers': 'feat_heavy', 'batch_size': 4, 'step_lr': 52.0, 'num_warmup': 4.0, 'dropout_p': 0.09117406501677668}
Best trial final validation loss: 0.7498626518249512
Best trial final validation error: 0.05195723533630371
== Status ==
Memory usage on this node: 20.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_58e329d6    | RUNNING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_2e5ee57e    | PENDING    |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_2e5f0ed2    | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_2e5f3312    | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_2e5f5310    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_2e5f755c    | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_2e5f955a    | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_2e5fb72e    | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_2e5fd682    | PENDING    |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_2e5ff572    | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_58e34cae    | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m Epoch:
[2m[36m(pid=12124)[0m 0, 
== Status ==
Memory usage on this node: 22.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------|
| train_58e329d6    | RUNNING    |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |        |                  |           |           |
| train_2e5ee57e    | PENDING    |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |
| train_2e5f0ed2    | PENDING    |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |
| train_2e5f3312    | PENDING    |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |
| train_2e5f5310    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |
| train_2e5f755c    | PENDING    |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |
| train_2e5f955a    | PENDING    |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |
| train_2e5fb72e    | PENDING    |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |
| train_2e5fd682    | PENDING    |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |
| train_2e5ff572    | PENDING    |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |
| train_58e34cae    | PENDING    |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |
| train_b1c14_00021 | TERMINATED |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m  train loss: 29.20539093017578
[2m[36m(pid=12124)[0m  eval loss: 115.26538482666015, eval err: 0.996777150630951
Result for train_58e329d6:
  date: 2021-09-10_15-40-15
  done: false
  err: 0.996777150630951
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 115.26538482666015
  loss_train: 29.20539093017578
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 119.22274112701416
  time_this_iter_s: 119.22274112701416
  time_total_s: 119.22274112701416
  timestamp: 1631281215
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |      1 |          119.223 | 115.265    | 0.996777  |      29.2054 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |            |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |            |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |            |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |            |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |            |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  |  20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |   0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |         4269.56  |  32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |         4297.5   |   6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 1, 
[2m[36m(pid=12124)[0m  train loss: 14.584213523864745
[2m[36m(pid=12124)[0m  eval loss: 14.595355415344239, eval err: 0.872953872680664
[2m[36m(pid=12124)[0m 2, 
[2m[36m(pid=12124)[0m  train loss: 12.604749259948731
[2m[36m(pid=12124)[0m  eval loss: 11.29376377105713, eval err: 0.8436350321769714
[2m[36m(pid=12124)[0m 3, 
[2m[36m(pid=12124)[0m  train loss: 11.624659957885742
[2m[36m(pid=12124)[0m  eval loss: 12.14592758178711, eval err: 0.8607143473625183
Result for train_58e329d6:
  date: 2021-09-10_15-46-07
  done: false
  err: 0.8607143473625183
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 12.14592758178711
  loss_train: 11.624659957885742
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 471.6368877887726
  time_this_iter_s: 352.4141466617584
  time_total_s: 471.6368877887726
  timestamp: 1631281567
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |      2 |          471.637 | 12.1459   | 0.860714  |      11.6247 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |         4269.56  | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |         4297.5   |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 4, 
[2m[36m(pid=12124)[0m  train loss: 12.132878799438476
[2m[36m(pid=12124)[0m  eval loss: 14.300012397766114, eval err: 0.8714227175712586
[2m[36m(pid=12124)[0m 5, 
[2m[36m(pid=12124)[0m  train loss: 11.866327056884765
[2m[36m(pid=12124)[0m  eval loss: 13.710108909606934, eval err: 0.8625263166427612
[2m[36m(pid=12124)[0m 6, 
[2m[36m(pid=12124)[0m  train loss: 11.2384010887146
[2m[36m(pid=12124)[0m  eval loss: 14.205239524841309, eval err: 0.8712968444824218
Result for train_58e329d6:
  date: 2021-09-10_15-51-57
  done: false
  err: 0.8712968444824218
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 14.205239524841309
  loss_train: 11.2384010887146
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 821.3334314823151
  time_this_iter_s: 349.6965436935425
  time_total_s: 821.3334314823151
  timestamp: 1631281917
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |      3 |          821.333 | 14.2052   | 0.871297  |      11.2384 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |         4269.56  | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |         4297.5   |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 7, 
[2m[36m(pid=12124)[0m  train loss: 11.023942394256592
[2m[36m(pid=12124)[0m  eval loss: 12.380984115600587, eval err: 0.8427745699882507
[2m[36m(pid=12124)[0m 8, 
[2m[36m(pid=12124)[0m  train loss: 10.722318153381348
[2m[36m(pid=12124)[0m  eval loss: 11.72455810546875, eval err: 0.8400165104866028
[2m[36m(pid=12124)[0m 9, 
[2m[36m(pid=12124)[0m  train loss: 9.660768280029297
[2m[36m(pid=12124)[0m  eval loss: 10.953700332641601, eval err: 0.8363329410552979
Result for train_58e329d6:
  date: 2021-09-10_15-57-45
  done: false
  err: 0.8363329410552979
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 10.953700332641601
  loss_train: 9.660768280029297
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 1169.9145066738129
  time_this_iter_s: 348.5810751914978
  time_total_s: 1169.9145066738129
  timestamp: 1631282265
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |      4 |          1169.91 | 10.9537   | 0.836333  |      9.66077 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 10, 
[2m[36m(pid=12124)[0m  train loss: 10.548341636657716
[2m[36m(pid=12124)[0m  eval loss: 11.926020317077636, eval err: 0.8467379307746887
[2m[36m(pid=12124)[0m 11, 
[2m[36m(pid=12124)[0m  train loss: 10.651743984222412
[2m[36m(pid=12124)[0m  eval loss: 11.77247959136963, eval err: 0.8470114755630493
[2m[36m(pid=12124)[0m 12, 
[2m[36m(pid=12124)[0m  train loss: 9.575772476196288
Result for train_58e329d6:
  date: 2021-09-10_16-03-33
  done: false
  err: 0.863448622226715
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 12.511354141235351
  loss_train: 9.575772476196288
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 1517.850498199463
  time_this_iter_s: 347.93599152565
  time_total_s: 1517.850498199463
  timestamp: 1631282613
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 58e329d6
  
[2m[36m(pid=12124)[0m  eval loss: 12.511354141235351, eval err: 0.863448622226715
== Status ==
Memory usage on this node: 22.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |      5 |          1517.85 | 12.5114   | 0.863449  |      9.57577 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 13, 
[2m[36m(pid=12124)[0m  train loss: 9.903924522399903
[2m[36m(pid=12124)[0m  eval loss: 9.734456100463866, eval err: 0.8314683508872985
[2m[36m(pid=12124)[0m 14, 
[2m[36m(pid=12124)[0m  train loss: 10.482943515777588
[2m[36m(pid=12124)[0m  eval loss: 13.137925262451171, eval err: 0.8603908228874206
[2m[36m(pid=12124)[0m 15, 
[2m[36m(pid=12124)[0m  train loss: 9.984914073944092
[2m[36m(pid=12124)[0m  eval loss: 13.912883148193359, eval err: 0.8786893606185913
Result for train_58e329d6:
  date: 2021-09-10_16-09-21
  done: false
  err: 0.8786893606185913
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 13.912883148193359
  loss_train: 9.984914073944092
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 1865.4263792037964
  time_this_iter_s: 347.5758810043335
  time_total_s: 1865.4263792037964
  timestamp: 1631282961
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |      6 |          1865.43 | 13.9129   | 0.878689  |      9.98491 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 16, 
[2m[36m(pid=12124)[0m  train loss: 9.86710350036621
[2m[36m(pid=12124)[0m  eval loss: 13.88867733001709, eval err: 0.8809859490394593
[2m[36m(pid=12124)[0m 17, 
[2m[36m(pid=12124)[0m  train loss: 9.197660655975342
[2m[36m(pid=12124)[0m  eval loss: 11.703666343688965, eval err: 0.8558664608001709
[2m[36m(pid=12124)[0m 18, 
[2m[36m(pid=12124)[0m  train loss: 9.987938976287841
[2m[36m(pid=12124)[0m  eval loss: 11.11583667755127, eval err: 0.860081160068512
Result for train_58e329d6:
  date: 2021-09-10_16-15-08
  done: false
  err: 0.860081160068512
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 11.11583667755127
  loss_train: 9.987938976287841
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 2212.541026830673
  time_this_iter_s: 347.11464762687683
  time_total_s: 2212.541026830673
  timestamp: 1631283308
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |      7 |          2212.54 | 11.1158   | 0.860081  |      9.98794 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 19, 
[2m[36m(pid=12124)[0m  train loss: 9.939326248168946
[2m[36m(pid=12124)[0m  eval loss: 11.817001876831055, eval err: 0.8711186623573304
[2m[36m(pid=12124)[0m 20, 
[2m[36m(pid=12124)[0m  train loss: 9.820637798309326
[2m[36m(pid=12124)[0m  eval loss: 12.445253829956055, eval err: 0.8731395697593689
[2m[36m(pid=12124)[0m 21, 
[2m[36m(pid=12124)[0m  train loss: 9.534534320831298
[2m[36m(pid=12124)[0m  eval loss: 11.670576057434083, eval err: 0.8673109459877014
Result for train_58e329d6:
  date: 2021-09-10_16-20-57
  done: false
  err: 0.8673109459877014
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 11.670576057434083
  loss_train: 9.534534320831298
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 2561.578362941742
  time_this_iter_s: 349.0373361110687
  time_total_s: 2561.578362941742
  timestamp: 1631283657
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |      8 |          2561.58 | 11.6706   | 0.867311  |      9.53453 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 22, 
[2m[36m(pid=12124)[0m  train loss: 9.400089206695556
[2m[36m(pid=12124)[0m  eval loss: 12.683458480834961, eval err: 0.8734278845787048
[2m[36m(pid=12124)[0m 23, 
[2m[36m(pid=12124)[0m  train loss: 9.40812343597412
[2m[36m(pid=12124)[0m  eval loss: 11.310678329467773, eval err: 0.8654741072654724
[2m[36m(pid=12124)[0m 24, 
[2m[36m(pid=12124)[0m  train loss: 9.48719373703003
[2m[36m(pid=12124)[0m  eval loss: 12.36613468170166, eval err: 0.8699573087692261
Result for train_58e329d6:
  date: 2021-09-10_16-26-45
  done: false
  err: 0.8699573087692261
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 12.36613468170166
  loss_train: 9.48719373703003
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 2909.4942700862885
  time_this_iter_s: 347.9159071445465
  time_total_s: 2909.4942700862885
  timestamp: 1631284005
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |      9 |          2909.49 | 12.3661   | 0.869957  |      9.48719 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 25, 
[2m[36m(pid=12124)[0m  train loss: 8.98488775253296
[2m[36m(pid=12124)[0m  eval loss: 11.780824279785156, eval err: 0.860797049999237
[2m[36m(pid=12124)[0m 26, 
[2m[36m(pid=12124)[0m  train loss: 8.786272277832031
[2m[36m(pid=12124)[0m  eval loss: 10.90049114227295, eval err: 0.8535490775108338
[2m[36m(pid=12124)[0m 27, 
[2m[36m(pid=12124)[0m  train loss: 8.877327785491943
[2m[36m(pid=12124)[0m  eval loss: 13.230787086486817, eval err: 0.8735481929779053
Result for train_58e329d6:
  date: 2021-09-10_16-32-33
  done: false
  err: 0.8735481929779053
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 13.230787086486817
  loss_train: 8.877327785491943
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 3257.664615869522
  time_this_iter_s: 348.17034578323364
  time_total_s: 3257.664615869522
  timestamp: 1631284353
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     10 |          3257.66 | 13.2308   | 0.873548  |      8.87733 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 28, 
[2m[36m(pid=12124)[0m  train loss: 10.10965835571289
[2m[36m(pid=12124)[0m  eval loss: 11.547765769958495, eval err: 0.8503602004051208
[2m[36m(pid=12124)[0m 29, 
[2m[36m(pid=12124)[0m  train loss: 9.60194953918457
[2m[36m(pid=12124)[0m  eval loss: 10.77472141265869, eval err: 0.8636934018135071
[2m[36m(pid=12124)[0m 30, 
[2m[36m(pid=12124)[0m  train loss: 8.854324779510499
Result for train_58e329d6:
  date: 2021-09-10_16-38-22
  done: false
  err: 0.8515211319923401
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 11.057388076782226
  loss_train: 8.854324779510499
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 3606.3606929779053
  time_this_iter_s: 348.6960771083832
  time_total_s: 3606.3606929779053
  timestamp: 1631284702
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 58e329d6
  
[2m[36m(pid=12124)[0m  eval loss: 11.057388076782226, eval err: 0.8515211319923401
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     11 |          3606.36 | 11.0574   | 0.851521  |      8.85432 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 31, 
[2m[36m(pid=12124)[0m  train loss: 9.050338153839112
[2m[36m(pid=12124)[0m  eval loss: 10.97031135559082, eval err: 0.853037667274475
[2m[36m(pid=12124)[0m 32, 
[2m[36m(pid=12124)[0m  train loss: 8.550669078826905
[2m[36m(pid=12124)[0m  eval loss: 12.240994338989259, eval err: 0.8639026689529419
[2m[36m(pid=12124)[0m 33, 
[2m[36m(pid=12124)[0m  train loss: 8.86508409500122
[2m[36m(pid=12124)[0m  eval loss: 9.757469215393066, eval err: 0.8427522873878479
Result for train_58e329d6:
  date: 2021-09-10_16-44-11
  done: false
  err: 0.8427522873878479
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 9.757469215393066
  loss_train: 8.86508409500122
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 3955.011308193207
  time_this_iter_s: 348.6506152153015
  time_total_s: 3955.011308193207
  timestamp: 1631285051
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     12 |          3955.01 |  9.75747  | 0.842752  |      8.86508 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 34, 
[2m[36m(pid=12124)[0m  train loss: 8.439781703948974
[2m[36m(pid=12124)[0m  eval loss: 10.37144744873047, eval err: 0.8334350204467773
[2m[36m(pid=12124)[0m 35, 
[2m[36m(pid=12124)[0m  train loss: 8.826429233551025
[2m[36m(pid=12124)[0m  eval loss: 8.690924739837646, eval err: 0.8031251883506775
[2m[36m(pid=12124)[0m 36, 
[2m[36m(pid=12124)[0m  train loss: 8.83622631072998
[2m[36m(pid=12124)[0m  eval loss: 10.389296188354493, eval err: 0.8199331068992615
Result for train_58e329d6:
  date: 2021-09-10_16-49-59
  done: false
  err: 0.8199331068992615
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 10.389296188354493
  loss_train: 8.83622631072998
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 4303.252903223038
  time_this_iter_s: 348.24159502983093
  time_total_s: 4303.252903223038
  timestamp: 1631285399
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     13 |          4303.25 | 10.3893   | 0.819933  |      8.83623 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 37, 
[2m[36m(pid=12124)[0m  train loss: 7.889794292449952
[2m[36m(pid=12124)[0m  eval loss: 7.989284019470215, eval err: 0.7539683413505555
[2m[36m(pid=12124)[0m 38, 
[2m[36m(pid=12124)[0m  train loss: 7.173025894165039
[2m[36m(pid=12124)[0m  eval loss: 5.882515649795533, eval err: 0.6154146838188171
[2m[36m(pid=12124)[0m 39, 
[2m[36m(pid=12124)[0m  train loss: 5.746240310668945
[2m[36m(pid=12124)[0m  eval loss: 3.934867248535156, eval err: 0.43274216175079344
Result for train_58e329d6:
  date: 2021-09-10_16-55-46
  done: false
  err: 0.43274216175079344
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 3.934867248535156
  loss_train: 5.746240310668945
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 4649.9918892383575
  time_this_iter_s: 346.7389860153198
  time_total_s: 4649.9918892383575
  timestamp: 1631285746
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     14 |          4649.99 |  3.93487  | 0.432742  |      5.74624 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 40, 
[2m[36m(pid=12124)[0m  train loss: 4.732672567367554
[2m[36m(pid=12124)[0m  eval loss: 2.0947112941741945, eval err: 0.17572503566741943
[2m[36m(pid=12124)[0m 41, 
[2m[36m(pid=12124)[0m  train loss: 3.2446936559677124
[2m[36m(pid=12124)[0m  eval loss: 2.242894902229309, eval err: 0.21177443265914916
[2m[36m(pid=12124)[0m 42, 
[2m[36m(pid=12124)[0m  train loss: 3.2297786617279054
[2m[36m(pid=12124)[0m  eval loss: 1.5679964351654052, eval err: 0.12819704055786132
Result for train_58e329d6:
  date: 2021-09-10_17-01-27
  done: false
  err: 0.12819704055786132
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 1.5679964351654052
  loss_train: 3.2297786617279054
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 4991.440062999725
  time_this_iter_s: 341.4481737613678
  time_total_s: 4991.440062999725
  timestamp: 1631286087
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     15 |          4991.44 |  1.568    | 0.128197  |      3.22978 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 43, 
[2m[36m(pid=12124)[0m  train loss: 2.8430266046524046
[2m[36m(pid=12124)[0m  eval loss: 1.7219809842109681, eval err: 0.13067127227783204
[2m[36m(pid=12124)[0m 44, 
[2m[36m(pid=12124)[0m  train loss: 2.615914936065674
[2m[36m(pid=12124)[0m  eval loss: 1.7113765740394593, eval err: 0.13437196254730224
[2m[36m(pid=12124)[0m 45, 
[2m[36m(pid=12124)[0m  train loss: 2.3827572774887087
[2m[36m(pid=12124)[0m  eval loss: 1.708573386669159, eval err: 0.13282777786254882
Result for train_58e329d6:
  date: 2021-09-10_17-07-06
  done: false
  err: 0.13282777786254882
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 1.708573386669159
  loss_train: 2.3827572774887087
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 5330.3904049396515
  time_this_iter_s: 338.95034193992615
  time_total_s: 5330.3904049396515
  timestamp: 1631286426
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     16 |          5330.39 |  1.70857  | 0.132828  |      2.38276 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 46, 
[2m[36m(pid=12124)[0m  train loss: 2.3519897651672363
[2m[36m(pid=12124)[0m  eval loss: 1.4684375739097595, eval err: 0.10731464385986328
[2m[36m(pid=12124)[0m 47, 
[2m[36m(pid=12124)[0m  train loss: 2.277651538848877
[2m[36m(pid=12124)[0m  eval loss: 1.2806942176818847, eval err: 0.09484115600585938
[2m[36m(pid=12124)[0m 48, 
[2m[36m(pid=12124)[0m  train loss: 2.0287071371078493
[2m[36m(pid=12124)[0m  eval loss: 1.316555185317993, eval err: 0.09656819581985474
Result for train_58e329d6:
  date: 2021-09-10_17-12-45
  done: false
  err: 0.09656819581985474
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 1.316555185317993
  loss_train: 2.0287071371078493
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 5669.091048002243
  time_this_iter_s: 338.70064306259155
  time_total_s: 5669.091048002243
  timestamp: 1631286765
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     17 |          5669.09 |  1.31656  | 0.0965682 |      2.02871 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 49, 
[2m[36m(pid=12124)[0m  train loss: 2.3734200382232666
[2m[36m(pid=12124)[0m  eval loss: 1.581305799484253, eval err: 0.11381215333938599
[2m[36m(pid=12124)[0m 50, 
[2m[36m(pid=12124)[0m  train loss: 2.2116673517227174
[2m[36m(pid=12124)[0m  eval loss: 1.3530251574516297, eval err: 0.09612272262573242
[2m[36m(pid=12124)[0m 51, 
[2m[36m(pid=12124)[0m  train loss: 1.8439086627960206
[2m[36m(pid=12124)[0m  eval loss: 1.1919877171516418, eval err: 0.08690885305404664
Result for train_58e329d6:
  date: 2021-09-10_17-18-23
  done: false
  err: 0.08690885305404664
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 1.1919877171516418
  loss_train: 1.8439086627960206
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 6007.505937099457
  time_this_iter_s: 338.41488909721375
  time_total_s: 6007.505937099457
  timestamp: 1631287103
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 19.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     18 |          6007.51 |  1.19199  | 0.0869089 |      1.84391 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 52, 
[2m[36m(pid=12124)[0m  train loss: 1.8952564597129822
[2m[36m(pid=12124)[0m  eval loss: 1.2273402547836303, eval err: 0.08589277744293213
[2m[36m(pid=12124)[0m 53, 
[2m[36m(pid=12124)[0m  train loss: 1.763712739944458
[2m[36m(pid=12124)[0m  eval loss: 1.1453875470161439, eval err: 0.08079123973846436
[2m[36m(pid=12124)[0m 54, 
[2m[36m(pid=12124)[0m  train loss: 2.1195069980621337
[2m[36m(pid=12124)[0m  eval loss: 1.1623915719985962, eval err: 0.08156106948852539
Result for train_58e329d6:
  date: 2021-09-10_17-24-03
  done: false
  err: 0.08156106948852539
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 1.1623915719985962
  loss_train: 2.1195069980621337
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 6347.54536652565
  time_this_iter_s: 340.03942942619324
  time_total_s: 6347.54536652565
  timestamp: 1631287443
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 20.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     19 |          6347.55 |  1.16239  | 0.0815611 |      2.11951 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 55, 
[2m[36m(pid=12124)[0m  train loss: 1.863541178703308
[2m[36m(pid=12124)[0m  eval loss: 1.1479300665855408, eval err: 0.08355772733688355
[2m[36m(pid=12124)[0m 56, 
[2m[36m(pid=12124)[0m  train loss: 1.8843153238296508
[2m[36m(pid=12124)[0m  eval loss: 1.155824453830719, eval err: 0.08080552577972412
[2m[36m(pid=12124)[0m 57, 
[2m[36m(pid=12124)[0m  train loss: 2.0071046686172487
[2m[36m(pid=12124)[0m  eval loss: 1.1353789448738099, eval err: 0.08111955165863037
Result for train_58e329d6:
  date: 2021-09-10_17-29-41
  done: false
  err: 0.08111955165863037
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 1.1353789448738099
  loss_train: 2.0071046686172487
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 6685.506569862366
  time_this_iter_s: 337.9612033367157
  time_total_s: 6685.506569862366
  timestamp: 1631287781
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 20.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     20 |          6685.51 |  1.13538  | 0.0811196 |       2.0071 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 58, 
[2m[36m(pid=12124)[0m  train loss: 1.6155417203903197
[2m[36m(pid=12124)[0m  eval loss: 1.073045961856842, eval err: 0.07448085069656372
[2m[36m(pid=12124)[0m 59, 
[2m[36m(pid=12124)[0m  train loss: 1.7193785405158997
[2m[36m(pid=12124)[0m  eval loss: 1.1367918419837952, eval err: 0.08000179529190063
[2m[36m(pid=12124)[0m 60, 
[2m[36m(pid=12124)[0m  train loss: 1.587910029888153
[2m[36m(pid=12124)[0m  eval loss: 1.1062053608894349, eval err: 0.0786538028717041
Result for train_58e329d6:
  date: 2021-09-10_17-35-18
  done: false
  err: 0.0786538028717041
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 1.1062053608894349
  loss_train: 1.587910029888153
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 7022.241566419601
  time_this_iter_s: 336.7349965572357
  time_total_s: 7022.241566419601
  timestamp: 1631288118
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 20.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     21 |          7022.24 |  1.10621  | 0.0786538 |      1.58791 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 61, 
[2m[36m(pid=12124)[0m  train loss: 1.733566677570343
[2m[36m(pid=12124)[0m  eval loss: 1.0719353461265564, eval err: 0.07586792469024659
[2m[36m(pid=12124)[0m 62, 
[2m[36m(pid=12124)[0m  train loss: 1.7785995626449584
[2m[36m(pid=12124)[0m  eval loss: 1.094841594696045, eval err: 0.07424824714660644
[2m[36m(pid=12124)[0m 63, 
[2m[36m(pid=12124)[0m  train loss: 1.5158264255523681
[2m[36m(pid=12124)[0m  eval loss: 1.0584395337104797, eval err: 0.07177232503890991
Result for train_58e329d6:
  date: 2021-09-10_17-40-54
  done: false
  err: 0.07177232503890991
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 22
  loss: 1.0584395337104797
  loss_train: 1.5158264255523681
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 7358.561385631561
  time_this_iter_s: 336.31981921195984
  time_total_s: 7358.561385631561
  timestamp: 1631288454
  timesteps_since_restore: 0
  training_iteration: 22
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     22 |          7358.56 |  1.05844  | 0.0717723 |      1.51583 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 64, 
[2m[36m(pid=12124)[0m  train loss: 1.7823916339874268
[2m[36m(pid=12124)[0m  eval loss: 1.0865286302566528, eval err: 0.07694757223129273
[2m[36m(pid=12124)[0m 65, 
[2m[36m(pid=12124)[0m  train loss: 1.6474667835235595
[2m[36m(pid=12124)[0m  eval loss: 1.0466605234146118, eval err: 0.070087251663208
[2m[36m(pid=12124)[0m 66, 
[2m[36m(pid=12124)[0m  train loss: 1.5829070520401
[2m[36m(pid=12124)[0m  eval loss: 1.0224181914329529, eval err: 0.07008868455886841
Result for train_58e329d6:
  date: 2021-09-10_17-46-43
  done: false
  err: 0.07008868455886841
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 23
  loss: 1.0224181914329529
  loss_train: 1.5829070520401
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 7707.527017593384
  time_this_iter_s: 348.9656319618225
  time_total_s: 7707.527017593384
  timestamp: 1631288803
  timesteps_since_restore: 0
  training_iteration: 23
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     23 |          7707.53 |  1.02242  | 0.0700887 |      1.58291 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 67, 
[2m[36m(pid=12124)[0m  train loss: 1.7943090915679931
[2m[36m(pid=12124)[0m  eval loss: 1.0731455636024476, eval err: 0.07554165601730346
[2m[36m(pid=12124)[0m 68, 
[2m[36m(pid=12124)[0m  train loss: 1.7300603246688844
[2m[36m(pid=12124)[0m  eval loss: 1.0724983382225037, eval err: 0.07214582443237305
[2m[36m(pid=12124)[0m 69, 
[2m[36m(pid=12124)[0m  train loss: 1.5565643548965453
[2m[36m(pid=12124)[0m  eval loss: 1.054670684337616, eval err: 0.07093438148498535
Result for train_58e329d6:
  date: 2021-09-10_17-52-35
  done: false
  err: 0.07093438148498535
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 24
  loss: 1.054670684337616
  loss_train: 1.5565643548965453
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 8059.885540962219
  time_this_iter_s: 352.35852336883545
  time_total_s: 8059.885540962219
  timestamp: 1631289155
  timesteps_since_restore: 0
  training_iteration: 24
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     24 |          8059.89 |  1.05467  | 0.0709344 |      1.55656 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 70, 
[2m[36m(pid=12124)[0m  train loss: 1.5831188130378724
[2m[36m(pid=12124)[0m  eval loss: 1.1073222947120667, eval err: 0.07493746280670166
[2m[36m(pid=12124)[0m 71, 
[2m[36m(pid=12124)[0m  train loss: 1.7232933855056762
[2m[36m(pid=12124)[0m  eval loss: 1.0517322659492492, eval err: 0.07367074728012085
[2m[36m(pid=12124)[0m 72, 
[2m[36m(pid=12124)[0m  train loss: 1.4557244849205018
[2m[36m(pid=12124)[0m  eval loss: 0.9734153366088867, eval err: 0.06627192497253417
Result for train_58e329d6:
  date: 2021-09-10_17-58-27
  done: false
  err: 0.06627192497253417
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 25
  loss: 0.9734153366088867
  loss_train: 1.4557244849205018
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 8411.68223452568
  time_this_iter_s: 351.7966935634613
  time_total_s: 8411.68223452568
  timestamp: 1631289507
  timesteps_since_restore: 0
  training_iteration: 25
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     25 |          8411.68 |  0.973415 | 0.0662719 |      1.45572 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 73, 
[2m[36m(pid=12124)[0m  train loss: 1.6391620421409607
[2m[36m(pid=12124)[0m  eval loss: 0.99664963722229, eval err: 0.06821125984191895
[2m[36m(pid=12124)[0m 74, 
[2m[36m(pid=12124)[0m  train loss: 1.493332278728485
[2m[36m(pid=12124)[0m  eval loss: 1.002497251033783, eval err: 0.067646005153656
[2m[36m(pid=12124)[0m 75, 
[2m[36m(pid=12124)[0m  train loss: 1.4744377112388611
[2m[36m(pid=12124)[0m  eval loss: 1.0056980156898498, eval err: 0.06731209993362426
Result for train_58e329d6:
  date: 2021-09-10_18-04-20
  done: false
  err: 0.06731209993362426
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 26
  loss: 1.0056980156898498
  loss_train: 1.4744377112388611
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 8764.069534540176
  time_this_iter_s: 352.38730001449585
  time_total_s: 8764.069534540176
  timestamp: 1631289860
  timesteps_since_restore: 0
  training_iteration: 26
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     26 |          8764.07 |  1.0057   | 0.0673121 |      1.47444 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 76, 
[2m[36m(pid=12124)[0m  train loss: 1.5008053159713746
[2m[36m(pid=12124)[0m  eval loss: 1.017528953552246, eval err: 0.06960906744003297
[2m[36m(pid=12124)[0m 77, 
[2m[36m(pid=12124)[0m  train loss: 1.4665896773338318
[2m[36m(pid=12124)[0m  eval loss: 0.9730836641788483, eval err: 0.06488434076309205
[2m[36m(pid=12124)[0m 78, 
[2m[36m(pid=12124)[0m  train loss: 1.4330804228782654
[2m[36m(pid=12124)[0m  eval loss: 0.9387475240230561, eval err: 0.06283385515213012
Result for train_58e329d6:
  date: 2021-09-10_18-10-12
  done: false
  err: 0.06283385515213012
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 27
  loss: 0.9387475240230561
  loss_train: 1.4330804228782654
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 9116.620358228683
  time_this_iter_s: 352.5508236885071
  time_total_s: 9116.620358228683
  timestamp: 1631290212
  timesteps_since_restore: 0
  training_iteration: 27
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     27 |          9116.62 |  0.938748 | 0.0628339 |      1.43308 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 79, 
[2m[36m(pid=12124)[0m  train loss: 1.4709701943397522
[2m[36m(pid=12124)[0m  eval loss: 1.0261127400398253, eval err: 0.0687289834022522
[2m[36m(pid=12124)[0m 80, 
[2m[36m(pid=12124)[0m  train loss: 1.3926254177093507
[2m[36m(pid=12124)[0m  eval loss: 0.9431522595882416, eval err: 0.0645083737373352
[2m[36m(pid=12124)[0m 81, 
[2m[36m(pid=12124)[0m  train loss: 1.3758073687553405
[2m[36m(pid=12124)[0m  eval loss: 0.9873815488815307, eval err: 0.06880356788635254
Result for train_58e329d6:
  date: 2021-09-10_18-16-04
  done: false
  err: 0.06880356788635254
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 28
  loss: 0.9873815488815307
  loss_train: 1.3758073687553405
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 9468.364979028702
  time_this_iter_s: 351.7446208000183
  time_total_s: 9468.364979028702
  timestamp: 1631290564
  timesteps_since_restore: 0
  training_iteration: 28
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     28 |          9468.36 |  0.987382 | 0.0688036 |      1.37581 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 82, 
[2m[36m(pid=12124)[0m  train loss: 1.3199450182914734
[2m[36m(pid=12124)[0m  eval loss: 0.9465762674808502, eval err: 0.06382649898529053
[2m[36m(pid=12124)[0m 83, 
[2m[36m(pid=12124)[0m  train loss: 1.5016297435760497
[2m[36m(pid=12124)[0m  eval loss: 1.003454327583313, eval err: 0.07121827363967896
[2m[36m(pid=12124)[0m 84, 
[2m[36m(pid=12124)[0m  train loss: 1.435166199207306
[2m[36m(pid=12124)[0m  eval loss: 0.8891174614429473, eval err: 0.06025263547897339
Result for train_58e329d6:
  date: 2021-09-10_18-21-56
  done: false
  err: 0.06025263547897339
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 29
  loss: 0.8891174614429473
  loss_train: 1.435166199207306
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 9820.905587911606
  time_this_iter_s: 352.54060888290405
  time_total_s: 9820.905587911606
  timestamp: 1631290916
  timesteps_since_restore: 0
  training_iteration: 29
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     29 |          9820.91 |  0.889117 | 0.0602526 |      1.43517 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 85, 
[2m[36m(pid=12124)[0m  train loss: 1.1901505637168883
[2m[36m(pid=12124)[0m  eval loss: 0.8863184297084808, eval err: 0.06015684127807617
[2m[36m(pid=12124)[0m 86, 
[2m[36m(pid=12124)[0m  train loss: 1.16699373960495
[2m[36m(pid=12124)[0m  eval loss: 0.8822816896438599, eval err: 0.060406720638275145
[2m[36m(pid=12124)[0m 87, 
[2m[36m(pid=12124)[0m  train loss: 1.4547065234184264
[2m[36m(pid=12124)[0m  eval loss: 0.8867430555820465, eval err: 0.058452119827270506
Result for train_58e329d6:
  date: 2021-09-10_18-27-49
  done: false
  err: 0.058452119827270506
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 30
  loss: 0.8867430555820465
  loss_train: 1.4547065234184264
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 10173.263362646103
  time_this_iter_s: 352.35777473449707
  time_total_s: 10173.263362646103
  timestamp: 1631291269
  timesteps_since_restore: 0
  training_iteration: 30
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     30 |         10173.3  |  0.886743 | 0.0584521 |      1.45471 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 88, 
[2m[36m(pid=12124)[0m  train loss: 1.2767196774482727
[2m[36m(pid=12124)[0m  eval loss: 0.9079975128173828, eval err: 0.06318537235260009
[2m[36m(pid=12124)[0m 89, 
[2m[36m(pid=12124)[0m  train loss: 1.3419767236709594
[2m[36m(pid=12124)[0m  eval loss: 0.8888929879665375, eval err: 0.059244010448455814
[2m[36m(pid=12124)[0m 90, 
[2m[36m(pid=12124)[0m  train loss: 1.2640469837188721
[2m[36m(pid=12124)[0m  eval loss: 0.8720636570453644, eval err: 0.058972620964050294
Result for train_58e329d6:
  date: 2021-09-10_18-33-41
  done: false
  err: 0.058972620964050294
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 31
  loss: 0.8720636570453644
  loss_train: 1.2640469837188721
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 10525.527683019638
  time_this_iter_s: 352.26432037353516
  time_total_s: 10525.527683019638
  timestamp: 1631291621
  timesteps_since_restore: 0
  training_iteration: 31
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 22.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     31 |         10525.5  |  0.872064 | 0.0589726 |      1.26405 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12124)[0m 91, 
[2m[36m(pid=12124)[0m  train loss: 1.2846697640419007
[2m[36m(pid=12124)[0m  eval loss: 0.8643961811065674, eval err: 0.05845968961715698
[2m[36m(pid=12124)[0m 92, 
[2m[36m(pid=12124)[0m  train loss: 1.3058060812950134
[2m[36m(pid=12124)[0m  eval loss: 0.8562070620059967, eval err: 0.05736047506332397
[2m[36m(pid=12124)[0m 93, 
[2m[36m(pid=12124)[0m  train loss: 1.2960734009742736
[2m[36m(pid=12124)[0m  eval loss: 0.8988266062736511, eval err: 0.05960246324539185
Result for train_58e329d6:
  date: 2021-09-10_18-39-33
  done: true
  err: 0.05960246324539185
  experiment_id: 197c2446e6464c2b84843def1facb881
  hostname: bigcuda4
  iterations_since_restore: 32
  loss: 0.8988266062736511
  loss_train: 1.2960734009742736
  node_ip: 131.220.7.54
  pid: 12124
  should_checkpoint: true
  time_since_restore: 10877.891565084457
  time_this_iter_s: 352.36388206481934
  time_total_s: 10877.891565084457
  timestamp: 1631291973
  timesteps_since_restore: 0
  training_iteration: 32
  trial_id: 58e329d6
  
== Status ==
Memory usage on this node: 23.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (13 PENDING, 1 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e329d6    | RUNNING    | 131.220.7.54:12124 |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e34cae    | PENDING    |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |        |                  |           |           |              |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00021 | TERMINATED |                    |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(pid=12122)[0m Epoch:
[2m[36m(pid=12122)[0m 0, 
[2m[36m(pid=12122)[0m  train loss: 29.38302810668945
[2m[36m(pid=12122)[0m  eval loss: 109.67204437255859, eval err: 0.9929694223403931
Result for train_58e34cae:
  date: 2021-09-10_18-41-54
  done: false
  err: 0.9929694223403931
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 109.67204437255859
  loss_train: 29.38302810668945
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 139.15381264686584
  time_this_iter_s: 139.15381264686584
  time_total_s: 139.15381264686584
  timestamp: 1631292114
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |      1 |          139.154 | 109.672    | 0.992969  |     29.383   |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |            |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |            |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |            |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |            |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |   0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  |  20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |   0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |         4269.56  |  32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 1, 
[2m[36m(pid=12122)[0m  train loss: 25.53654899597168
[2m[36m(pid=12122)[0m  eval loss: 105.44341400146484, eval err: 0.991638252735138
[2m[36m(pid=12122)[0m 2, 
[2m[36m(pid=12122)[0m  train loss: 16.79738067626953
[2m[36m(pid=12122)[0m  eval loss: 31.619994354248046, eval err: 0.9394523429870606
[2m[36m(pid=12122)[0m 3, 
[2m[36m(pid=12122)[0m  train loss: 13.360866584777831
[2m[36m(pid=12122)[0m  eval loss: 18.039815254211426, eval err: 0.8952469038963318
Result for train_58e34cae:
  date: 2021-09-10_18-48-39
  done: false
  err: 0.8952469038963318
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 18.039815254211426
  loss_train: 13.360866584777831
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 543.8534438610077
  time_this_iter_s: 404.69963121414185
  time_total_s: 543.8534438610077
  timestamp: 1631292519
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 24.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |      2 |          543.853 | 18.0398   | 0.895247  |     13.3609  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |         4269.56  | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 4, 
[2m[36m(pid=12122)[0m  train loss: 11.335907478332519
[2m[36m(pid=12122)[0m  eval loss: 20.89984375, eval err: 0.9149528431892395
[2m[36m(pid=12122)[0m 5, 
[2m[36m(pid=12122)[0m  train loss: 10.601532402038574
[2m[36m(pid=12122)[0m  eval loss: 19.6563419342041, eval err: 0.9135949730873107
[2m[36m(pid=12122)[0m 6, 
[2m[36m(pid=12122)[0m  train loss: 10.897301712036132
[2m[36m(pid=12122)[0m  eval loss: 21.180241088867188, eval err: 0.9269704294204711
Result for train_58e34cae:
  date: 2021-09-10_18-55-25
  done: false
  err: 0.9269704294204711
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 21.180241088867188
  loss_train: 10.897301712036132
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 949.585676908493
  time_this_iter_s: 405.73223304748535
  time_total_s: 949.585676908493
  timestamp: 1631292925
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 25.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |      3 |          949.586 | 21.1802   | 0.92697   |     10.8973  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |         4269.56  | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 7, 
[2m[36m(pid=12122)[0m  train loss: 10.49100772857666
[2m[36m(pid=12122)[0m  eval loss: 15.823635635375977, eval err: 0.8857457566261292
[2m[36m(pid=12122)[0m 8, 
[2m[36m(pid=12122)[0m  train loss: 9.784459857940673
[2m[36m(pid=12122)[0m  eval loss: 13.31980712890625, eval err: 0.864335560798645
[2m[36m(pid=12122)[0m 9, 
[2m[36m(pid=12122)[0m  train loss: 10.548104286193848
[2m[36m(pid=12122)[0m  eval loss: 15.40044288635254, eval err: 0.8781351208686828
Result for train_58e34cae:
  date: 2021-09-10_19-02-11
  done: false
  err: 0.8781351208686828
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 15.40044288635254
  loss_train: 10.548104286193848
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 1355.8078689575195
  time_this_iter_s: 406.2221920490265
  time_total_s: 1355.8078689575195
  timestamp: 1631293331
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |      4 |          1355.81 | 15.4004   | 0.878135  |     10.5481  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 10, 
[2m[36m(pid=12122)[0m  train loss: 10.054762001037597
[2m[36m(pid=12122)[0m  eval loss: 11.41211685180664, eval err: 0.84121737241745
[2m[36m(pid=12122)[0m 11, 
[2m[36m(pid=12122)[0m  train loss: 9.540563354492187
[2m[36m(pid=12122)[0m  eval loss: 15.038154220581054, eval err: 0.8727281212806701
[2m[36m(pid=12122)[0m 12, 
[2m[36m(pid=12122)[0m  train loss: 9.708775901794434
Result for train_58e34cae:
  date: 2021-09-10_19-08-57
  done: false
  err: 0.8531804132461548
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 12.148408584594726
  loss_train: 9.708775901794434
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 1761.7919745445251
  time_this_iter_s: 405.9841055870056
  time_total_s: 1761.7919745445251
  timestamp: 1631293737
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 58e34cae
  
[2m[36m(pid=12122)[0m  eval loss: 12.148408584594726, eval err: 0.8531804132461548
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |      5 |          1761.79 | 12.1484   | 0.85318   |      9.70878 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 13, 
[2m[36m(pid=12122)[0m  train loss: 9.788718872070312
[2m[36m(pid=12122)[0m  eval loss: 13.475135536193848, eval err: 0.8700451898574829
[2m[36m(pid=12122)[0m 14, 
[2m[36m(pid=12122)[0m  train loss: 8.986063404083252
[2m[36m(pid=12122)[0m  eval loss: 11.314455680847168, eval err: 0.8308567571640014
[2m[36m(pid=12122)[0m 15, 
[2m[36m(pid=12122)[0m  train loss: 9.487997035980225
[2m[36m(pid=12122)[0m  eval loss: 13.203598747253418, eval err: 0.8631256484985351
Result for train_58e34cae:
  date: 2021-09-10_19-15-43
  done: false
  err: 0.8631256484985351
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 13.203598747253418
  loss_train: 9.487997035980225
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 2167.506016254425
  time_this_iter_s: 405.7140417098999
  time_total_s: 2167.506016254425
  timestamp: 1631294143
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 25.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |      6 |          2167.51 | 13.2036   | 0.863126  |      9.488   |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 16, 
[2m[36m(pid=12122)[0m  train loss: 9.972641277313233
[2m[36m(pid=12122)[0m  eval loss: 14.485104370117188, eval err: 0.8579609513282775
[2m[36m(pid=12122)[0m 17, 
[2m[36m(pid=12122)[0m  train loss: 9.403934574127197
[2m[36m(pid=12122)[0m  eval loss: 10.904675178527832, eval err: 0.839634177684784
[2m[36m(pid=12122)[0m 18, 
[2m[36m(pid=12122)[0m  train loss: 9.32669361114502
Result for train_58e34cae:
  date: 2021-09-10_19-22-29
  done: false
  err: 0.848300724029541
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 11.55501121520996
  loss_train: 9.32669361114502
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 2573.367243528366
  time_this_iter_s: 405.86122727394104
  time_total_s: 2573.367243528366
  timestamp: 1631294549
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 58e34cae
  
[2m[36m(pid=12122)[0m  eval loss: 11.55501121520996, eval err: 0.848300724029541
== Status ==
Memory usage on this node: 25.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |      7 |          2573.37 | 11.555    | 0.848301  |      9.32669 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 19, 
[2m[36m(pid=12122)[0m  train loss: 8.822249412536621
[2m[36m(pid=12122)[0m  eval loss: 12.350250663757324, eval err: 0.851426408290863
[2m[36m(pid=12122)[0m 20, 
[2m[36m(pid=12122)[0m  train loss: 9.451168994903565
[2m[36m(pid=12122)[0m  eval loss: 11.814197540283203, eval err: 0.8572434091567993
[2m[36m(pid=12122)[0m 21, 
[2m[36m(pid=12122)[0m  train loss: 9.021895236968994
[2m[36m(pid=12122)[0m  eval loss: 14.026128158569335, eval err: 0.8623423838615417
Result for train_58e34cae:
  date: 2021-09-10_19-29-15
  done: false
  err: 0.8623423838615417
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 14.026128158569335
  loss_train: 9.021895236968994
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 2979.3749127388
  time_this_iter_s: 406.00766921043396
  time_total_s: 2979.3749127388
  timestamp: 1631294955
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 25.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |      8 |          2979.37 | 14.0261   | 0.862342  |      9.0219  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 22, 
[2m[36m(pid=12122)[0m  train loss: 9.405793437957763
[2m[36m(pid=12122)[0m  eval loss: 11.781893310546875, eval err: 0.8567402219772339
[2m[36m(pid=12122)[0m 23, 
[2m[36m(pid=12122)[0m  train loss: 9.465712642669677
[2m[36m(pid=12122)[0m  eval loss: 14.49484474182129, eval err: 0.8674017810821533
[2m[36m(pid=12122)[0m 24, 
[2m[36m(pid=12122)[0m  train loss: 9.10932502746582
[2m[36m(pid=12122)[0m  eval loss: 14.718564949035645, eval err: 0.8827032017707824
Result for train_58e34cae:
  date: 2021-09-10_19-36-01
  done: false
  err: 0.8827032017707824
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 14.718564949035645
  loss_train: 9.10932502746582
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 3385.6590507030487
  time_this_iter_s: 406.28413796424866
  time_total_s: 3385.6590507030487
  timestamp: 1631295361
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 25.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |      9 |          3385.66 | 14.7186   | 0.882703  |      9.10933 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 25, 
[2m[36m(pid=12122)[0m  train loss: 8.536724033355712
[2m[36m(pid=12122)[0m  eval loss: 10.769358005523681, eval err: 0.8466336154937744
[2m[36m(pid=12122)[0m 26, 
[2m[36m(pid=12122)[0m  train loss: 8.658641834259033
[2m[36m(pid=12122)[0m  eval loss: 13.43767723083496, eval err: 0.8608928227424621
[2m[36m(pid=12122)[0m 27, 
[2m[36m(pid=12122)[0m  train loss: 8.647761116027832
[2m[36m(pid=12122)[0m  eval loss: 13.704909439086913, eval err: 0.8647151923179627
Result for train_58e34cae:
  date: 2021-09-10_19-42-47
  done: false
  err: 0.8647151923179627
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 13.704909439086913
  loss_train: 8.647761116027832
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 3792.0748660564423
  time_this_iter_s: 406.41581535339355
  time_total_s: 3792.0748660564423
  timestamp: 1631295767
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 25.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     10 |          3792.07 | 13.7049   | 0.864715  |      8.64776 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 28, 
[2m[36m(pid=12122)[0m  train loss: 8.481108226776122
[2m[36m(pid=12122)[0m  eval loss: 12.655434112548829, eval err: 0.8575697350502014
[2m[36m(pid=12122)[0m 29, 
[2m[36m(pid=12122)[0m  train loss: 8.791396522521973
[2m[36m(pid=12122)[0m  eval loss: 11.339131546020507, eval err: 0.8431400990486145
[2m[36m(pid=12122)[0m 30, 
[2m[36m(pid=12122)[0m  train loss: 8.739514827728271
[2m[36m(pid=12122)[0m  eval loss: 11.93877191543579, eval err: 0.8514678859710694
Result for train_58e34cae:
  date: 2021-09-10_19-49-34
  done: false
  err: 0.8514678859710694
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 11.93877191543579
  loss_train: 8.739514827728271
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 4198.407238006592
  time_this_iter_s: 406.33237195014954
  time_total_s: 4198.407238006592
  timestamp: 1631296174
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 25.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     11 |          4198.41 | 11.9388   | 0.851468  |      8.73951 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 31, 
[2m[36m(pid=12122)[0m  train loss: 9.34297218322754
[2m[36m(pid=12122)[0m  eval loss: 9.899155616760254, eval err: 0.8279661083221436
[2m[36m(pid=12122)[0m 32, 
[2m[36m(pid=12122)[0m  train loss: 8.690581283569337
[2m[36m(pid=12122)[0m  eval loss: 11.09474464416504, eval err: 0.8473026943206787
[2m[36m(pid=12122)[0m 33, 
[2m[36m(pid=12122)[0m  train loss: 9.256638374328613
[2m[36m(pid=12122)[0m  eval loss: 11.824560775756837, eval err: 0.8575601863861084
Result for train_58e34cae:
  date: 2021-09-10_19-56-20
  done: false
  err: 0.8575601863861084
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 11.824560775756837
  loss_train: 9.256638374328613
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 4605.14564371109
  time_this_iter_s: 406.7384057044983
  time_total_s: 4605.14564371109
  timestamp: 1631296580
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 25.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     12 |          4605.15 | 11.8246   | 0.85756   |      9.25664 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 34, 
[2m[36m(pid=12122)[0m  train loss: 8.65996343612671
[2m[36m(pid=12122)[0m  eval loss: 10.633629570007324, eval err: 0.8473187184333801
[2m[36m(pid=12122)[0m 35, 
[2m[36m(pid=12122)[0m  train loss: 8.399340171813964
[2m[36m(pid=12122)[0m  eval loss: 14.370007400512696, eval err: 0.8809156298637391
[2m[36m(pid=12122)[0m 36, 
[2m[36m(pid=12122)[0m  train loss: 8.446356658935548
[2m[36m(pid=12122)[0m  eval loss: 11.008477821350098, eval err: 0.857627432346344
Result for train_58e34cae:
  date: 2021-09-10_20-03-06
  done: false
  err: 0.857627432346344
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 11.008477821350098
  loss_train: 8.446356658935548
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 5011.150909423828
  time_this_iter_s: 406.00526571273804
  time_total_s: 5011.150909423828
  timestamp: 1631296986
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 58e34cae
  
== Status ==
Memory usage on this node: 26.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     13 |          5011.15 | 11.0085   | 0.857627  |      8.44636 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12122)[0m 37, 
[2m[36m(pid=12122)[0m  train loss: 8.256573619842529
[2m[36m(pid=12122)[0m  eval loss: 13.201671562194825, eval err: 0.8545295453071594
[2m[36m(pid=12122)[0m 38, 
[2m[36m(pid=12122)[0m  train loss: 8.344140090942382
[2m[36m(pid=12122)[0m  eval loss: 14.310594863891602, eval err: 0.866164710521698
[2m[36m(pid=12122)[0m 39, 
[2m[36m(pid=12122)[0m  train loss: 8.517640438079834
Result for train_58e34cae:
  date: 2021-09-10_20-09-51
  done: true
  err: 0.8461653661727905
  experiment_id: d9afd53d090f46dea4e674e464a9f1c8
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 11.708207626342773
  loss_train: 8.517640438079834
  node_ip: 131.220.7.54
  pid: 12122
  should_checkpoint: true
  time_since_restore: 5416.251828193665
  time_this_iter_s: 405.1009187698364
  time_total_s: 5416.251828193665
  timestamp: 1631297391
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 58e34cae
  
[2m[36m(pid=12122)[0m  eval loss: 11.708207626342773, eval err: 0.8461653661727905
== Status ==
Memory usage on this node: 27.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (12 PENDING, 1 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e34cae    | RUNNING    | 131.220.7.54:12122 |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fb72e    | PENDING    |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00020 | TERMINATED |                    |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(pid=12119)[0m Epoch:
[2m[36m(pid=12119)[0m 0, 
[2m[36m(pid=12119)[0m  train loss: 30.495689849853516
[2m[36m(pid=12119)[0m  eval loss: 101.14876052856445, eval err: 0.990889527797699
Result for train_2e5fb72e:
  date: 2021-09-10_20-11-57
  done: false
  err: 0.990889527797699
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 101.14876052856445
  loss_train: 30.495689849853516
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 123.87317991256714
  time_this_iter_s: 123.87317991256714
  time_total_s: 123.87317991256714
  timestamp: 1631297517
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |      1 |          123.873 | 101.149    | 0.99089   |     30.4957  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |            |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |            |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |            |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |            |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |   0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  |  11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  |  20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |   0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 1, 
[2m[36m(pid=12119)[0m  train loss: 14.514456901550293
[2m[36m(pid=12119)[0m  eval loss: 18.065671920776367, eval err: 0.8969641280174255
[2m[36m(pid=12119)[0m 2, 
[2m[36m(pid=12119)[0m  train loss: 12.681067810058593
[2m[36m(pid=12119)[0m  eval loss: 17.9546671295166, eval err: 0.8891718173027039
[2m[36m(pid=12119)[0m 3, 
[2m[36m(pid=12119)[0m  train loss: 12.922280197143555
[2m[36m(pid=12119)[0m  eval loss: 14.350089683532715, eval err: 0.8638885140419006
Result for train_2e5fb72e:
  date: 2021-09-10_20-18-02
  done: false
  err: 0.8638885140419006
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 14.350089683532715
  loss_train: 12.922280197143555
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 488.285502910614
  time_this_iter_s: 364.4123229980469
  time_total_s: 488.285502910614
  timestamp: 1631297882
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |      2 |          488.286 | 14.3501   | 0.863889  |     12.9223  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 4, 
[2m[36m(pid=12119)[0m  train loss: 10.77041618347168
[2m[36m(pid=12119)[0m  eval loss: 12.268884506225586, eval err: 0.8561616539955139
[2m[36m(pid=12119)[0m 5, 
[2m[36m(pid=12119)[0m  train loss: 10.547369422912597
[2m[36m(pid=12119)[0m  eval loss: 13.155707092285157, eval err: 0.8563192772865296
[2m[36m(pid=12119)[0m 6, 
[2m[36m(pid=12119)[0m  train loss: 10.30119520187378
[2m[36m(pid=12119)[0m  eval loss: 11.130477294921874, eval err: 0.8471110367774963
Result for train_2e5fb72e:
  date: 2021-09-10_20-24-05
  done: false
  err: 0.8471110367774963
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 11.130477294921874
  loss_train: 10.30119520187378
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 852.1999349594116
  time_this_iter_s: 363.9144320487976
  time_total_s: 852.1999349594116
  timestamp: 1631298245
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |      3 |           852.2  | 11.1305   | 0.847111  |     10.3012  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 7, 
[2m[36m(pid=12119)[0m  train loss: 10.479309329986572
[2m[36m(pid=12119)[0m  eval loss: 15.054225807189942, eval err: 0.8803149509429932
[2m[36m(pid=12119)[0m 8, 
[2m[36m(pid=12119)[0m  train loss: 10.342193717956542
[2m[36m(pid=12119)[0m  eval loss: 13.64123462677002, eval err: 0.8694365859031677
[2m[36m(pid=12119)[0m 9, 
[2m[36m(pid=12119)[0m  train loss: 9.977865810394286
[2m[36m(pid=12119)[0m  eval loss: 13.373448371887207, eval err: 0.863483054637909
Result for train_2e5fb72e:
  date: 2021-09-10_20-30-09
  done: false
  err: 0.863483054637909
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 13.373448371887207
  loss_train: 9.977865810394286
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 1215.6758601665497
  time_this_iter_s: 363.47592520713806
  time_total_s: 1215.6758601665497
  timestamp: 1631298609
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |      4 |          1215.68 | 13.3734   | 0.863483  |      9.97787 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 10, 
[2m[36m(pid=12119)[0m  train loss: 9.971009159088135
[2m[36m(pid=12119)[0m  eval loss: 12.663517265319824, eval err: 0.8599378800392151
[2m[36m(pid=12119)[0m 11, 
[2m[36m(pid=12119)[0m  train loss: 10.289452075958252
[2m[36m(pid=12119)[0m  eval loss: 11.297439041137695, eval err: 0.8384459471702576
[2m[36m(pid=12119)[0m 12, 
[2m[36m(pid=12119)[0m  train loss: 9.67431589126587
[2m[36m(pid=12119)[0m  eval loss: 13.099834556579589, eval err: 0.874230055809021
Result for train_2e5fb72e:
  date: 2021-09-10_20-36-12
  done: false
  err: 0.874230055809021
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 13.099834556579589
  loss_train: 9.67431589126587
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 1579.077176809311
  time_this_iter_s: 363.40131664276123
  time_total_s: 1579.077176809311
  timestamp: 1631298972
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |      5 |          1579.08 | 13.0998   | 0.87423   |      9.67432 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 13, 
[2m[36m(pid=12119)[0m  train loss: 9.151161079406739
[2m[36m(pid=12119)[0m  eval loss: 11.124089546203614, eval err: 0.8369912385940552
[2m[36m(pid=12119)[0m 14, 
[2m[36m(pid=12119)[0m  train loss: 9.311423740386964
[2m[36m(pid=12119)[0m  eval loss: 12.745607147216797, eval err: 0.8748495268821717
[2m[36m(pid=12119)[0m 15, 
[2m[36m(pid=12119)[0m  train loss: 8.822381439208984
[2m[36m(pid=12119)[0m  eval loss: 11.059773979187012, eval err: 0.8463291978836059
Result for train_2e5fb72e:
  date: 2021-09-10_20-42-15
  done: false
  err: 0.8463291978836059
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 11.059773979187012
  loss_train: 8.822381439208984
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 1941.4804582595825
  time_this_iter_s: 362.4032814502716
  time_total_s: 1941.4804582595825
  timestamp: 1631299335
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |      6 |          1941.48 | 11.0598   | 0.846329  |      8.82238 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 16, 
[2m[36m(pid=12119)[0m  train loss: 8.83132905960083
[2m[36m(pid=12119)[0m  eval loss: 11.518431854248046, eval err: 0.8591047811508179
[2m[36m(pid=12119)[0m 17, 
[2m[36m(pid=12119)[0m  train loss: 9.040278339385987
[2m[36m(pid=12119)[0m  eval loss: 10.71760959625244, eval err: 0.8504203534126282
[2m[36m(pid=12119)[0m 18, 
[2m[36m(pid=12119)[0m  train loss: 8.85172124862671
[2m[36m(pid=12119)[0m  eval loss: 10.819983100891113, eval err: 0.8649331498146057
Result for train_2e5fb72e:
  date: 2021-09-10_20-48-17
  done: false
  err: 0.8649331498146057
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 10.819983100891113
  loss_train: 8.85172124862671
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 2303.9973607063293
  time_this_iter_s: 362.5169024467468
  time_total_s: 2303.9973607063293
  timestamp: 1631299697
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |      7 |          2304    | 10.82     | 0.864933  |      8.85172 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 19, 
[2m[36m(pid=12119)[0m  train loss: 9.015765323638917
[2m[36m(pid=12119)[0m  eval loss: 10.321466026306153, eval err: 0.8425378751754761
[2m[36m(pid=12119)[0m 20, 
[2m[36m(pid=12119)[0m  train loss: 8.468130569458008
[2m[36m(pid=12119)[0m  eval loss: 11.862424163818359, eval err: 0.8669791626930237
[2m[36m(pid=12119)[0m 21, 
[2m[36m(pid=12119)[0m  train loss: 8.509380416870117
[2m[36m(pid=12119)[0m  eval loss: 9.465461502075195, eval err: 0.8380317378044129
Result for train_2e5fb72e:
  date: 2021-09-10_20-54-20
  done: false
  err: 0.8380317378044129
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 9.465461502075195
  loss_train: 8.509380416870117
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 2666.275375843048
  time_this_iter_s: 362.27801513671875
  time_total_s: 2666.275375843048
  timestamp: 1631300060
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |      8 |          2666.28 |  9.46546  | 0.838032  |      8.50938 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 22, 
[2m[36m(pid=12119)[0m  train loss: 8.86664192199707
[2m[36m(pid=12119)[0m  eval loss: 9.911807346343995, eval err: 0.8427111029624939
[2m[36m(pid=12119)[0m 23, 
[2m[36m(pid=12119)[0m  train loss: 8.530705604553223
[2m[36m(pid=12119)[0m  eval loss: 11.197037200927735, eval err: 0.840843505859375
[2m[36m(pid=12119)[0m 24, 
[2m[36m(pid=12119)[0m  train loss: 8.255163745880127
[2m[36m(pid=12119)[0m  eval loss: 11.613700981140136, eval err: 0.8512319374084473
Result for train_2e5fb72e:
  date: 2021-09-10_21-00-22
  done: false
  err: 0.8512319374084473
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 11.613700981140136
  loss_train: 8.255163745880127
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 3028.3949732780457
  time_this_iter_s: 362.11959743499756
  time_total_s: 3028.3949732780457
  timestamp: 1631300422
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |      9 |          3028.39 | 11.6137   | 0.851232  |      8.25516 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 25, 
[2m[36m(pid=12119)[0m  train loss: 8.990470848083497
[2m[36m(pid=12119)[0m  eval loss: 10.552359924316406, eval err: 0.830741126537323
[2m[36m(pid=12119)[0m 26, 
[2m[36m(pid=12119)[0m  train loss: 8.039925594329834
[2m[36m(pid=12119)[0m  eval loss: 9.165961093902588, eval err: 0.804092082977295
[2m[36m(pid=12119)[0m 27, 
[2m[36m(pid=12119)[0m  train loss: 7.992319736480713
[2m[36m(pid=12119)[0m  eval loss: 10.63193271636963, eval err: 0.8024450063705444
Result for train_2e5fb72e:
  date: 2021-09-10_21-06-25
  done: false
  err: 0.8024450063705444
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 10.63193271636963
  loss_train: 7.992319736480713
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 3391.277127981186
  time_this_iter_s: 362.88215470314026
  time_total_s: 3391.277127981186
  timestamp: 1631300785
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     10 |          3391.28 | 10.6319   | 0.802445  |      7.99232 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 28, 
[2m[36m(pid=12119)[0m  train loss: 6.990384902954101
[2m[36m(pid=12119)[0m  eval loss: 9.841716480255126, eval err: 0.7672121667861939
[2m[36m(pid=12119)[0m 29, 
[2m[36m(pid=12119)[0m  train loss: 7.0958928298950195
[2m[36m(pid=12119)[0m  eval loss: 7.344685783386231, eval err: 0.6970594477653503
[2m[36m(pid=12119)[0m 30, 
[2m[36m(pid=12119)[0m  train loss: 6.35657341003418
Result for train_2e5fb72e:
  date: 2021-09-10_21-12-28
  done: false
  err: 0.5254311728477478
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 4.276813774108887
  loss_train: 6.35657341003418
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 3754.8929255008698
  time_this_iter_s: 363.61579751968384
  time_total_s: 3754.8929255008698
  timestamp: 1631301148
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 2e5fb72e
  
[2m[36m(pid=12119)[0m  eval loss: 4.276813774108887, eval err: 0.5254311728477478
== Status ==
Memory usage on this node: 27.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     11 |          3754.89 |  4.27681  | 0.525431  |      6.35657 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 31, 
[2m[36m(pid=12119)[0m  train loss: 4.672042055130005
[2m[36m(pid=12119)[0m  eval loss: 2.8117579317092893, eval err: 0.3139679479598999
[2m[36m(pid=12119)[0m 32, 
[2m[36m(pid=12119)[0m  train loss: 3.489248914718628
[2m[36m(pid=12119)[0m  eval loss: 1.9166567373275756, eval err: 0.18298011541366577
[2m[36m(pid=12119)[0m 33, 
[2m[36m(pid=12119)[0m  train loss: 3.119312286376953
[2m[36m(pid=12119)[0m  eval loss: 2.0851047086715697, eval err: 0.1813323974609375
Result for train_2e5fb72e:
  date: 2021-09-10_21-18-27
  done: false
  err: 0.1813323974609375
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 2.0851047086715697
  loss_train: 3.119312286376953
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 4113.858329296112
  time_this_iter_s: 358.9654037952423
  time_total_s: 4113.858329296112
  timestamp: 1631301507
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 27.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     12 |          4113.86 |  2.0851   | 0.181332  |      3.11931 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 34, 
[2m[36m(pid=12119)[0m  train loss: 2.466912212371826
[2m[36m(pid=12119)[0m  eval loss: 1.4315519690513612, eval err: 0.12315441608428955
[2m[36m(pid=12119)[0m 35, 
[2m[36m(pid=12119)[0m  train loss: 2.1734933471679687
[2m[36m(pid=12119)[0m  eval loss: 1.2883380055427551, eval err: 0.1073091435432434
[2m[36m(pid=12119)[0m 36, 
[2m[36m(pid=12119)[0m  train loss: 1.8601505804061889
[2m[36m(pid=12119)[0m  eval loss: 1.194260711669922, eval err: 0.09511296272277832
Result for train_2e5fb72e:
  date: 2021-09-10_21-24-22
  done: false
  err: 0.09511296272277832
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 1.194260711669922
  loss_train: 1.8601505804061889
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 4468.217064142227
  time_this_iter_s: 354.3587348461151
  time_total_s: 4468.217064142227
  timestamp: 1631301862
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 16.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     13 |          4468.22 |  1.19426  | 0.095113  |      1.86015 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 37, 
[2m[36m(pid=12119)[0m  train loss: 1.990687484741211
[2m[36m(pid=12119)[0m  eval loss: 1.2937311363220214, eval err: 0.10669697761535644
[2m[36m(pid=12119)[0m 38, 
[2m[36m(pid=12119)[0m  train loss: 1.887012128829956
[2m[36m(pid=12119)[0m  eval loss: 1.2198524236679078, eval err: 0.09973330497741699
[2m[36m(pid=12119)[0m 39, 
[2m[36m(pid=12119)[0m  train loss: 1.6621067523956299
[2m[36m(pid=12119)[0m  eval loss: 1.1197613978385925, eval err: 0.08597785472869873
Result for train_2e5fb72e:
  date: 2021-09-10_21-30-13
  done: false
  err: 0.08597785472869873
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 1.1197613978385925
  loss_train: 1.6621067523956299
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 4819.277841091156
  time_this_iter_s: 351.06077694892883
  time_total_s: 4819.277841091156
  timestamp: 1631302213
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     14 |          4819.28 |  1.11976  | 0.0859779 |      1.66211 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 40, 
[2m[36m(pid=12119)[0m  train loss: 1.9664853525161743
[2m[36m(pid=12119)[0m  eval loss: 1.1286500787734985, eval err: 0.0849852967262268
[2m[36m(pid=12119)[0m 41, 
[2m[36m(pid=12119)[0m  train loss: 1.8062987613677979
[2m[36m(pid=12119)[0m  eval loss: 1.1382516980171205, eval err: 0.08525879621505737
[2m[36m(pid=12119)[0m 42, 
[2m[36m(pid=12119)[0m  train loss: 1.5847903823852538
[2m[36m(pid=12119)[0m  eval loss: 1.0637040281295775, eval err: 0.07721851587295532
Result for train_2e5fb72e:
  date: 2021-09-10_21-36-04
  done: false
  err: 0.07721851587295532
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 1.0637040281295775
  loss_train: 1.5847903823852538
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 5170.444365501404
  time_this_iter_s: 351.1665244102478
  time_total_s: 5170.444365501404
  timestamp: 1631302564
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 17.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     15 |          5170.44 |  1.0637   | 0.0772185 |      1.58479 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 43, 
[2m[36m(pid=12119)[0m  train loss: 1.5320515608787537
[2m[36m(pid=12119)[0m  eval loss: 1.084251778125763, eval err: 0.07661711215972901
[2m[36m(pid=12119)[0m 44, 
[2m[36m(pid=12119)[0m  train loss: 1.4246149253845215
[2m[36m(pid=12119)[0m  eval loss: 0.9960652673244477, eval err: 0.07225126028060913
[2m[36m(pid=12119)[0m 45, 
[2m[36m(pid=12119)[0m  train loss: 1.4765066623687744
[2m[36m(pid=12119)[0m  eval loss: 1.0408196568489074, eval err: 0.07570941925048828
Result for train_2e5fb72e:
  date: 2021-09-10_21-41-55
  done: false
  err: 0.07570941925048828
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 1.0408196568489074
  loss_train: 1.4765066623687744
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 5521.7770166397095
  time_this_iter_s: 351.33265113830566
  time_total_s: 5521.7770166397095
  timestamp: 1631302915
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 17.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     16 |          5521.78 |  1.04082  | 0.0757094 |      1.47651 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 46, 
[2m[36m(pid=12119)[0m  train loss: 1.4073973727226257
[2m[36m(pid=12119)[0m  eval loss: 1.0365557837486268, eval err: 0.0741202449798584
[2m[36m(pid=12119)[0m 47, 
[2m[36m(pid=12119)[0m  train loss: 1.7062980008125306
[2m[36m(pid=12119)[0m  eval loss: 1.1598009300231933, eval err: 0.08639681577682495
[2m[36m(pid=12119)[0m 48, 
[2m[36m(pid=12119)[0m  train loss: 1.3816848707199096
[2m[36m(pid=12119)[0m  eval loss: 0.9912592124938965, eval err: 0.06978120803833007
Result for train_2e5fb72e:
  date: 2021-09-10_21-47-47
  done: false
  err: 0.06978120803833007
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 0.9912592124938965
  loss_train: 1.3816848707199096
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 5874.120516300201
  time_this_iter_s: 352.34349966049194
  time_total_s: 5874.120516300201
  timestamp: 1631303267
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 16.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     17 |          5874.12 |  0.991259 | 0.0697812 |      1.38168 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 49, 
[2m[36m(pid=12119)[0m  train loss: 1.354693431854248
[2m[36m(pid=12119)[0m  eval loss: 0.9770012247562408, eval err: 0.06855879068374633
[2m[36m(pid=12119)[0m 50, 
[2m[36m(pid=12119)[0m  train loss: 1.254281976222992
[2m[36m(pid=12119)[0m  eval loss: 0.9381049799919129, eval err: 0.06812711954116821
[2m[36m(pid=12119)[0m 51, 
[2m[36m(pid=12119)[0m  train loss: 1.4819074511528014
[2m[36m(pid=12119)[0m  eval loss: 0.9335184979438782, eval err: 0.06753174304962158
Result for train_2e5fb72e:
  date: 2021-09-10_21-53-39
  done: false
  err: 0.06753174304962158
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 0.9335184979438782
  loss_train: 1.4819074511528014
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 6225.980262756348
  time_this_iter_s: 351.85974645614624
  time_total_s: 6225.980262756348
  timestamp: 1631303619
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 16.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     18 |          6225.98 |  0.933518 | 0.0675317 |      1.48191 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 52, 
[2m[36m(pid=12119)[0m  train loss: 1.3339830732345581
[2m[36m(pid=12119)[0m  eval loss: 0.9285166358947754, eval err: 0.06696228504180908
[2m[36m(pid=12119)[0m 53, 
[2m[36m(pid=12119)[0m  train loss: 1.4277420353889465
[2m[36m(pid=12119)[0m  eval loss: 0.9261285674571991, eval err: 0.06611918210983277
[2m[36m(pid=12119)[0m 54, 
[2m[36m(pid=12119)[0m  train loss: 1.4296464920043945
[2m[36m(pid=12119)[0m  eval loss: 0.9124374794960022, eval err: 0.06519296169281005
Result for train_2e5fb72e:
  date: 2021-09-10_21-59-31
  done: false
  err: 0.06519296169281005
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 0.9124374794960022
  loss_train: 1.4296464920043945
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 6577.720352649689
  time_this_iter_s: 351.74008989334106
  time_total_s: 6577.720352649689
  timestamp: 1631303971
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     19 |          6577.72 |  0.912437 | 0.065193  |      1.42965 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 55, 
[2m[36m(pid=12119)[0m  train loss: 1.4339654088020324
[2m[36m(pid=12119)[0m  eval loss: 0.9197009646892548, eval err: 0.06627572536468505
[2m[36m(pid=12119)[0m 56, 
[2m[36m(pid=12119)[0m  train loss: 1.3568649339675902
[2m[36m(pid=12119)[0m  eval loss: 0.904238076210022, eval err: 0.06466378688812256
[2m[36m(pid=12119)[0m 57, 
[2m[36m(pid=12119)[0m  train loss: 1.2898903727531432
[2m[36m(pid=12119)[0m  eval loss: 0.9054436004161834, eval err: 0.06452438831329346
Result for train_2e5fb72e:
  date: 2021-09-10_22-05-23
  done: false
  err: 0.06452438831329346
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 0.9054436004161834
  loss_train: 1.2898903727531432
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 6929.532520532608
  time_this_iter_s: 351.8121678829193
  time_total_s: 6929.532520532608
  timestamp: 1631304323
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     20 |          6929.53 |  0.905444 | 0.0645244 |      1.28989 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12119)[0m 58, 
[2m[36m(pid=12119)[0m  train loss: 1.2430500841140748
[2m[36m(pid=12119)[0m  eval loss: 0.8994640231132507, eval err: 0.06432534456253051
[2m[36m(pid=12119)[0m 59, 
[2m[36m(pid=12119)[0m  train loss: 1.3455612087249755
[2m[36m(pid=12119)[0m  eval loss: 0.9050632429122925, eval err: 0.06508303165435791
[2m[36m(pid=12119)[0m 60, 
[2m[36m(pid=12119)[0m  train loss: 1.3869090390205383
[2m[36m(pid=12119)[0m  eval loss: 0.8974899804592132, eval err: 0.06386085271835328
Result for train_2e5fb72e:
  date: 2021-09-10_22-11-15
  done: true
  err: 0.06386085271835328
  experiment_id: 8eb8535788e34da99aced0ac22a260e1
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 0.8974899804592132
  loss_train: 1.3869090390205383
  node_ip: 131.220.7.54
  pid: 12119
  should_checkpoint: true
  time_since_restore: 7281.350744724274
  time_this_iter_s: 351.81822419166565
  time_total_s: 7281.350744724274
  timestamp: 1631304675
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: 2e5fb72e
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (11 PENDING, 1 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fb72e    | RUNNING    | 131.220.7.54:12119 |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_2e5ff572    | PENDING    |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(pid=12118)[0m Epoch:
[2m[36m(pid=12118)[0m 0, 
[2m[36m(pid=12118)[0m  train loss: 30.819377899169922
[2m[36m(pid=12118)[0m  eval loss: 100.27528442382813, eval err: 0.9907927536964416
Result for train_2e5ff572:
  date: 2021-09-10_22-13-24
  done: false
  err: 0.9907927536964416
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 100.27528442382813
  loss_train: 30.819377899169922
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 127.3686101436615
  time_this_iter_s: 127.3686101436615
  time_total_s: 127.3686101436615
  timestamp: 1631304804
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |      1 |          127.369 | 100.275    | 0.990793  |     30.8194  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |            |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |            |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |            |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |            |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |   0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |   0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  |  11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  |  20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 1, 
[2m[36m(pid=12118)[0m  train loss: 14.05188304901123
[2m[36m(pid=12118)[0m  eval loss: 15.058585090637207, eval err: 0.8734541273117066
[2m[36m(pid=12118)[0m 2, 
[2m[36m(pid=12118)[0m  train loss: 12.723597564697265
[2m[36m(pid=12118)[0m  eval loss: 14.288888549804687, eval err: 0.8641491556167602
[2m[36m(pid=12118)[0m 3, 
[2m[36m(pid=12118)[0m  train loss: 12.351776313781738
[2m[36m(pid=12118)[0m  eval loss: 12.633050956726073, eval err: 0.8453007292747498
Result for train_2e5ff572:
  date: 2021-09-10_22-19-36
  done: false
  err: 0.8453007292747498
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 12.633050956726073
  loss_train: 12.351776313781738
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 499.91583228111267
  time_this_iter_s: 372.5472221374512
  time_total_s: 499.91583228111267
  timestamp: 1631305176
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |      2 |          499.916 | 12.6331   | 0.845301  |     12.3518  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 4, 
[2m[36m(pid=12118)[0m  train loss: 11.160635986328124
[2m[36m(pid=12118)[0m  eval loss: 11.990411949157714, eval err: 0.8403862810134888
[2m[36m(pid=12118)[0m 5, 
[2m[36m(pid=12118)[0m  train loss: 11.187137222290039
[2m[36m(pid=12118)[0m  eval loss: 11.950311775207519, eval err: 0.8279786992073059
[2m[36m(pid=12118)[0m 6, 
[2m[36m(pid=12118)[0m  train loss: 9.67674524307251
[2m[36m(pid=12118)[0m  eval loss: 13.309078979492188, eval err: 0.856388201713562
Result for train_2e5ff572:
  date: 2021-09-10_22-25-48
  done: false
  err: 0.856388201713562
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 13.309078979492188
  loss_train: 9.67674524307251
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 871.0875778198242
  time_this_iter_s: 371.17174553871155
  time_total_s: 871.0875778198242
  timestamp: 1631305548
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |      3 |          871.088 | 13.3091   | 0.856388  |      9.67675 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 7, 
[2m[36m(pid=12118)[0m  train loss: 9.872590351104737
[2m[36m(pid=12118)[0m  eval loss: 11.708231544494629, eval err: 0.8385823845863343
[2m[36m(pid=12118)[0m 8, 
[2m[36m(pid=12118)[0m  train loss: 9.486436100006104
[2m[36m(pid=12118)[0m  eval loss: 10.168271617889404, eval err: 0.823194751739502
[2m[36m(pid=12118)[0m 9, 
[2m[36m(pid=12118)[0m  train loss: 9.396120128631592
[2m[36m(pid=12118)[0m  eval loss: 12.289183464050293, eval err: 0.845651741027832
Result for train_2e5ff572:
  date: 2021-09-10_22-31-59
  done: false
  err: 0.845651741027832
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 12.289183464050293
  loss_train: 9.396120128631592
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 1242.325171470642
  time_this_iter_s: 371.23759365081787
  time_total_s: 1242.325171470642
  timestamp: 1631305919
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 17.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |      4 |          1242.33 | 12.2892   | 0.845652  |      9.39612 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 10, 
[2m[36m(pid=12118)[0m  train loss: 9.106142463684082
[2m[36m(pid=12118)[0m  eval loss: 10.43925313949585, eval err: 0.8314076733589172
[2m[36m(pid=12118)[0m 11, 
[2m[36m(pid=12118)[0m  train loss: 9.61353536605835
[2m[36m(pid=12118)[0m  eval loss: 11.033067569732665, eval err: 0.8303305721282959
[2m[36m(pid=12118)[0m 12, 
[2m[36m(pid=12118)[0m  train loss: 9.620231456756592
[2m[36m(pid=12118)[0m  eval loss: 13.79923526763916, eval err: 0.8546952843666077
Result for train_2e5ff572:
  date: 2021-09-10_22-38-10
  done: false
  err: 0.8546952843666077
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 13.79923526763916
  loss_train: 9.620231456756592
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 1613.3637001514435
  time_this_iter_s: 371.0385286808014
  time_total_s: 1613.3637001514435
  timestamp: 1631306290
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 17.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |      5 |          1613.36 | 13.7992   | 0.854695  |      9.62023 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 13, 
[2m[36m(pid=12118)[0m  train loss: 8.96662202835083
[2m[36m(pid=12118)[0m  eval loss: 13.482851104736328, eval err: 0.8596596932411193
[2m[36m(pid=12118)[0m 14, 
[2m[36m(pid=12118)[0m  train loss: 9.549551429748535
[2m[36m(pid=12118)[0m  eval loss: 10.878567657470704, eval err: 0.8239035415649414
[2m[36m(pid=12118)[0m 15, 
[2m[36m(pid=12118)[0m  train loss: 9.655909538269043
[2m[36m(pid=12118)[0m  eval loss: 13.203241271972656, eval err: 0.8527318716049195
Result for train_2e5ff572:
  date: 2021-09-10_22-44-13
  done: false
  err: 0.8527318716049195
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 13.203241271972656
  loss_train: 9.655909538269043
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 1976.524479150772
  time_this_iter_s: 363.1607789993286
  time_total_s: 1976.524479150772
  timestamp: 1631306653
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |      6 |          1976.52 | 13.2032   | 0.852732  |      9.65591 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 16, 
[2m[36m(pid=12118)[0m  train loss: 9.742111892700196
[2m[36m(pid=12118)[0m  eval loss: 10.155942287445068, eval err: 0.8261665439605713
[2m[36m(pid=12118)[0m 17, 
[2m[36m(pid=12118)[0m  train loss: 9.731873378753662
[2m[36m(pid=12118)[0m  eval loss: 9.369504890441895, eval err: 0.8280505037307739
[2m[36m(pid=12118)[0m 18, 
[2m[36m(pid=12118)[0m  train loss: 9.48482084274292
[2m[36m(pid=12118)[0m  eval loss: 11.215483589172363, eval err: 0.8416382670402527
Result for train_2e5ff572:
  date: 2021-09-10_22-50-11
  done: false
  err: 0.8416382670402527
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 11.215483589172363
  loss_train: 9.48482084274292
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 2334.2965383529663
  time_this_iter_s: 357.7720592021942
  time_total_s: 2334.2965383529663
  timestamp: 1631307011
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |      7 |          2334.3  | 11.2155   | 0.841638  |      9.48482 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 19, 
[2m[36m(pid=12118)[0m  train loss: 8.864632263183594
[2m[36m(pid=12118)[0m  eval loss: 9.504911670684814, eval err: 0.8096780252456665
[2m[36m(pid=12118)[0m 20, 
[2m[36m(pid=12118)[0m  train loss: 8.33791229248047
[2m[36m(pid=12118)[0m  eval loss: 10.842337341308594, eval err: 0.8133342885971069
[2m[36m(pid=12118)[0m 21, 
[2m[36m(pid=12118)[0m  train loss: 7.77671064376831
[2m[36m(pid=12118)[0m  eval loss: 10.31683048248291, eval err: 0.8107868814468384
Result for train_2e5ff572:
  date: 2021-09-10_22-56-08
  done: false
  err: 0.8107868814468384
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 10.31683048248291
  loss_train: 7.77671064376831
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 2691.924955844879
  time_this_iter_s: 357.62841749191284
  time_total_s: 2691.924955844879
  timestamp: 1631307368
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |      8 |          2691.92 | 10.3168   | 0.810787  |      7.77671 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 22, 
[2m[36m(pid=12118)[0m  train loss: 8.155964126586914
[2m[36m(pid=12118)[0m  eval loss: 7.870749740600586, eval err: 0.7762139558792114
[2m[36m(pid=12118)[0m 23, 
[2m[36m(pid=12118)[0m  train loss: 7.335837650299072
[2m[36m(pid=12118)[0m  eval loss: 10.078665943145753, eval err: 0.8022445869445801
[2m[36m(pid=12118)[0m 24, 
[2m[36m(pid=12118)[0m  train loss: 7.615532970428466
[2m[36m(pid=12118)[0m  eval loss: 8.979527492523193, eval err: 0.7935230493545532
Result for train_2e5ff572:
  date: 2021-09-10_23-02-06
  done: false
  err: 0.7935230493545532
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 8.979527492523193
  loss_train: 7.615532970428466
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 3049.621280193329
  time_this_iter_s: 357.6963243484497
  time_total_s: 3049.621280193329
  timestamp: 1631307726
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |      9 |          3049.62 |  8.97953  | 0.793523  |      7.61553 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 25, 
[2m[36m(pid=12118)[0m  train loss: 7.1950705909729
[2m[36m(pid=12118)[0m  eval loss: 6.649051723480224, eval err: 0.7004075050354004
[2m[36m(pid=12118)[0m 26, 
[2m[36m(pid=12118)[0m  train loss: 6.798670921325684
[2m[36m(pid=12118)[0m  eval loss: 5.1463022041320805, eval err: 0.5872746562957764
[2m[36m(pid=12118)[0m 27, 
[2m[36m(pid=12118)[0m  train loss: 5.917420892715454
[2m[36m(pid=12118)[0m  eval loss: 5.122585926055908, eval err: 0.5388412618637085
Result for train_2e5ff572:
  date: 2021-09-10_23-08-04
  done: false
  err: 0.5388412618637085
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 5.122585926055908
  loss_train: 5.917420892715454
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 3407.569142818451
  time_this_iter_s: 357.94786262512207
  time_total_s: 3407.569142818451
  timestamp: 1631308084
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     10 |          3407.57 |  5.12259  | 0.538841  |      5.91742 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 28, 
[2m[36m(pid=12118)[0m  train loss: 4.347321729660035
[2m[36m(pid=12118)[0m  eval loss: 2.5487903642654417, eval err: 0.31885682106018065
[2m[36m(pid=12118)[0m 29, 
[2m[36m(pid=12118)[0m  train loss: 3.6778400135040283
[2m[36m(pid=12118)[0m  eval loss: 2.0937451314926148, eval err: 0.21144100904464722
[2m[36m(pid=12118)[0m 30, 
[2m[36m(pid=12118)[0m  train loss: 2.8318859052658083
[2m[36m(pid=12118)[0m  eval loss: 2.0506007432937623, eval err: 0.1970836353302002
Result for train_2e5ff572:
  date: 2021-09-10_23-14-01
  done: false
  err: 0.1970836353302002
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 2.0506007432937623
  loss_train: 2.8318859052658083
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 3764.3720548152924
  time_this_iter_s: 356.80291199684143
  time_total_s: 3764.3720548152924
  timestamp: 1631308441
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     11 |          3764.37 |  2.0506   | 0.197084  |      2.83189 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 31, 
[2m[36m(pid=12118)[0m  train loss: 2.5506963872909547
[2m[36m(pid=12118)[0m  eval loss: 1.492214198112488, eval err: 0.13413225412368773
[2m[36m(pid=12118)[0m 32, 
[2m[36m(pid=12118)[0m  train loss: 2.1501284885406493
[2m[36m(pid=12118)[0m  eval loss: 1.3437919855117797, eval err: 0.11462235689163208
[2m[36m(pid=12118)[0m 33, 
[2m[36m(pid=12118)[0m  train loss: 1.7892036104202271
[2m[36m(pid=12118)[0m  eval loss: 1.2631574892997741, eval err: 0.10745452642440796
Result for train_2e5ff572:
  date: 2021-09-10_23-19-54
  done: false
  err: 0.10745452642440796
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 1.2631574892997741
  loss_train: 1.7892036104202271
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 4117.764429330826
  time_this_iter_s: 353.39237451553345
  time_total_s: 4117.764429330826
  timestamp: 1631308794
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     12 |          4117.76 |  1.26316  | 0.107455  |      1.7892  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 34, 
[2m[36m(pid=12118)[0m  train loss: 1.9232068586349487
[2m[36m(pid=12118)[0m  eval loss: 1.3244105100631713, eval err: 0.11371430397033691
[2m[36m(pid=12118)[0m 35, 
[2m[36m(pid=12118)[0m  train loss: 2.0224903631210327
[2m[36m(pid=12118)[0m  eval loss: 1.1810638213157654, eval err: 0.09459451913833618
[2m[36m(pid=12118)[0m 36, 
[2m[36m(pid=12118)[0m  train loss: 1.6235810256004333
[2m[36m(pid=12118)[0m  eval loss: 1.1770076942443848, eval err: 0.0970534610748291
Result for train_2e5ff572:
  date: 2021-09-10_23-25-46
  done: false
  err: 0.0970534610748291
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 1.1770076942443848
  loss_train: 1.6235810256004333
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 4469.502502918243
  time_this_iter_s: 351.7380735874176
  time_total_s: 4469.502502918243
  timestamp: 1631309146
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     13 |          4469.5  |  1.17701  | 0.0970535 |      1.62358 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 37, 
[2m[36m(pid=12118)[0m  train loss: 1.760519461631775
[2m[36m(pid=12118)[0m  eval loss: 1.1885217547416687, eval err: 0.0958432650566101
[2m[36m(pid=12118)[0m 38, 
[2m[36m(pid=12118)[0m  train loss: 1.7170267868041993
[2m[36m(pid=12118)[0m  eval loss: 1.100759472846985, eval err: 0.08640822649002075
[2m[36m(pid=12118)[0m 39, 
[2m[36m(pid=12118)[0m  train loss: 1.678591148853302
[2m[36m(pid=12118)[0m  eval loss: 1.1176977777481079, eval err: 0.08664687156677246
Result for train_2e5ff572:
  date: 2021-09-10_23-31-37
  done: false
  err: 0.08664687156677246
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 1.1176977777481079
  loss_train: 1.678591148853302
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 4820.538803577423
  time_this_iter_s: 351.0363006591797
  time_total_s: 4820.538803577423
  timestamp: 1631309497
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     14 |          4820.54 |  1.1177   | 0.0866469 |      1.67859 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 40, 
[2m[36m(pid=12118)[0m  train loss: 1.7256740736961365
[2m[36m(pid=12118)[0m  eval loss: 1.0785349202156067, eval err: 0.08332086563110351
[2m[36m(pid=12118)[0m 41, 
[2m[36m(pid=12118)[0m  train loss: 1.6816327285766601
[2m[36m(pid=12118)[0m  eval loss: 1.0584564805030823, eval err: 0.07951987028121948
[2m[36m(pid=12118)[0m 42, 
[2m[36m(pid=12118)[0m  train loss: 1.7966345071792602
[2m[36m(pid=12118)[0m  eval loss: 1.0858390426635742, eval err: 0.08321073532104492
Result for train_2e5ff572:
  date: 2021-09-10_23-37-27
  done: false
  err: 0.08321073532104492
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 1.0858390426635742
  loss_train: 1.7966345071792602
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 5170.775958299637
  time_this_iter_s: 350.23715472221375
  time_total_s: 5170.775958299637
  timestamp: 1631309847
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     15 |          5170.78 |  1.08584  | 0.0832107 |      1.79663 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 43, 
[2m[36m(pid=12118)[0m  train loss: 1.5609229755401612
[2m[36m(pid=12118)[0m  eval loss: 1.1207961273193359, eval err: 0.0891621971130371
[2m[36m(pid=12118)[0m 44, 
[2m[36m(pid=12118)[0m  train loss: 1.778291370868683
[2m[36m(pid=12118)[0m  eval loss: 1.0305566239356994, eval err: 0.07650527477264404
[2m[36m(pid=12118)[0m 45, 
[2m[36m(pid=12118)[0m  train loss: 1.7930494356155395
[2m[36m(pid=12118)[0m  eval loss: 1.0891368079185486, eval err: 0.08620107889175416
Result for train_2e5ff572:
  date: 2021-09-10_23-43-17
  done: false
  err: 0.08620107889175416
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 1.0891368079185486
  loss_train: 1.7930494356155395
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 5520.368499040604
  time_this_iter_s: 349.5925407409668
  time_total_s: 5520.368499040604
  timestamp: 1631310197
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     16 |          5520.37 |  1.08914  | 0.0862011 |      1.79305 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 46, 
[2m[36m(pid=12118)[0m  train loss: 1.7223222661018371
[2m[36m(pid=12118)[0m  eval loss: 1.0191469442844392, eval err: 0.07472395420074462
[2m[36m(pid=12118)[0m 47, 
[2m[36m(pid=12118)[0m  train loss: 1.325819959640503
[2m[36m(pid=12118)[0m  eval loss: 1.3191078209877014, eval err: 0.10345343828201294
[2m[36m(pid=12118)[0m 48, 
[2m[36m(pid=12118)[0m  train loss: 1.6183559846878053
[2m[36m(pid=12118)[0m  eval loss: 1.046632583141327, eval err: 0.07832742929458618
Result for train_2e5ff572:
  date: 2021-09-10_23-49-06
  done: false
  err: 0.07832742929458618
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 1.046632583141327
  loss_train: 1.6183559846878053
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 5869.511980056763
  time_this_iter_s: 349.14348101615906
  time_total_s: 5869.511980056763
  timestamp: 1631310546
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     17 |          5869.51 |  1.04663  | 0.0783274 |      1.61836 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 49, 
[2m[36m(pid=12118)[0m  train loss: 1.6673559951782226
[2m[36m(pid=12118)[0m  eval loss: 0.9948163247108459, eval err: 0.07337525606155396
[2m[36m(pid=12118)[0m 50, 
[2m[36m(pid=12118)[0m  train loss: 1.5602487158775329
[2m[36m(pid=12118)[0m  eval loss: 0.9870204555988312, eval err: 0.07269874811172486
[2m[36m(pid=12118)[0m 51, 
[2m[36m(pid=12118)[0m  train loss: 1.4524925136566162
[2m[36m(pid=12118)[0m  eval loss: 0.99114692568779, eval err: 0.07364965677261352
Result for train_2e5ff572:
  date: 2021-09-10_23-54-55
  done: false
  err: 0.07364965677261352
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 0.99114692568779
  loss_train: 1.4524925136566162
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 6218.227857351303
  time_this_iter_s: 348.7158772945404
  time_total_s: 6218.227857351303
  timestamp: 1631310895
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     18 |          6218.23 |  0.991147 | 0.0736497 |      1.45249 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 52, 
[2m[36m(pid=12118)[0m  train loss: 1.6351188015937805
[2m[36m(pid=12118)[0m  eval loss: 1.212513120174408, eval err: 0.09562917709350587
[2m[36m(pid=12118)[0m 53, 
[2m[36m(pid=12118)[0m  train loss: 1.8283442401885985
[2m[36m(pid=12118)[0m  eval loss: 1.6561895203590393, eval err: 0.10933083772659302
[2m[36m(pid=12118)[0m 54, 
[2m[36m(pid=12118)[0m  train loss: 2.064966149330139
[2m[36m(pid=12118)[0m  eval loss: 1.862715208530426, eval err: 0.15547552585601807
Result for train_2e5ff572:
  date: 2021-09-11_00-00-44
  done: false
  err: 0.15547552585601807
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 1.862715208530426
  loss_train: 2.064966149330139
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 6567.38445353508
  time_this_iter_s: 349.15659618377686
  time_total_s: 6567.38445353508
  timestamp: 1631311244
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     19 |          6567.38 |  1.86272  | 0.155476  |      2.06497 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 55, 
[2m[36m(pid=12118)[0m  train loss: 1.6877675676345825
[2m[36m(pid=12118)[0m  eval loss: 1.1408607125282288, eval err: 0.08056962013244628
[2m[36m(pid=12118)[0m 56, 
[2m[36m(pid=12118)[0m  train loss: 2.1154848670959474
[2m[36m(pid=12118)[0m  eval loss: 1.1793815398216247, eval err: 0.08773643970489502
[2m[36m(pid=12118)[0m 57, 
[2m[36m(pid=12118)[0m  train loss: 1.7371253156661988
[2m[36m(pid=12118)[0m  eval loss: 1.035696198940277, eval err: 0.07590410232543945
Result for train_2e5ff572:
  date: 2021-09-11_00-06-34
  done: false
  err: 0.07590410232543945
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 1.035696198940277
  loss_train: 1.7371253156661988
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 6917.144176721573
  time_this_iter_s: 349.7597231864929
  time_total_s: 6917.144176721573
  timestamp: 1631311594
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     20 |          6917.14 |  1.0357   | 0.0759041 |      1.73713 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 58, 
[2m[36m(pid=12118)[0m  train loss: 1.5741175389289856
[2m[36m(pid=12118)[0m  eval loss: 1.0397807252407074, eval err: 0.07484230041503906
[2m[36m(pid=12118)[0m 59, 
[2m[36m(pid=12118)[0m  train loss: 1.4902889943122863
[2m[36m(pid=12118)[0m  eval loss: 1.0148665368556977, eval err: 0.07284053325653077
[2m[36m(pid=12118)[0m 60, 
[2m[36m(pid=12118)[0m  train loss: 1.4739116883277894
[2m[36m(pid=12118)[0m  eval loss: 0.9943058252334595, eval err: 0.07178996801376343
Result for train_2e5ff572:
  date: 2021-09-11_00-12-22
  done: false
  err: 0.07178996801376343
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 0.9943058252334595
  loss_train: 1.4739116883277894
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 7265.955716609955
  time_this_iter_s: 348.81153988838196
  time_total_s: 7265.955716609955
  timestamp: 1631311942
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     21 |          7265.96 |  0.994306 | 0.07179   |      1.47391 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 61, 
[2m[36m(pid=12118)[0m  train loss: 1.4784966731071472
[2m[36m(pid=12118)[0m  eval loss: 0.9984912943840026, eval err: 0.07220028638839722
[2m[36m(pid=12118)[0m 62, 
[2m[36m(pid=12118)[0m  train loss: 1.452503411769867
[2m[36m(pid=12118)[0m  eval loss: 0.9815192425251007, eval err: 0.07099085569381713
[2m[36m(pid=12118)[0m 63, 
[2m[36m(pid=12118)[0m  train loss: 1.3588504314422607
[2m[36m(pid=12118)[0m  eval loss: 0.9709548962116241, eval err: 0.06998257398605347
Result for train_2e5ff572:
  date: 2021-09-11_00-18-11
  done: false
  err: 0.06998257398605347
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 22
  loss: 0.9709548962116241
  loss_train: 1.3588504314422607
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 7614.7612657547
  time_this_iter_s: 348.8055491447449
  time_total_s: 7614.7612657547
  timestamp: 1631312291
  timesteps_since_restore: 0
  training_iteration: 22
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     22 |          7614.76 |  0.970955 | 0.0699826 |      1.35885 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 64, 
[2m[36m(pid=12118)[0m  train loss: 1.417516164779663
[2m[36m(pid=12118)[0m  eval loss: 0.9532973897457123, eval err: 0.0684752607345581
[2m[36m(pid=12118)[0m 65, 
[2m[36m(pid=12118)[0m  train loss: 1.4857307410240173
[2m[36m(pid=12118)[0m  eval loss: 0.9573243308067322, eval err: 0.06951653003692627
[2m[36m(pid=12118)[0m 66, 
[2m[36m(pid=12118)[0m  train loss: 1.2503617334365844
[2m[36m(pid=12118)[0m  eval loss: 0.9672098708152771, eval err: 0.0698020601272583
Result for train_2e5ff572:
  date: 2021-09-11_00-24-00
  done: false
  err: 0.0698020601272583
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 23
  loss: 0.9672098708152771
  loss_train: 1.2503617334365844
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 7963.437571287155
  time_this_iter_s: 348.67630553245544
  time_total_s: 7963.437571287155
  timestamp: 1631312640
  timesteps_since_restore: 0
  training_iteration: 23
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     23 |          7963.44 |  0.96721  | 0.0698021 |      1.25036 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 67, 
[2m[36m(pid=12118)[0m  train loss: 1.2936299490928649
[2m[36m(pid=12118)[0m  eval loss: 0.9415106534957886, eval err: 0.06768841028213501
[2m[36m(pid=12118)[0m 68, 
[2m[36m(pid=12118)[0m  train loss: 1.3260180974006652
[2m[36m(pid=12118)[0m  eval loss: 0.9399760615825653, eval err: 0.06778340578079224
[2m[36m(pid=12118)[0m 69, 
[2m[36m(pid=12118)[0m  train loss: 1.3979453659057617
[2m[36m(pid=12118)[0m  eval loss: 0.9183031964302063, eval err: 0.06608621835708618
Result for train_2e5ff572:
  date: 2021-09-11_00-29-49
  done: false
  err: 0.06608621835708618
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 24
  loss: 0.9183031964302063
  loss_train: 1.3979453659057617
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 8312.299503803253
  time_this_iter_s: 348.861932516098
  time_total_s: 8312.299503803253
  timestamp: 1631312989
  timesteps_since_restore: 0
  training_iteration: 24
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     24 |          8312.3  |  0.918303 | 0.0660862 |      1.39795 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 70, 
[2m[36m(pid=12118)[0m  train loss: 1.3404121398925781
[2m[36m(pid=12118)[0m  eval loss: 0.9189107966423035, eval err: 0.06639812707901001
[2m[36m(pid=12118)[0m 71, 
[2m[36m(pid=12118)[0m  train loss: 1.299397919178009
[2m[36m(pid=12118)[0m  eval loss: 0.9113930010795593, eval err: 0.06550849437713623
[2m[36m(pid=12118)[0m 72, 
[2m[36m(pid=12118)[0m  train loss: 1.22110600233078
[2m[36m(pid=12118)[0m  eval loss: 0.9123267543315887, eval err: 0.06516494035720825
Result for train_2e5ff572:
  date: 2021-09-11_00-35-37
  done: false
  err: 0.06516494035720825
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 25
  loss: 0.9123267543315887
  loss_train: 1.22110600233078
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 8660.530393600464
  time_this_iter_s: 348.2308897972107
  time_total_s: 8660.530393600464
  timestamp: 1631313337
  timesteps_since_restore: 0
  training_iteration: 25
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     25 |          8660.53 |  0.912327 | 0.0651649 |      1.22111 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 73, 
[2m[36m(pid=12118)[0m  train loss: 1.3809896087646485
[2m[36m(pid=12118)[0m  eval loss: 0.9067486023902893, eval err: 0.06450292110443115
[2m[36m(pid=12118)[0m 74, 
[2m[36m(pid=12118)[0m  train loss: 1.2911275076866149
[2m[36m(pid=12118)[0m  eval loss: 0.9085886311531067, eval err: 0.06480695724487305
[2m[36m(pid=12118)[0m 75, 
[2m[36m(pid=12118)[0m  train loss: 1.1309723711013795
[2m[36m(pid=12118)[0m  eval loss: 0.9066692125797272, eval err: 0.06541115283966065
Result for train_2e5ff572:
  date: 2021-09-11_00-41-25
  done: false
  err: 0.06541115283966065
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 26
  loss: 0.9066692125797272
  loss_train: 1.1309723711013795
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 9008.484944105148
  time_this_iter_s: 347.95455050468445
  time_total_s: 9008.484944105148
  timestamp: 1631313685
  timesteps_since_restore: 0
  training_iteration: 26
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     26 |          9008.48 |  0.906669 | 0.0654112 |      1.13097 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12118)[0m 76, 
[2m[36m(pid=12118)[0m  train loss: 1.255344786643982
[2m[36m(pid=12118)[0m  eval loss: 0.8968480372428894, eval err: 0.06424493551254272
[2m[36m(pid=12118)[0m 77, 
[2m[36m(pid=12118)[0m  train loss: 1.3384727239608765
[2m[36m(pid=12118)[0m  eval loss: 0.9037581717967987, eval err: 0.0648620080947876
[2m[36m(pid=12118)[0m 78, 
[2m[36m(pid=12118)[0m  train loss: 1.191042854785919
[2m[36m(pid=12118)[0m  eval loss: 0.8929968392848968, eval err: 0.06374239921569824
Result for train_2e5ff572:
  date: 2021-09-11_00-47-13
  done: true
  err: 0.06374239921569824
  experiment_id: 6182879d3a564234a93c418ed0965a8a
  hostname: bigcuda4
  iterations_since_restore: 27
  loss: 0.8929968392848968
  loss_train: 1.191042854785919
  node_ip: 131.220.7.54
  pid: 12118
  should_checkpoint: true
  time_since_restore: 9356.761360168457
  time_this_iter_s: 348.2764160633087
  time_total_s: 9356.761360168457
  timestamp: 1631314033
  timesteps_since_restore: 0
  training_iteration: 27
  trial_id: 2e5ff572
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (10 PENDING, 1 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ff572    | RUNNING    | 131.220.7.54:12118 |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_2e5fd682    | PENDING    |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (4 TERMINATED)


[2m[36m(pid=12125)[0m Epoch:
[2m[36m(pid=12125)[0m 0, 
[2m[36m(pid=12125)[0m  train loss: 29.547378311157228
[2m[36m(pid=12125)[0m  eval loss: 107.60886199951172, eval err: 0.9903466176986694
Result for train_2e5fd682:
  date: 2021-09-11_00-49-28
  done: false
  err: 0.9903466176986694
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 107.60886199951172
  loss_train: 29.547378311157228
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 132.94858527183533
  time_this_iter_s: 132.94858527183533
  time_total_s: 132.94858527183533
  timestamp: 1631314168
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |      1 |          132.949 | 107.609    | 0.990347  |     29.5474  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |            |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |            |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |            |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |   0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |   0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |   0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  |  11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 1, 
[2m[36m(pid=12125)[0m  train loss: 14.809646606445312
[2m[36m(pid=12125)[0m  eval loss: 13.33859977722168, eval err: 0.8538435053825378
[2m[36m(pid=12125)[0m 2, 
[2m[36m(pid=12125)[0m  train loss: 10.890050201416015
[2m[36m(pid=12125)[0m  eval loss: 11.199287948608399, eval err: 0.8327140736579896
[2m[36m(pid=12125)[0m 3, 
[2m[36m(pid=12125)[0m  train loss: 11.79739891052246
[2m[36m(pid=12125)[0m  eval loss: 16.326199226379394, eval err: 0.8783327007293701
Result for train_2e5fd682:
  date: 2021-09-11_00-55-58
  done: false
  err: 0.8783327007293701
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 16.326199226379394
  loss_train: 11.79739891052246
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 523.2277710437775
  time_this_iter_s: 390.27918577194214
  time_total_s: 523.2277710437775
  timestamp: 1631314558
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |      2 |          523.228 | 16.3262   | 0.878333  |     11.7974  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 4, 
[2m[36m(pid=12125)[0m  train loss: 11.168040466308593
[2m[36m(pid=12125)[0m  eval loss: 14.901087760925293, eval err: 0.8809629154205322
[2m[36m(pid=12125)[0m 5, 
[2m[36m(pid=12125)[0m  train loss: 10.887745552062988
[2m[36m(pid=12125)[0m  eval loss: 12.86257038116455, eval err: 0.8540846133232116
[2m[36m(pid=12125)[0m 6, 
[2m[36m(pid=12125)[0m  train loss: 10.73774326324463
[2m[36m(pid=12125)[0m  eval loss: 16.851622848510743, eval err: 0.8978127193450928
Result for train_2e5fd682:
  date: 2021-09-11_01-02-28
  done: false
  err: 0.8978127193450928
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 16.851622848510743
  loss_train: 10.73774326324463
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 912.7060594558716
  time_this_iter_s: 389.4782884120941
  time_total_s: 912.7060594558716
  timestamp: 1631314948
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |      3 |          912.706 | 16.8516   | 0.897813  |     10.7377  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 7, 
[2m[36m(pid=12125)[0m  train loss: 10.510213832855225
[2m[36m(pid=12125)[0m  eval loss: 15.128812026977538, eval err: 0.8883509254455566
[2m[36m(pid=12125)[0m 8, 
[2m[36m(pid=12125)[0m  train loss: 10.074114208221436
[2m[36m(pid=12125)[0m  eval loss: 10.695815925598145, eval err: 0.8402701878547668
[2m[36m(pid=12125)[0m 9, 
[2m[36m(pid=12125)[0m  train loss: 9.789635372161865
Result for train_2e5fd682:
  date: 2021-09-11_01-08-58
  done: false
  err: 0.8846593308448791
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 14.454632644653321
  loss_train: 9.789635372161865
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 1302.5501008033752
  time_this_iter_s: 389.84404134750366
  time_total_s: 1302.5501008033752
  timestamp: 1631315338
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 2e5fd682
  
[2m[36m(pid=12125)[0m  eval loss: 14.454632644653321, eval err: 0.8846593308448791
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |      4 |          1302.55 | 14.4546   | 0.884659  |      9.78964 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 10, 
[2m[36m(pid=12125)[0m  train loss: 10.695887908935546
[2m[36m(pid=12125)[0m  eval loss: 11.106304740905761, eval err: 0.8382323241233826
[2m[36m(pid=12125)[0m 11, 
[2m[36m(pid=12125)[0m  train loss: 10.472978744506836
[2m[36m(pid=12125)[0m  eval loss: 9.913398818969727, eval err: 0.8288761520385742
[2m[36m(pid=12125)[0m 12, 
[2m[36m(pid=12125)[0m  train loss: 10.129579944610596
Result for train_2e5fd682:
  date: 2021-09-11_01-15-26
  done: false
  err: 0.8517569971084594
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 11.338535423278808
  loss_train: 10.129579944610596
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 1691.227366924286
  time_this_iter_s: 388.67726612091064
  time_total_s: 1691.227366924286
  timestamp: 1631315726
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 2e5fd682
  
[2m[36m(pid=12125)[0m  eval loss: 11.338535423278808, eval err: 0.8517569971084594
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |      5 |          1691.23 | 11.3385   | 0.851757  |     10.1296  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 13, 
[2m[36m(pid=12125)[0m  train loss: 9.471934604644776
[2m[36m(pid=12125)[0m  eval loss: 9.755335216522218, eval err: 0.8209476923942566
[2m[36m(pid=12125)[0m 14, 
[2m[36m(pid=12125)[0m  train loss: 9.133584156036378
[2m[36m(pid=12125)[0m  eval loss: 12.135584411621094, eval err: 0.8576463890075684
[2m[36m(pid=12125)[0m 15, 
[2m[36m(pid=12125)[0m  train loss: 9.327447681427001
[2m[36m(pid=12125)[0m  eval loss: 10.224233245849609, eval err: 0.833784556388855
Result for train_2e5fd682:
  date: 2021-09-11_01-21-55
  done: false
  err: 0.833784556388855
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 10.224233245849609
  loss_train: 9.327447681427001
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 2080.1472215652466
  time_this_iter_s: 388.9198546409607
  time_total_s: 2080.1472215652466
  timestamp: 1631316115
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |      6 |          2080.15 | 10.2242   | 0.833785  |      9.32745 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 16, 
[2m[36m(pid=12125)[0m  train loss: 9.696618995666505
[2m[36m(pid=12125)[0m  eval loss: 12.477794189453125, eval err: 0.8686917352676392
[2m[36m(pid=12125)[0m 17, 
[2m[36m(pid=12125)[0m  train loss: 8.646364555358886
[2m[36m(pid=12125)[0m  eval loss: 12.196824645996093, eval err: 0.8672459077835083
[2m[36m(pid=12125)[0m 18, 
[2m[36m(pid=12125)[0m  train loss: 8.745950088500976
[2m[36m(pid=12125)[0m  eval loss: 11.31644775390625, eval err: 0.8462659382820129
Result for train_2e5fd682:
  date: 2021-09-11_01-28-23
  done: false
  err: 0.8462659382820129
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 11.31644775390625
  loss_train: 8.745950088500976
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 2468.379629611969
  time_this_iter_s: 388.2324080467224
  time_total_s: 2468.379629611969
  timestamp: 1631316503
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |      7 |          2468.38 | 11.3164   | 0.846266  |      8.74595 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 19, 
[2m[36m(pid=12125)[0m  train loss: 9.084327545166015
[2m[36m(pid=12125)[0m  eval loss: 12.023680114746094, eval err: 0.8544583082199096
[2m[36m(pid=12125)[0m 20, 
[2m[36m(pid=12125)[0m  train loss: 9.236229629516602
[2m[36m(pid=12125)[0m  eval loss: 10.97439109802246, eval err: 0.8365252852439881
[2m[36m(pid=12125)[0m 21, 
[2m[36m(pid=12125)[0m  train loss: 9.305706882476807
[2m[36m(pid=12125)[0m  eval loss: 13.124547080993652, eval err: 0.8602257585525512
Result for train_2e5fd682:
  date: 2021-09-11_01-34-51
  done: false
  err: 0.8602257585525512
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 13.124547080993652
  loss_train: 9.305706882476807
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 2856.2054183483124
  time_this_iter_s: 387.8257887363434
  time_total_s: 2856.2054183483124
  timestamp: 1631316891
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |      8 |          2856.21 | 13.1245   | 0.860226  |      9.30571 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 22, 
[2m[36m(pid=12125)[0m  train loss: 8.227169971466065
[2m[36m(pid=12125)[0m  eval loss: 9.783013744354248, eval err: 0.8132590007781982
[2m[36m(pid=12125)[0m 23, 
[2m[36m(pid=12125)[0m  train loss: 8.352968692779541
[2m[36m(pid=12125)[0m  eval loss: 11.029658184051513, eval err: 0.8473639440536499
[2m[36m(pid=12125)[0m 24, 
[2m[36m(pid=12125)[0m  train loss: 8.149277458190918
[2m[36m(pid=12125)[0m  eval loss: 11.25724308013916, eval err: 0.8509331798553467
Result for train_2e5fd682:
  date: 2021-09-11_01-41-20
  done: false
  err: 0.8509331798553467
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 11.25724308013916
  loss_train: 8.149277458190918
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 3244.5081593990326
  time_this_iter_s: 388.3027410507202
  time_total_s: 3244.5081593990326
  timestamp: 1631317280
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |      9 |          3244.51 | 11.2572   | 0.850933  |      8.14928 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 25, 
[2m[36m(pid=12125)[0m  train loss: 8.15546531677246
[2m[36m(pid=12125)[0m  eval loss: 11.145085487365723, eval err: 0.8423820924758911
[2m[36m(pid=12125)[0m 26, 
[2m[36m(pid=12125)[0m  train loss: 7.894700126647949
[2m[36m(pid=12125)[0m  eval loss: 11.722172241210938, eval err: 0.8263355040550232
[2m[36m(pid=12125)[0m 27, 
[2m[36m(pid=12125)[0m  train loss: 7.203490600585938
[2m[36m(pid=12125)[0m  eval loss: 9.131268138885497, eval err: 0.7334141230583191
Result for train_2e5fd682:
  date: 2021-09-11_01-47-49
  done: false
  err: 0.7334141230583191
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 9.131268138885497
  loss_train: 7.203490600585938
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 3633.9538345336914
  time_this_iter_s: 389.4456751346588
  time_total_s: 3633.9538345336914
  timestamp: 1631317669
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     10 |          3633.95 |  9.13127  | 0.733414  |      7.20349 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 28, 
[2m[36m(pid=12125)[0m  train loss: 6.943054428100586
[2m[36m(pid=12125)[0m  eval loss: 6.450245399475097, eval err: 0.6205528116226197
[2m[36m(pid=12125)[0m 29, 
[2m[36m(pid=12125)[0m  train loss: 6.16340404510498
[2m[36m(pid=12125)[0m  eval loss: 6.202462892532349, eval err: 0.5155536198616028
[2m[36m(pid=12125)[0m 30, 
[2m[36m(pid=12125)[0m  train loss: 4.296914691925049
[2m[36m(pid=12125)[0m  eval loss: 3.0394588565826415, eval err: 0.3074781441688538
Result for train_2e5fd682:
  date: 2021-09-11_01-54-17
  done: false
  err: 0.3074781441688538
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 3.0394588565826415
  loss_train: 4.296914691925049
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 4021.9711623191833
  time_this_iter_s: 388.01732778549194
  time_total_s: 4021.9711623191833
  timestamp: 1631318057
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     11 |          4021.97 |  3.03946  | 0.307478  |      4.29691 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 31, 
[2m[36m(pid=12125)[0m  train loss: 3.5930178356170654
[2m[36m(pid=12125)[0m  eval loss: 2.119869408607483, eval err: 0.22246708869934081
[2m[36m(pid=12125)[0m 32, 
[2m[36m(pid=12125)[0m  train loss: 2.9354927587509154
[2m[36m(pid=12125)[0m  eval loss: 2.163394546508789, eval err: 0.205839319229126
[2m[36m(pid=12125)[0m 33, 
[2m[36m(pid=12125)[0m  train loss: 2.914191174507141
[2m[36m(pid=12125)[0m  eval loss: 1.548308937549591, eval err: 0.14565720558166503
Result for train_2e5fd682:
  date: 2021-09-11_02-00-42
  done: false
  err: 0.14565720558166503
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 1.548308937549591
  loss_train: 2.914191174507141
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 4407.038834810257
  time_this_iter_s: 385.0676724910736
  time_total_s: 4407.038834810257
  timestamp: 1631318442
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     12 |          4407.04 |  1.54831  | 0.145657  |      2.91419 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 34, 
[2m[36m(pid=12125)[0m  train loss: 2.2841596841812133
[2m[36m(pid=12125)[0m  eval loss: 1.4277311849594116, eval err: 0.12197937488555909
[2m[36m(pid=12125)[0m 35, 
[2m[36m(pid=12125)[0m  train loss: 2.181568856239319
[2m[36m(pid=12125)[0m  eval loss: 1.3634019875526429, eval err: 0.1110901689529419
[2m[36m(pid=12125)[0m 36, 
[2m[36m(pid=12125)[0m  train loss: 2.0325691747665404
[2m[36m(pid=12125)[0m  eval loss: 1.1987188339233399, eval err: 0.09691665410995483
Result for train_2e5fd682:
  date: 2021-09-11_02-07-04
  done: false
  err: 0.09691665410995483
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 1.1987188339233399
  loss_train: 2.0325691747665404
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 4789.079306840897
  time_this_iter_s: 382.04047203063965
  time_total_s: 4789.079306840897
  timestamp: 1631318824
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     13 |          4789.08 |  1.19872  | 0.0969167 |      2.03257 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 37, 
[2m[36m(pid=12125)[0m  train loss: 1.9948494386672975
[2m[36m(pid=12125)[0m  eval loss: 1.204203362464905, eval err: 0.09563311338424682
[2m[36m(pid=12125)[0m 38, 
[2m[36m(pid=12125)[0m  train loss: 1.7558506059646606
[2m[36m(pid=12125)[0m  eval loss: 1.213518536090851, eval err: 0.09889224290847778
[2m[36m(pid=12125)[0m 39, 
[2m[36m(pid=12125)[0m  train loss: 1.832044425010681
[2m[36m(pid=12125)[0m  eval loss: 1.2529931879043579, eval err: 0.09803965330123901
Result for train_2e5fd682:
  date: 2021-09-11_02-13-25
  done: false
  err: 0.09803965330123901
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 1.2529931879043579
  loss_train: 1.832044425010681
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 5169.667682170868
  time_this_iter_s: 380.5883753299713
  time_total_s: 5169.667682170868
  timestamp: 1631319205
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     14 |          5169.67 |  1.25299  | 0.0980397 |      1.83204 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 40, 
[2m[36m(pid=12125)[0m  train loss: 1.5924704360961914
[2m[36m(pid=12125)[0m  eval loss: 1.0765509390830994, eval err: 0.08287813901901245
[2m[36m(pid=12125)[0m 41, 
[2m[36m(pid=12125)[0m  train loss: 1.3833545541763306
[2m[36m(pid=12125)[0m  eval loss: 1.0336836504936218, eval err: 0.07955903053283692
[2m[36m(pid=12125)[0m 42, 
[2m[36m(pid=12125)[0m  train loss: 1.527778992652893
[2m[36m(pid=12125)[0m  eval loss: 1.175284366607666, eval err: 0.09086372137069702
Result for train_2e5fd682:
  date: 2021-09-11_02-19-44
  done: false
  err: 0.09086372137069702
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 1.175284366607666
  loss_train: 1.527778992652893
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 5549.43768453598
  time_this_iter_s: 379.7700023651123
  time_total_s: 5549.43768453598
  timestamp: 1631319584
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     15 |          5549.44 |  1.17528  | 0.0908637 |      1.52778 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 43, 
[2m[36m(pid=12125)[0m  train loss: 1.6243130993843078
[2m[36m(pid=12125)[0m  eval loss: 1.0457539677619934, eval err: 0.07832166194915771
[2m[36m(pid=12125)[0m 44, 
[2m[36m(pid=12125)[0m  train loss: 1.5138064551353454
[2m[36m(pid=12125)[0m  eval loss: 1.0055111479759216, eval err: 0.07601984977722168
[2m[36m(pid=12125)[0m 45, 
[2m[36m(pid=12125)[0m  train loss: 1.489158308506012
Result for train_2e5fd682:
  date: 2021-09-11_02-26-04
  done: false
  err: 0.07601963996887207
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 1.0592041850090026
  loss_train: 1.489158308506012
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 5928.462247610092
  time_this_iter_s: 379.02456307411194
  time_total_s: 5928.462247610092
  timestamp: 1631319964
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: 2e5fd682
  
[2m[36m(pid=12125)[0m  eval loss: 1.0592041850090026, eval err: 0.07601963996887207
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     16 |          5928.46 |  1.0592   | 0.0760196 |      1.48916 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 46, 
[2m[36m(pid=12125)[0m  train loss: 1.51562753200531
[2m[36m(pid=12125)[0m  eval loss: 1.0244422888755798, eval err: 0.07130783557891845
[2m[36m(pid=12125)[0m 47, 
[2m[36m(pid=12125)[0m  train loss: 1.396650457382202
[2m[36m(pid=12125)[0m  eval loss: 0.9780644309520722, eval err: 0.0696604871749878
[2m[36m(pid=12125)[0m 48, 
[2m[36m(pid=12125)[0m  train loss: 1.3131840991973878
[2m[36m(pid=12125)[0m  eval loss: 0.964820705652237, eval err: 0.06880625486373901
Result for train_2e5fd682:
  date: 2021-09-11_02-32-23
  done: false
  err: 0.06880625486373901
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 0.964820705652237
  loss_train: 1.3131840991973878
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 6307.989375591278
  time_this_iter_s: 379.5271279811859
  time_total_s: 6307.989375591278
  timestamp: 1631320343
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     17 |          6307.99 |  0.964821 | 0.0688063 |      1.31318 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 49, 
[2m[36m(pid=12125)[0m  train loss: 1.4010191297531127
[2m[36m(pid=12125)[0m  eval loss: 0.9531452119350433, eval err: 0.06786760568618774
[2m[36m(pid=12125)[0m 50, 
[2m[36m(pid=12125)[0m  train loss: 1.3839578413963318
[2m[36m(pid=12125)[0m  eval loss: 0.9422667503356934, eval err: 0.06714397192001342
[2m[36m(pid=12125)[0m 51, 
[2m[36m(pid=12125)[0m  train loss: 1.3975807070732116
[2m[36m(pid=12125)[0m  eval loss: 0.9389696907997132, eval err: 0.0663817572593689
Result for train_2e5fd682:
  date: 2021-09-11_02-38-43
  done: false
  err: 0.0663817572593689
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 0.9389696907997132
  loss_train: 1.3975807070732116
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 6687.479599475861
  time_this_iter_s: 379.4902238845825
  time_total_s: 6687.479599475861
  timestamp: 1631320723
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     18 |          6687.48 |  0.93897  | 0.0663818 |      1.39758 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 52, 
[2m[36m(pid=12125)[0m  train loss: 1.232514979839325
[2m[36m(pid=12125)[0m  eval loss: 0.9369061577320099, eval err: 0.06643560171127319
[2m[36m(pid=12125)[0m 53, 
[2m[36m(pid=12125)[0m  train loss: 1.370900774002075
[2m[36m(pid=12125)[0m  eval loss: 0.9395107972621918, eval err: 0.06687199115753174
[2m[36m(pid=12125)[0m 54, 
[2m[36m(pid=12125)[0m  train loss: 1.337764072418213
Result for train_2e5fd682:
  date: 2021-09-11_02-45-02
  done: false
  err: 0.06641286849975586
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 0.9375113010406494
  loss_train: 1.337764072418213
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 7066.707889795303
  time_this_iter_s: 379.22829031944275
  time_total_s: 7066.707889795303
  timestamp: 1631321102
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: 2e5fd682
  
[2m[36m(pid=12125)[0m  eval loss: 0.9375113010406494, eval err: 0.06641286849975586
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     19 |          7066.71 |  0.937511 | 0.0664129 |      1.33776 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 55, 
[2m[36m(pid=12125)[0m  train loss: 1.2751543712615967
[2m[36m(pid=12125)[0m  eval loss: 0.9252866649627686, eval err: 0.06493295907974243
[2m[36m(pid=12125)[0m 56, 
[2m[36m(pid=12125)[0m  train loss: 1.2390724968910218
[2m[36m(pid=12125)[0m  eval loss: 0.929592308998108, eval err: 0.06578718662261963
[2m[36m(pid=12125)[0m 57, 
[2m[36m(pid=12125)[0m  train loss: 1.1832034397125244
[2m[36m(pid=12125)[0m  eval loss: 0.9224459779262543, eval err: 0.06478003740310669
Result for train_2e5fd682:
  date: 2021-09-11_02-51-22
  done: false
  err: 0.06478003740310669
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 0.9224459779262543
  loss_train: 1.1832034397125244
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 7446.54464173317
  time_this_iter_s: 379.8367519378662
  time_total_s: 7446.54464173317
  timestamp: 1631321482
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     20 |          7446.54 |  0.922446 | 0.06478   |      1.1832  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12125)[0m 58, 
[2m[36m(pid=12125)[0m  train loss: 1.1763161849975585
[2m[36m(pid=12125)[0m  eval loss: 0.917393684387207, eval err: 0.06502687454223632
[2m[36m(pid=12125)[0m 59, 
[2m[36m(pid=12125)[0m  train loss: 1.2576001119613647
[2m[36m(pid=12125)[0m  eval loss: 0.9148051047325134, eval err: 0.06479191541671753
[2m[36m(pid=12125)[0m 60, 
[2m[36m(pid=12125)[0m  train loss: 1.2964636826515197
[2m[36m(pid=12125)[0m  eval loss: 0.9095545709133148, eval err: 0.06376141309738159
Result for train_2e5fd682:
  date: 2021-09-11_02-57-41
  done: true
  err: 0.06376141309738159
  experiment_id: eb0312e9b1a9476db7d4c3b7e3485aca
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 0.9095545709133148
  loss_train: 1.2964636826515197
  node_ip: 131.220.7.54
  pid: 12125
  should_checkpoint: true
  time_since_restore: 7825.887162685394
  time_this_iter_s: 379.34252095222473
  time_total_s: 7825.887162685394
  timestamp: 1631321861
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: 2e5fd682
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (9 PENDING, 1 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5fd682    | RUNNING    | 131.220.7.54:12125 |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_2e5f955a    | PENDING    |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m Epoch:
[2m[36m(pid=12123)[0m 0, 
[2m[36m(pid=12123)[0m  train loss: 30.717321395874023
Result for train_2e5f955a:
  date: 2021-09-11_02-59-42
  done: false
  err: 0.998632915019989
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 114.08098480224609
  loss_train: 30.717321395874023
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 119.29361605644226
  time_this_iter_s: 119.29361605644226
  time_total_s: 119.29361605644226
  timestamp: 1631321982
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 2e5f955a
  
[2m[36m(pid=12123)[0m  eval loss: 114.08098480224609, eval err: 0.998632915019989
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |      1 |          119.294 | 114.081    | 0.998633  |     30.7173  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |            |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |            |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |            |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |   0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |   0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |   0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |   0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  |  11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 1, 
[2m[36m(pid=12123)[0m  train loss: 15.961605415344238
[2m[36m(pid=12123)[0m  eval loss: 15.100307159423828, eval err: 0.873195698261261
[2m[36m(pid=12123)[0m 2, 
[2m[36m(pid=12123)[0m  train loss: 11.952847175598144
[2m[36m(pid=12123)[0m  eval loss: 28.011105041503907, eval err: 0.9391921949386597
[2m[36m(pid=12123)[0m 3, 
[2m[36m(pid=12123)[0m  train loss: 11.430443077087402
[2m[36m(pid=12123)[0m  eval loss: 24.845405502319338, eval err: 0.9477978539466858
Result for train_2e5f955a:
  date: 2021-09-11_03-05-31
  done: false
  err: 0.9477978539466858
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 24.845405502319338
  loss_train: 11.430443077087402
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 467.80492520332336
  time_this_iter_s: 348.5113091468811
  time_total_s: 467.80492520332336
  timestamp: 1631322331
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |      2 |          467.805 | 24.8454   | 0.947798  |     11.4304  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 4, 
[2m[36m(pid=12123)[0m  train loss: 12.444084892272949
[2m[36m(pid=12123)[0m  eval loss: 16.129955673217772, eval err: 0.8794231462478638
[2m[36m(pid=12123)[0m 5, 
[2m[36m(pid=12123)[0m  train loss: 12.070229225158691
[2m[36m(pid=12123)[0m  eval loss: 12.80836524963379, eval err: 0.8541968464851379
[2m[36m(pid=12123)[0m 6, 
[2m[36m(pid=12123)[0m  train loss: 10.868807601928712
[2m[36m(pid=12123)[0m  eval loss: 15.171436767578125, eval err: 0.8801763677597045
Result for train_2e5f955a:
  date: 2021-09-11_03-11-19
  done: false
  err: 0.8801763677597045
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 15.171436767578125
  loss_train: 10.868807601928712
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 815.7852783203125
  time_this_iter_s: 347.98035311698914
  time_total_s: 815.7852783203125
  timestamp: 1631322679
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |      3 |          815.785 | 15.1714   | 0.880176  |     10.8688  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 7, 
[2m[36m(pid=12123)[0m  train loss: 10.56763011932373
[2m[36m(pid=12123)[0m  eval loss: 11.46545467376709, eval err: 0.8439289331436157
[2m[36m(pid=12123)[0m 8, 
[2m[36m(pid=12123)[0m  train loss: 10.41270990371704
[2m[36m(pid=12123)[0m  eval loss: 12.442978401184082, eval err: 0.8556117868423462
[2m[36m(pid=12123)[0m 9, 
[2m[36m(pid=12123)[0m  train loss: 11.07583782196045
[2m[36m(pid=12123)[0m  eval loss: 10.596330490112305, eval err: 0.8356017518043518
Result for train_2e5f955a:
  date: 2021-09-11_03-17-06
  done: false
  err: 0.8356017518043518
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 10.596330490112305
  loss_train: 11.07583782196045
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 1163.6194078922272
  time_this_iter_s: 347.8341295719147
  time_total_s: 1163.6194078922272
  timestamp: 1631323026
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |      4 |          1163.62 | 10.5963   | 0.835602  |     11.0758  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 10, 
[2m[36m(pid=12123)[0m  train loss: 10.143566627502441
[2m[36m(pid=12123)[0m  eval loss: 11.253599929809571, eval err: 0.835929102897644
[2m[36m(pid=12123)[0m 11, 
[2m[36m(pid=12123)[0m  train loss: 10.383411960601807
[2m[36m(pid=12123)[0m  eval loss: 12.853264923095702, eval err: 0.8544113159179687
[2m[36m(pid=12123)[0m 12, 
[2m[36m(pid=12123)[0m  train loss: 10.438719787597655
[2m[36m(pid=12123)[0m  eval loss: 10.423139419555664, eval err: 0.8389266967773438
Result for train_2e5f955a:
  date: 2021-09-11_03-22-54
  done: false
  err: 0.8389266967773438
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 10.423139419555664
  loss_train: 10.438719787597655
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 1511.3328535556793
  time_this_iter_s: 347.71344566345215
  time_total_s: 1511.3328535556793
  timestamp: 1631323374
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |      5 |          1511.33 | 10.4231   | 0.838927  |     10.4387  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 13, 
[2m[36m(pid=12123)[0m  train loss: 10.206818084716797
[2m[36m(pid=12123)[0m  eval loss: 11.963442077636719, eval err: 0.8570863437652588
[2m[36m(pid=12123)[0m 14, 
[2m[36m(pid=12123)[0m  train loss: 9.06711275100708
[2m[36m(pid=12123)[0m  eval loss: 11.372781028747559, eval err: 0.8632619047164917
[2m[36m(pid=12123)[0m 15, 
[2m[36m(pid=12123)[0m  train loss: 8.966951885223388
[2m[36m(pid=12123)[0m  eval loss: 12.273678283691407, eval err: 0.8575307989120483
Result for train_2e5f955a:
  date: 2021-09-11_03-28-42
  done: false
  err: 0.8575307989120483
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 12.273678283691407
  loss_train: 8.966951885223388
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 1858.9393260478973
  time_this_iter_s: 347.606472492218
  time_total_s: 1858.9393260478973
  timestamp: 1631323722
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |      6 |          1858.94 | 12.2737   | 0.857531  |      8.96695 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 16, 
[2m[36m(pid=12123)[0m  train loss: 9.731266269683838
[2m[36m(pid=12123)[0m  eval loss: 9.016240215301513, eval err: 0.7888470077514649
[2m[36m(pid=12123)[0m 17, 
[2m[36m(pid=12123)[0m  train loss: 9.12974271774292
[2m[36m(pid=12123)[0m  eval loss: 10.869325885772705, eval err: 0.8352525043487549
[2m[36m(pid=12123)[0m 18, 
[2m[36m(pid=12123)[0m  train loss: 10.065212135314942
[2m[36m(pid=12123)[0m  eval loss: 14.557946662902832, eval err: 0.8770167398452758
Result for train_2e5f955a:
  date: 2021-09-11_03-34-29
  done: false
  err: 0.8770167398452758
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 14.557946662902832
  loss_train: 10.065212135314942
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 2206.4361493587494
  time_this_iter_s: 347.49682331085205
  time_total_s: 2206.4361493587494
  timestamp: 1631324069
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |      7 |          2206.44 | 14.5579   | 0.877017  |     10.0652  |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 19, 
[2m[36m(pid=12123)[0m  train loss: 8.846851291656494
[2m[36m(pid=12123)[0m  eval loss: 12.765675659179687, eval err: 0.8795530414581298
[2m[36m(pid=12123)[0m 20, 
[2m[36m(pid=12123)[0m  train loss: 9.932982025146485
[2m[36m(pid=12123)[0m  eval loss: 10.37670768737793, eval err: 0.8424095964431763
[2m[36m(pid=12123)[0m 21, 
[2m[36m(pid=12123)[0m  train loss: 9.428354969024658
Result for train_2e5f955a:
  date: 2021-09-11_03-40-17
  done: false
  err: 0.8444914221763611
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 10.119115123748779
  loss_train: 9.428354969024658
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 2554.044340610504
  time_this_iter_s: 347.60819125175476
  time_total_s: 2554.044340610504
  timestamp: 1631324417
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 2e5f955a
  
[2m[36m(pid=12123)[0m  eval loss: 10.119115123748779, eval err: 0.8444914221763611
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |      8 |          2554.04 | 10.1191   | 0.844491  |      9.42835 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 22, 
[2m[36m(pid=12123)[0m  train loss: 9.891256198883056
[2m[36m(pid=12123)[0m  eval loss: 12.5810595703125, eval err: 0.8646775841712951
[2m[36m(pid=12123)[0m 23, 
[2m[36m(pid=12123)[0m  train loss: 9.223374900817872
[2m[36m(pid=12123)[0m  eval loss: 11.488663063049316, eval err: 0.8470534181594849
[2m[36m(pid=12123)[0m 24, 
[2m[36m(pid=12123)[0m  train loss: 9.353629341125488
[2m[36m(pid=12123)[0m  eval loss: 9.346062183380127, eval err: 0.8105293941497803
Result for train_2e5f955a:
  date: 2021-09-11_03-46-04
  done: false
  err: 0.8105293941497803
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 9.346062183380127
  loss_train: 9.353629341125488
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 2901.525249004364
  time_this_iter_s: 347.48090839385986
  time_total_s: 2901.525249004364
  timestamp: 1631324764
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |      9 |          2901.53 |  9.34606  | 0.810529  |      9.35363 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 25, 
[2m[36m(pid=12123)[0m  train loss: 9.20324234008789
[2m[36m(pid=12123)[0m  eval loss: 10.914341011047362, eval err: 0.8354730296134949
[2m[36m(pid=12123)[0m 26, 
[2m[36m(pid=12123)[0m  train loss: 8.684872245788574
[2m[36m(pid=12123)[0m  eval loss: 9.030099506378173, eval err: 0.7845302224159241
[2m[36m(pid=12123)[0m 27, 
[2m[36m(pid=12123)[0m  train loss: 9.656626873016357
[2m[36m(pid=12123)[0m  eval loss: 15.568290634155273, eval err: 0.8793440866470337
Result for train_2e5f955a:
  date: 2021-09-11_03-51-51
  done: false
  err: 0.8793440866470337
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 15.568290634155273
  loss_train: 9.656626873016357
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 3248.6921684741974
  time_this_iter_s: 347.1669194698334
  time_total_s: 3248.6921684741974
  timestamp: 1631325111
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     10 |          3248.69 | 15.5683   | 0.879344  |      9.65663 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 28, 
[2m[36m(pid=12123)[0m  train loss: 8.896286907196044
[2m[36m(pid=12123)[0m  eval loss: 11.265737113952637, eval err: 0.839929780960083
[2m[36m(pid=12123)[0m 29, 
[2m[36m(pid=12123)[0m  train loss: 8.853199100494384
[2m[36m(pid=12123)[0m  eval loss: 10.596488647460937, eval err: 0.829121732711792
[2m[36m(pid=12123)[0m 30, 
[2m[36m(pid=12123)[0m  train loss: 8.658649196624756
[2m[36m(pid=12123)[0m  eval loss: 9.794501800537109, eval err: 0.8184196305274963
Result for train_2e5f955a:
  date: 2021-09-11_03-57-39
  done: false
  err: 0.8184196305274963
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 9.794501800537109
  loss_train: 8.658649196624756
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 3596.193670272827
  time_this_iter_s: 347.50150179862976
  time_total_s: 3596.193670272827
  timestamp: 1631325459
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     11 |          3596.19 |  9.7945   | 0.81842   |      8.65865 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 31, 
[2m[36m(pid=12123)[0m  train loss: 8.646531162261963
[2m[36m(pid=12123)[0m  eval loss: 10.052826156616211, eval err: 0.8301387643814087
[2m[36m(pid=12123)[0m 32, 
[2m[36m(pid=12123)[0m  train loss: 8.799602012634278
[2m[36m(pid=12123)[0m  eval loss: 11.074114952087402, eval err: 0.8425602459907532
[2m[36m(pid=12123)[0m 33, 
[2m[36m(pid=12123)[0m  train loss: 8.824937553405762
[2m[36m(pid=12123)[0m  eval loss: 11.267014961242676, eval err: 0.8442793750762939
Result for train_2e5f955a:
  date: 2021-09-11_04-03-26
  done: false
  err: 0.8442793750762939
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 11.267014961242676
  loss_train: 8.824937553405762
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 3943.1619510650635
  time_this_iter_s: 346.9682807922363
  time_total_s: 3943.1619510650635
  timestamp: 1631325806
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     12 |          3943.16 | 11.267    | 0.844279  |      8.82494 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 34, 
[2m[36m(pid=12123)[0m  train loss: 8.299195137023926
[2m[36m(pid=12123)[0m  eval loss: 8.179516983032226, eval err: 0.7916072964668274
[2m[36m(pid=12123)[0m 35, 
[2m[36m(pid=12123)[0m  train loss: 8.98119535446167
[2m[36m(pid=12123)[0m  eval loss: 10.085216941833496, eval err: 0.8234232354164124
[2m[36m(pid=12123)[0m 36, 
[2m[36m(pid=12123)[0m  train loss: 8.554737434387206
[2m[36m(pid=12123)[0m  eval loss: 9.379698543548583, eval err: 0.8122426438331604
Result for train_2e5f955a:
  date: 2021-09-11_04-09-13
  done: false
  err: 0.8122426438331604
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 9.379698543548583
  loss_train: 8.554737434387206
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 4290.406542778015
  time_this_iter_s: 347.24459171295166
  time_total_s: 4290.406542778015
  timestamp: 1631326153
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     13 |          4290.41 |  9.3797   | 0.812243  |      8.55474 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12123)[0m 37, 
[2m[36m(pid=12123)[0m  train loss: 8.05330286026001
[2m[36m(pid=12123)[0m  eval loss: 7.615863475799561, eval err: 0.7651763725280761
[2m[36m(pid=12123)[0m 38, 
[2m[36m(pid=12123)[0m  train loss: 8.043930587768555
[2m[36m(pid=12123)[0m  eval loss: 11.447797393798828, eval err: 0.8398323845863342
[2m[36m(pid=12123)[0m 39, 
[2m[36m(pid=12123)[0m  train loss: 7.255137634277344
[2m[36m(pid=12123)[0m  eval loss: 8.813295822143555, eval err: 0.7360653376579285
Result for train_2e5f955a:
  date: 2021-09-11_04-15-01
  done: true
  err: 0.7360653376579285
  experiment_id: 2e9b85e7bdc04b4c802d1078497165a2
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 8.813295822143555
  loss_train: 7.255137634277344
  node_ip: 131.220.7.54
  pid: 12123
  should_checkpoint: true
  time_since_restore: 4638.250486373901
  time_this_iter_s: 347.84394359588623
  time_total_s: 4638.250486373901
  timestamp: 1631326501
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 2e5f955a
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (8 PENDING, 1 RUNNING, 16 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f955a    | RUNNING    | 131.220.7.54:12123 |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5ee57e    | PENDING    |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |        |                  |           |           |              |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m Epoch:
[2m[36m(pid=12126)[0m 0, 
[2m[36m(pid=12126)[0m  train loss: 31.91431816101074
[2m[36m(pid=12126)[0m  eval loss: 85.52226455688476, eval err: 0.9820820593833923
Result for train_2e5ee57e:
  date: 2021-09-11_04-17-02
  done: false
  err: 0.9820820593833923
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 85.52226455688476
  loss_train: 31.91431816101074
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 119.09583568572998
  time_this_iter_s: 119.09583568572998
  time_total_s: 119.09583568572998
  timestamp: 1631326622
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |      1 |          119.096 | 85.5223   | 0.982082  |     31.9143  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 1, 
[2m[36m(pid=12126)[0m  train loss: 31.37030876159668
[2m[36m(pid=12126)[0m  eval loss: 74.07659942626952, eval err: 0.971277768611908
[2m[36m(pid=12126)[0m 2, 
[2m[36m(pid=12126)[0m  train loss: 26.312214279174803
[2m[36m(pid=12126)[0m  eval loss: 58.933528442382816, eval err: 0.9668365836143493
[2m[36m(pid=12126)[0m 3, 
[2m[36m(pid=12126)[0m  train loss: 18.753717651367186
[2m[36m(pid=12126)[0m  eval loss: 19.389814720153808, eval err: 0.8886161684989929
Result for train_2e5ee57e:
  date: 2021-09-11_04-22-51
  done: false
  err: 0.8886161684989929
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 19.389814720153808
  loss_train: 18.753717651367186
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 468.17183017730713
  time_this_iter_s: 349.07599449157715
  time_total_s: 468.17183017730713
  timestamp: 1631326971
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |      2 |          468.172 | 19.3898   | 0.888616  |     18.7537  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 4, 
[2m[36m(pid=12126)[0m  train loss: 14.808567390441894
[2m[36m(pid=12126)[0m  eval loss: 17.060875549316407, eval err: 0.8857649374008179
[2m[36m(pid=12126)[0m 5, 
[2m[36m(pid=12126)[0m  train loss: 12.495961952209473
[2m[36m(pid=12126)[0m  eval loss: 15.776967430114746, eval err: 0.8788460946083069
[2m[36m(pid=12126)[0m 6, 
[2m[36m(pid=12126)[0m  train loss: 11.670628051757813
[2m[36m(pid=12126)[0m  eval loss: 13.828504676818847, eval err: 0.8636123251914978
Result for train_2e5ee57e:
  date: 2021-09-11_04-28-40
  done: false
  err: 0.8636123251914978
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 13.828504676818847
  loss_train: 11.670628051757813
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 817.0311534404755
  time_this_iter_s: 348.85932326316833
  time_total_s: 817.0311534404755
  timestamp: 1631327320
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |      3 |          817.031 | 13.8285   | 0.863612  |     11.6706  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 7, 
[2m[36m(pid=12126)[0m  train loss: 11.446092224121093
[2m[36m(pid=12126)[0m  eval loss: 12.841434478759766, eval err: 0.8507366061210633
[2m[36m(pid=12126)[0m 8, 
[2m[36m(pid=12126)[0m  train loss: 10.381082324981689
[2m[36m(pid=12126)[0m  eval loss: 12.776535987854004, eval err: 0.853330249786377
[2m[36m(pid=12126)[0m 9, 
[2m[36m(pid=12126)[0m  train loss: 10.814288349151612
[2m[36m(pid=12126)[0m  eval loss: 12.079242820739745, eval err: 0.8419656825065612
Result for train_2e5ee57e:
  date: 2021-09-11_04-34-28
  done: false
  err: 0.8419656825065612
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 12.079242820739745
  loss_train: 10.814288349151612
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 1165.4790844917297
  time_this_iter_s: 348.4479310512543
  time_total_s: 1165.4790844917297
  timestamp: 1631327668
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |      4 |          1165.48 | 12.0792   | 0.841966  |     10.8143  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 10, 
[2m[36m(pid=12126)[0m  train loss: 10.416181964874268
[2m[36m(pid=12126)[0m  eval loss: 12.601988143920899, eval err: 0.8517496132850647
[2m[36m(pid=12126)[0m 11, 
[2m[36m(pid=12126)[0m  train loss: 10.259724311828613
[2m[36m(pid=12126)[0m  eval loss: 11.734102897644043, eval err: 0.8461732244491578
[2m[36m(pid=12126)[0m 12, 
[2m[36m(pid=12126)[0m  train loss: 9.843558731079101
[2m[36m(pid=12126)[0m  eval loss: 11.375587501525878, eval err: 0.8380281949043273
Result for train_2e5ee57e:
  date: 2021-09-11_04-40-16
  done: false
  err: 0.8380281949043273
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 11.375587501525878
  loss_train: 9.843558731079101
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 1513.661892414093
  time_this_iter_s: 348.1828079223633
  time_total_s: 1513.661892414093
  timestamp: 1631328016
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |      5 |          1513.66 | 11.3756   | 0.838028  |      9.84356 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 13, 
[2m[36m(pid=12126)[0m  train loss: 9.34722620010376
[2m[36m(pid=12126)[0m  eval loss: 11.57099967956543, eval err: 0.8393729519844055
[2m[36m(pid=12126)[0m 14, 
[2m[36m(pid=12126)[0m  train loss: 9.870271129608154
[2m[36m(pid=12126)[0m  eval loss: 11.510912399291993, eval err: 0.8387433409690856
[2m[36m(pid=12126)[0m 15, 
[2m[36m(pid=12126)[0m  train loss: 9.897229232788085
[2m[36m(pid=12126)[0m  eval loss: 11.528054428100585, eval err: 0.8436198663711548
Result for train_2e5ee57e:
  date: 2021-09-11_04-46-05
  done: false
  err: 0.8436198663711548
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 11.528054428100585
  loss_train: 9.897229232788085
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 1862.174204826355
  time_this_iter_s: 348.51231241226196
  time_total_s: 1862.174204826355
  timestamp: 1631328365
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |      6 |          1862.17 | 11.5281   | 0.84362   |      9.89723 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 16, 
[2m[36m(pid=12126)[0m  train loss: 9.94156841278076
[2m[36m(pid=12126)[0m  eval loss: 11.777782859802246, eval err: 0.8438382792472839
[2m[36m(pid=12126)[0m 17, 
[2m[36m(pid=12126)[0m  train loss: 10.52814058303833
[2m[36m(pid=12126)[0m  eval loss: 12.299805870056153, eval err: 0.8494008016586304
[2m[36m(pid=12126)[0m 18, 
[2m[36m(pid=12126)[0m  train loss: 9.966940841674806
[2m[36m(pid=12126)[0m  eval loss: 11.715110054016113, eval err: 0.8484755659103393
Result for train_2e5ee57e:
  date: 2021-09-11_04-51-54
  done: false
  err: 0.8484755659103393
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 11.715110054016113
  loss_train: 9.966940841674806
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 2211.2098824977875
  time_this_iter_s: 349.0356776714325
  time_total_s: 2211.2098824977875
  timestamp: 1631328714
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |      7 |          2211.21 | 11.7151   | 0.848476  |      9.96694 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 19, 
[2m[36m(pid=12126)[0m  train loss: 9.415581798553466
[2m[36m(pid=12126)[0m  eval loss: 11.356783199310303, eval err: 0.8407865810394287
[2m[36m(pid=12126)[0m 20, 
[2m[36m(pid=12126)[0m  train loss: 9.641402702331543
[2m[36m(pid=12126)[0m  eval loss: 11.466795978546143, eval err: 0.8480131673812866
[2m[36m(pid=12126)[0m 21, 
[2m[36m(pid=12126)[0m  train loss: 9.858139114379883
[2m[36m(pid=12126)[0m  eval loss: 11.428555698394776, eval err: 0.8411704611778259
Result for train_2e5ee57e:
  date: 2021-09-11_04-57-43
  done: false
  err: 0.8411704611778259
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 11.428555698394776
  loss_train: 9.858139114379883
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 2559.8259518146515
  time_this_iter_s: 348.616069316864
  time_total_s: 2559.8259518146515
  timestamp: 1631329063
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |      8 |          2559.83 | 11.4286   | 0.84117   |      9.85814 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 22, 
[2m[36m(pid=12126)[0m  train loss: 9.472769069671632
[2m[36m(pid=12126)[0m  eval loss: 10.57819221496582, eval err: 0.8271562671661377
[2m[36m(pid=12126)[0m 23, 
[2m[36m(pid=12126)[0m  train loss: 9.294816551208497
[2m[36m(pid=12126)[0m  eval loss: 11.106573085784913, eval err: 0.835659830570221
[2m[36m(pid=12126)[0m 24, 
[2m[36m(pid=12126)[0m  train loss: 9.687673377990723
[2m[36m(pid=12126)[0m  eval loss: 10.905653457641602, eval err: 0.8383789181709289
Result for train_2e5ee57e:
  date: 2021-09-11_05-03-31
  done: false
  err: 0.8383789181709289
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 10.905653457641602
  loss_train: 9.687673377990723
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 2907.945558309555
  time_this_iter_s: 348.11960649490356
  time_total_s: 2907.945558309555
  timestamp: 1631329411
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |      9 |          2907.95 | 10.9057   | 0.838379  |      9.68767 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 25, 
[2m[36m(pid=12126)[0m  train loss: 9.666489791870116
[2m[36m(pid=12126)[0m  eval loss: 11.123640098571776, eval err: 0.8454271340370179
[2m[36m(pid=12126)[0m 26, 
[2m[36m(pid=12126)[0m  train loss: 9.490748710632325
[2m[36m(pid=12126)[0m  eval loss: 10.923178844451904, eval err: 0.8368555855751038
[2m[36m(pid=12126)[0m 27, 
[2m[36m(pid=12126)[0m  train loss: 8.98642852783203
[2m[36m(pid=12126)[0m  eval loss: 10.874005317687988, eval err: 0.8394673585891723
Result for train_2e5ee57e:
  date: 2021-09-11_05-09-19
  done: false
  err: 0.8394673585891723
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 10.874005317687988
  loss_train: 8.98642852783203
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 3256.6107807159424
  time_this_iter_s: 348.66522240638733
  time_total_s: 3256.6107807159424
  timestamp: 1631329759
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     10 |          3256.61 | 10.874    | 0.839467  |      8.98643 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 28, 
[2m[36m(pid=12126)[0m  train loss: 9.068973693847656
[2m[36m(pid=12126)[0m  eval loss: 10.199675178527832, eval err: 0.824813585281372
[2m[36m(pid=12126)[0m 29, 
[2m[36m(pid=12126)[0m  train loss: 8.91520128250122
[2m[36m(pid=12126)[0m  eval loss: 9.86554292678833, eval err: 0.8130495738983154
[2m[36m(pid=12126)[0m 30, 
[2m[36m(pid=12126)[0m  train loss: 9.202510147094726
[2m[36m(pid=12126)[0m  eval loss: 10.724594593048096, eval err: 0.8325324106216431
Result for train_2e5ee57e:
  date: 2021-09-11_05-15-08
  done: false
  err: 0.8325324106216431
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 10.724594593048096
  loss_train: 9.202510147094726
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 3605.2881014347076
  time_this_iter_s: 348.67732071876526
  time_total_s: 3605.2881014347076
  timestamp: 1631330108
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     11 |          3605.29 | 10.7246   | 0.832532  |      9.20251 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 31, 
[2m[36m(pid=12126)[0m  train loss: 9.44114200592041
[2m[36m(pid=12126)[0m  eval loss: 10.252093753814698, eval err: 0.8248538994789123
[2m[36m(pid=12126)[0m 32, 
[2m[36m(pid=12126)[0m  train loss: 9.349143943786622
[2m[36m(pid=12126)[0m  eval loss: 9.96314655303955, eval err: 0.8156773352622986
[2m[36m(pid=12126)[0m 33, 
[2m[36m(pid=12126)[0m  train loss: 9.443308200836181
[2m[36m(pid=12126)[0m  eval loss: 10.110413398742676, eval err: 0.8239022970199585
Result for train_2e5ee57e:
  date: 2021-09-11_05-20-57
  done: false
  err: 0.8239022970199585
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 10.110413398742676
  loss_train: 9.443308200836181
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 3954.391669511795
  time_this_iter_s: 349.1035680770874
  time_total_s: 3954.391669511795
  timestamp: 1631330457
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     12 |          3954.39 | 10.1104   | 0.823902  |      9.44331 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 34, 
[2m[36m(pid=12126)[0m  train loss: 9.199042625427246
[2m[36m(pid=12126)[0m  eval loss: 10.13391170501709, eval err: 0.8239100003242492
[2m[36m(pid=12126)[0m 35, 
[2m[36m(pid=12126)[0m  train loss: 8.654406967163085
[2m[36m(pid=12126)[0m  eval loss: 9.867475109100342, eval err: 0.8204894590377808
[2m[36m(pid=12126)[0m 36, 
[2m[36m(pid=12126)[0m  train loss: 9.508598461151124
[2m[36m(pid=12126)[0m  eval loss: 9.312588596343994, eval err: 0.8029450559616089
Result for train_2e5ee57e:
  date: 2021-09-11_05-26-46
  done: false
  err: 0.8029450559616089
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 9.312588596343994
  loss_train: 9.508598461151124
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 4303.655237674713
  time_this_iter_s: 349.2635681629181
  time_total_s: 4303.655237674713
  timestamp: 1631330806
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     13 |          4303.66 |  9.31259  | 0.802945  |      9.5086  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12126)[0m 37, 
[2m[36m(pid=12126)[0m  train loss: 8.96594976425171
[2m[36m(pid=12126)[0m  eval loss: 9.58439640045166, eval err: 0.8103340721130371
[2m[36m(pid=12126)[0m 38, 
[2m[36m(pid=12126)[0m  train loss: 9.116984901428223
[2m[36m(pid=12126)[0m  eval loss: 9.596402244567871, eval err: 0.8087163400650025
[2m[36m(pid=12126)[0m 39, 
[2m[36m(pid=12126)[0m  train loss: 8.904728031158447
[2m[36m(pid=12126)[0m  eval loss: 9.678850078582764, eval err: 0.8164959669113159
Result for train_2e5ee57e:
  date: 2021-09-11_05-32-35
  done: true
  err: 0.8164959669113159
  experiment_id: a5b2c72de54f4541ac921fe0bd852710
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 9.678850078582764
  loss_train: 8.904728031158447
  node_ip: 131.220.7.54
  pid: 12126
  should_checkpoint: true
  time_since_restore: 4652.149630784988
  time_this_iter_s: 348.49439311027527
  time_total_s: 4652.149630784988
  timestamp: 1631331155
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 2e5ee57e
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (7 PENDING, 1 RUNNING, 17 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5ee57e    | RUNNING    | 131.220.7.54:12126 |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f3312    | PENDING    |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m Epoch:
[2m[36m(pid=12121)[0m 0, 
[2m[36m(pid=12121)[0m  train loss: 28.78424980163574
Result for train_2e5f3312:
  date: 2021-09-11_05-34-36
  done: false
  err: 0.9964942526817322
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 114.77935943603515
  loss_train: 28.78424980163574
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 118.92155981063843
  time_this_iter_s: 118.92155981063843
  time_total_s: 118.92155981063843
  timestamp: 1631331276
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 2e5f3312
  
[2m[36m(pid=12121)[0m  eval loss: 114.77935943603515, eval err: 0.9964942526817322
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |      1 |          118.922 | 114.779    | 0.996494  |     28.7842  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |            |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |            |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |            |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |   9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |   8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |   0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |   0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |   0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |   0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  |  11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 1, 
[2m[36m(pid=12121)[0m  train loss: 13.748521575927734
[2m[36m(pid=12121)[0m  eval loss: 16.355003662109375, eval err: 0.893444664478302
[2m[36m(pid=12121)[0m 2, 
[2m[36m(pid=12121)[0m  train loss: 11.842025680541992
[2m[36m(pid=12121)[0m  eval loss: 13.669089069366455, eval err: 0.8695720672607422
[2m[36m(pid=12121)[0m 3, 
[2m[36m(pid=12121)[0m  train loss: 12.109451789855957
[2m[36m(pid=12121)[0m  eval loss: 13.454165344238282, eval err: 0.8605056118965149
Result for train_2e5f3312:
  date: 2021-09-11_05-40-24
  done: false
  err: 0.8605056118965149
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 13.454165344238282
  loss_train: 12.109451789855957
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 467.2799139022827
  time_this_iter_s: 348.3583540916443
  time_total_s: 467.2799139022827
  timestamp: 1631331624
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |      2 |           467.28 | 13.4542   | 0.860506  |     12.1095  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 4, 
[2m[36m(pid=12121)[0m  train loss: 11.486815338134766
[2m[36m(pid=12121)[0m  eval loss: 13.294620399475098, eval err: 0.8629187273979188
[2m[36m(pid=12121)[0m 5, 
[2m[36m(pid=12121)[0m  train loss: 11.623641204833984
[2m[36m(pid=12121)[0m  eval loss: 13.691233367919923, eval err: 0.8561374640464783
[2m[36m(pid=12121)[0m 6, 
[2m[36m(pid=12121)[0m  train loss: 11.263410682678222
[2m[36m(pid=12121)[0m  eval loss: 11.997815055847168, eval err: 0.8654269123077393
Result for train_2e5f3312:
  date: 2021-09-11_05-46-12
  done: false
  err: 0.8654269123077393
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 11.997815055847168
  loss_train: 11.263410682678222
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 814.7958965301514
  time_this_iter_s: 347.51598262786865
  time_total_s: 814.7958965301514
  timestamp: 1631331972
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |      3 |          814.796 | 11.9978   | 0.865427  |     11.2634  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 7, 
[2m[36m(pid=12121)[0m  train loss: 10.988486938476562
[2m[36m(pid=12121)[0m  eval loss: 12.983469161987305, eval err: 0.8604587459564209
[2m[36m(pid=12121)[0m 8, 
[2m[36m(pid=12121)[0m  train loss: 10.99547119140625
[2m[36m(pid=12121)[0m  eval loss: 11.433017883300781, eval err: 0.8668111515045166
[2m[36m(pid=12121)[0m 9, 
[2m[36m(pid=12121)[0m  train loss: 11.429337615966796
[2m[36m(pid=12121)[0m  eval loss: 10.96015209197998, eval err: 0.8320376777648926
Result for train_2e5f3312:
  date: 2021-09-11_05-51-59
  done: false
  err: 0.8320376777648926
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 10.96015209197998
  loss_train: 11.429337615966796
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 1162.3207235336304
  time_this_iter_s: 347.524827003479
  time_total_s: 1162.3207235336304
  timestamp: 1631332319
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |      4 |          1162.32 | 10.9602   | 0.832038  |     11.4293  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 10, 
[2m[36m(pid=12121)[0m  train loss: 10.48498249053955
[2m[36m(pid=12121)[0m  eval loss: 10.358355751037598, eval err: 0.8313876557350158
[2m[36m(pid=12121)[0m 11, 
[2m[36m(pid=12121)[0m  train loss: 10.154073810577392
[2m[36m(pid=12121)[0m  eval loss: 11.059562568664552, eval err: 0.8516203808784485
[2m[36m(pid=12121)[0m 12, 
[2m[36m(pid=12121)[0m  train loss: 10.01173891067505
[2m[36m(pid=12121)[0m  eval loss: 12.869514045715333, eval err: 0.8580618333816529
Result for train_2e5f3312:
  date: 2021-09-11_05-57-47
  done: false
  err: 0.8580618333816529
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 12.869514045715333
  loss_train: 10.01173891067505
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 1509.7708582878113
  time_this_iter_s: 347.4501347541809
  time_total_s: 1509.7708582878113
  timestamp: 1631332667
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |      5 |          1509.77 | 12.8695   | 0.858062  |     10.0117  |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 13, 
[2m[36m(pid=12121)[0m  train loss: 9.457472839355468
[2m[36m(pid=12121)[0m  eval loss: 10.285804767608642, eval err: 0.8351612973213196
[2m[36m(pid=12121)[0m 14, 
[2m[36m(pid=12121)[0m  train loss: 9.575195751190186
[2m[36m(pid=12121)[0m  eval loss: 10.627609825134277, eval err: 0.8224254226684571
[2m[36m(pid=12121)[0m 15, 
[2m[36m(pid=12121)[0m  train loss: 9.817206535339356
Result for train_2e5f3312:
  date: 2021-09-11_06-03-34
  done: false
  err: 0.8334962558746338
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 10.174404106140138
  loss_train: 9.817206535339356
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 1857.0191204547882
  time_this_iter_s: 347.24826216697693
  time_total_s: 1857.0191204547882
  timestamp: 1631333014
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 2e5f3312
  
[2m[36m(pid=12121)[0m  eval loss: 10.174404106140138, eval err: 0.8334962558746338
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |      6 |          1857.02 | 10.1744   | 0.833496  |      9.81721 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 16, 
[2m[36m(pid=12121)[0m  train loss: 9.971099605560303
[2m[36m(pid=12121)[0m  eval loss: 9.851373844146728, eval err: 0.8166842555999756
[2m[36m(pid=12121)[0m 17, 
[2m[36m(pid=12121)[0m  train loss: 9.053887557983398
[2m[36m(pid=12121)[0m  eval loss: 9.493514575958251, eval err: 0.8090059065818787
[2m[36m(pid=12121)[0m 18, 
[2m[36m(pid=12121)[0m  train loss: 9.661510791778564
Result for train_2e5f3312:
  date: 2021-09-11_06-09-21
  done: false
  err: 0.8378334569931031
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 10.676318855285645
  loss_train: 9.661510791778564
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 2203.9527263641357
  time_this_iter_s: 346.93360590934753
  time_total_s: 2203.9527263641357
  timestamp: 1631333361
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 2e5f3312
  
[2m[36m(pid=12121)[0m  eval loss: 10.676318855285645, eval err: 0.8378334569931031
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |      7 |          2203.95 | 10.6763   | 0.837833  |      9.66151 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 19, 
[2m[36m(pid=12121)[0m  train loss: 8.164574298858643
[2m[36m(pid=12121)[0m  eval loss: 15.426627388000488, eval err: 0.8872164082527161
[2m[36m(pid=12121)[0m 20, 
[2m[36m(pid=12121)[0m  train loss: 9.890164337158204
[2m[36m(pid=12121)[0m  eval loss: 11.548485488891602, eval err: 0.8470724415779114
[2m[36m(pid=12121)[0m 21, 
[2m[36m(pid=12121)[0m  train loss: 9.41058053970337
Result for train_2e5f3312:
  date: 2021-09-11_06-15-07
  done: false
  err: 0.8449733066558838
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 10.483408203125
  loss_train: 9.41058053970337
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 2550.3858687877655
  time_this_iter_s: 346.43314242362976
  time_total_s: 2550.3858687877655
  timestamp: 1631333707
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 2e5f3312
  
[2m[36m(pid=12121)[0m  eval loss: 10.483408203125, eval err: 0.8449733066558838
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |      8 |          2550.39 | 10.4834   | 0.844973  |      9.41058 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 22, 
[2m[36m(pid=12121)[0m  train loss: 9.11464044570923
[2m[36m(pid=12121)[0m  eval loss: 10.126785469055175, eval err: 0.841290385723114
[2m[36m(pid=12121)[0m 23, 
[2m[36m(pid=12121)[0m  train loss: 8.840138149261474
[2m[36m(pid=12121)[0m  eval loss: 9.682478580474854, eval err: 0.8454119348526001
[2m[36m(pid=12121)[0m 24, 
[2m[36m(pid=12121)[0m  train loss: 8.219382801055907
[2m[36m(pid=12121)[0m  eval loss: 9.390986499786377, eval err: 0.8281566166877746
Result for train_2e5f3312:
  date: 2021-09-11_06-20-54
  done: false
  err: 0.8281566166877746
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 9.390986499786377
  loss_train: 8.219382801055907
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 2897.1096119880676
  time_this_iter_s: 346.7237432003021
  time_total_s: 2897.1096119880676
  timestamp: 1631334054
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |      9 |          2897.11 |  9.39099  | 0.828157  |      8.21938 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 25, 
[2m[36m(pid=12121)[0m  train loss: 9.044337787628173
[2m[36m(pid=12121)[0m  eval loss: 9.16478452682495, eval err: 0.8296740245819092
[2m[36m(pid=12121)[0m 26, 
[2m[36m(pid=12121)[0m  train loss: 8.798579559326171
[2m[36m(pid=12121)[0m  eval loss: 11.712845153808594, eval err: 0.8561777710914612
[2m[36m(pid=12121)[0m 27, 
[2m[36m(pid=12121)[0m  train loss: 8.526173973083496
[2m[36m(pid=12121)[0m  eval loss: 9.972257194519043, eval err: 0.8321601486206055
Result for train_2e5f3312:
  date: 2021-09-11_06-26-41
  done: false
  err: 0.8321601486206055
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 9.972257194519043
  loss_train: 8.526173973083496
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 3244.557106256485
  time_this_iter_s: 347.44749426841736
  time_total_s: 3244.557106256485
  timestamp: 1631334401
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     10 |          3244.56 |  9.97226  | 0.83216   |      8.52617 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 28, 
[2m[36m(pid=12121)[0m  train loss: 8.625373287200928
[2m[36m(pid=12121)[0m  eval loss: 10.026943740844727, eval err: 0.8329173159599305
[2m[36m(pid=12121)[0m 29, 
[2m[36m(pid=12121)[0m  train loss: 8.397901191711426
[2m[36m(pid=12121)[0m  eval loss: 8.296600685119628, eval err: 0.794539270401001
[2m[36m(pid=12121)[0m 30, 
[2m[36m(pid=12121)[0m  train loss: 7.579948463439941
Result for train_2e5f3312:
  date: 2021-09-11_06-32-28
  done: false
  err: 0.8264794301986694
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 10.452310256958008
  loss_train: 7.579948463439941
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 3591.6382489204407
  time_this_iter_s: 347.0811426639557
  time_total_s: 3591.6382489204407
  timestamp: 1631334748
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 2e5f3312
  
[2m[36m(pid=12121)[0m  eval loss: 10.452310256958008, eval err: 0.8264794301986694
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     11 |          3591.64 | 10.4523   | 0.826479  |      7.57995 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 31, 
[2m[36m(pid=12121)[0m  train loss: 7.958733558654785
[2m[36m(pid=12121)[0m  eval loss: 9.976694679260254, eval err: 0.7946972942352295
[2m[36m(pid=12121)[0m 32, 
[2m[36m(pid=12121)[0m  train loss: 7.830402374267578
[2m[36m(pid=12121)[0m  eval loss: 9.287240829467773, eval err: 0.7638403105735779
[2m[36m(pid=12121)[0m 33, 
[2m[36m(pid=12121)[0m  train loss: 6.946985645294189
[2m[36m(pid=12121)[0m  eval loss: 7.099424514770508, eval err: 0.6681023359298706
Result for train_2e5f3312:
  date: 2021-09-11_06-38-16
  done: false
  err: 0.6681023359298706
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 7.099424514770508
  loss_train: 6.946985645294189
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 3939.045625925064
  time_this_iter_s: 347.4073770046234
  time_total_s: 3939.045625925064
  timestamp: 1631335096
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     12 |          3939.05 |  7.09942  | 0.668102  |      6.94699 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 34, 
[2m[36m(pid=12121)[0m  train loss: 5.784494571685791
[2m[36m(pid=12121)[0m  eval loss: 4.626165428161621, eval err: 0.47798753023147583
[2m[36m(pid=12121)[0m 35, 
[2m[36m(pid=12121)[0m  train loss: 4.620221929550171
[2m[36m(pid=12121)[0m  eval loss: 3.943516721725464, eval err: 0.3773136854171753
[2m[36m(pid=12121)[0m 36, 
[2m[36m(pid=12121)[0m  train loss: 3.551385269165039
[2m[36m(pid=12121)[0m  eval loss: 2.051610689163208, eval err: 0.19343875408172606
Result for train_2e5f3312:
  date: 2021-09-11_06-44-01
  done: false
  err: 0.19343875408172606
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 2.051610689163208
  loss_train: 3.551385269165039
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 4283.965198278427
  time_this_iter_s: 344.91957235336304
  time_total_s: 4283.965198278427
  timestamp: 1631335441
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     13 |          4283.97 |  2.05161  | 0.193439  |      3.55139 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 37, 
[2m[36m(pid=12121)[0m  train loss: 3.3557772254943847
[2m[36m(pid=12121)[0m  eval loss: 1.6115672707557678, eval err: 0.13845759868621826
[2m[36m(pid=12121)[0m 38, 
[2m[36m(pid=12121)[0m  train loss: 3.1314251375198365
[2m[36m(pid=12121)[0m  eval loss: 2.645288624763489, eval err: 0.2301890516281128
[2m[36m(pid=12121)[0m 39, 
[2m[36m(pid=12121)[0m  train loss: 2.6200944709777834
Result for train_2e5f3312:
  date: 2021-09-11_06-49-41
  done: false
  err: 0.11984632968902588
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 1.4311310839653015
  loss_train: 2.6200944709777834
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 4623.930396795273
  time_this_iter_s: 339.9651985168457
  time_total_s: 4623.930396795273
  timestamp: 1631335781
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 2e5f3312
  
[2m[36m(pid=12121)[0m  eval loss: 1.4311310839653015, eval err: 0.11984632968902588
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     14 |          4623.93 |  1.43113  | 0.119846  |      2.62009 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 40, 
[2m[36m(pid=12121)[0m  train loss: 2.518693060874939
[2m[36m(pid=12121)[0m  eval loss: 1.8410034012794494, eval err: 0.1433723545074463
[2m[36m(pid=12121)[0m 41, 
[2m[36m(pid=12121)[0m  train loss: 2.09595862865448
[2m[36m(pid=12121)[0m  eval loss: 1.3734553027153016, eval err: 0.11044726133346558
[2m[36m(pid=12121)[0m 42, 
[2m[36m(pid=12121)[0m  train loss: 1.885877022743225
[2m[36m(pid=12121)[0m  eval loss: 1.2358496403694152, eval err: 0.09753651142120362
Result for train_2e5f3312:
  date: 2021-09-11_06-55-19
  done: false
  err: 0.09753651142120362
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 1.2358496403694152
  loss_train: 1.885877022743225
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 4962.527462720871
  time_this_iter_s: 338.59706592559814
  time_total_s: 4962.527462720871
  timestamp: 1631336119
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     15 |          4962.53 |  1.23585  | 0.0975365 |      1.88588 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 43, 
[2m[36m(pid=12121)[0m  train loss: 1.974960675239563
[2m[36m(pid=12121)[0m  eval loss: 1.3400420594215392, eval err: 0.10581292867660523
[2m[36m(pid=12121)[0m 44, 
[2m[36m(pid=12121)[0m  train loss: 1.863746428489685
[2m[36m(pid=12121)[0m  eval loss: 1.145109145641327, eval err: 0.08699640035629272
[2m[36m(pid=12121)[0m 45, 
[2m[36m(pid=12121)[0m  train loss: 1.7962477731704711
[2m[36m(pid=12121)[0m  eval loss: 1.1620182585716248, eval err: 0.09003020286560058
Result for train_2e5f3312:
  date: 2021-09-11_07-00-57
  done: false
  err: 0.09003020286560058
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 1.1620182585716248
  loss_train: 1.7962477731704711
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 5300.3590524196625
  time_this_iter_s: 337.8315896987915
  time_total_s: 5300.3590524196625
  timestamp: 1631336457
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     16 |          5300.36 |  1.16202  | 0.0900302 |      1.79625 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 46, 
[2m[36m(pid=12121)[0m  train loss: 1.7528546667098999
[2m[36m(pid=12121)[0m  eval loss: 1.0944539868831635, eval err: 0.08064776420593261
[2m[36m(pid=12121)[0m 47, 
[2m[36m(pid=12121)[0m  train loss: 1.6217569255828856
[2m[36m(pid=12121)[0m  eval loss: 1.0938896870613097, eval err: 0.0786474323272705
[2m[36m(pid=12121)[0m 48, 
[2m[36m(pid=12121)[0m  train loss: 1.5971074461936952
[2m[36m(pid=12121)[0m  eval loss: 1.0717872023582458, eval err: 0.07754342079162597
Result for train_2e5f3312:
  date: 2021-09-11_07-06-34
  done: false
  err: 0.07754342079162597
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 1.0717872023582458
  loss_train: 1.5971074461936952
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 5637.424139499664
  time_this_iter_s: 337.06508708000183
  time_total_s: 5637.424139499664
  timestamp: 1631336794
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     17 |          5637.42 |  1.07179  | 0.0775434 |      1.59711 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 49, 
[2m[36m(pid=12121)[0m  train loss: 1.6840413522720337
[2m[36m(pid=12121)[0m  eval loss: 1.0401377069950104, eval err: 0.07511181831359863
[2m[36m(pid=12121)[0m 50, 
[2m[36m(pid=12121)[0m  train loss: 1.4744978094100951
[2m[36m(pid=12121)[0m  eval loss: 1.0774918627738952, eval err: 0.07486705541610718
[2m[36m(pid=12121)[0m 51, 
[2m[36m(pid=12121)[0m  train loss: 1.7550135803222657
[2m[36m(pid=12121)[0m  eval loss: 1.0746503472328186, eval err: 0.07857130765914917
Result for train_2e5f3312:
  date: 2021-09-11_07-12-11
  done: false
  err: 0.07857130765914917
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 1.0746503472328186
  loss_train: 1.7550135803222657
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 5974.681019306183
  time_this_iter_s: 337.25687980651855
  time_total_s: 5974.681019306183
  timestamp: 1631337131
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     18 |          5974.68 |  1.07465  | 0.0785713 |      1.75501 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 52, 
[2m[36m(pid=12121)[0m  train loss: 1.6469826459884644
[2m[36m(pid=12121)[0m  eval loss: 1.120045096874237, eval err: 0.07673365354537964
[2m[36m(pid=12121)[0m 53, 
[2m[36m(pid=12121)[0m  train loss: 1.6427854561805726
[2m[36m(pid=12121)[0m  eval loss: 1.0171296525001525, eval err: 0.072042396068573
[2m[36m(pid=12121)[0m 54, 
[2m[36m(pid=12121)[0m  train loss: 1.395191421508789
[2m[36m(pid=12121)[0m  eval loss: 0.9911685967445374, eval err: 0.07014113187789917
Result for train_2e5f3312:
  date: 2021-09-11_07-17-49
  done: false
  err: 0.07014113187789917
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 0.9911685967445374
  loss_train: 1.395191421508789
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 6312.165129184723
  time_this_iter_s: 337.48410987854004
  time_total_s: 6312.165129184723
  timestamp: 1631337469
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     19 |          6312.17 |  0.991169 | 0.0701411 |      1.39519 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 55, 
[2m[36m(pid=12121)[0m  train loss: 1.482239179611206
[2m[36m(pid=12121)[0m  eval loss: 0.9976254940032959, eval err: 0.07097139835357666
[2m[36m(pid=12121)[0m 56, 
[2m[36m(pid=12121)[0m  train loss: 1.5100480604171753
[2m[36m(pid=12121)[0m  eval loss: 0.9961879146099091, eval err: 0.06993011236190796
[2m[36m(pid=12121)[0m 57, 
[2m[36m(pid=12121)[0m  train loss: 1.3495562434196473
[2m[36m(pid=12121)[0m  eval loss: 0.9869019639492035, eval err: 0.06915172100067139
Result for train_2e5f3312:
  date: 2021-09-11_07-23-26
  done: false
  err: 0.06915172100067139
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 0.9869019639492035
  loss_train: 1.3495562434196473
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 6649.57906293869
  time_this_iter_s: 337.4139337539673
  time_total_s: 6649.57906293869
  timestamp: 1631337806
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     20 |          6649.58 |  0.986902 | 0.0691517 |      1.34956 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 58, 
[2m[36m(pid=12121)[0m  train loss: 1.3506603503227235
[2m[36m(pid=12121)[0m  eval loss: 0.9801838612556457, eval err: 0.06955758094787598
[2m[36m(pid=12121)[0m 59, 
[2m[36m(pid=12121)[0m  train loss: 1.3473779892921447
[2m[36m(pid=12121)[0m  eval loss: 0.9777766644954682, eval err: 0.06833931922912598
[2m[36m(pid=12121)[0m 60, 
[2m[36m(pid=12121)[0m  train loss: 1.4053207755088806
[2m[36m(pid=12121)[0m  eval loss: 0.9711992287635803, eval err: 0.06841667890548705
Result for train_2e5f3312:
  date: 2021-09-11_07-29-04
  done: false
  err: 0.06841667890548705
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 0.9711992287635803
  loss_train: 1.4053207755088806
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 6986.832090854645
  time_this_iter_s: 337.2530279159546
  time_total_s: 6986.832090854645
  timestamp: 1631338144
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     21 |          6986.83 |  0.971199 | 0.0684167 |      1.40532 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12121)[0m 61, 
[2m[36m(pid=12121)[0m  train loss: 1.3561858797073365
[2m[36m(pid=12121)[0m  eval loss: 0.9635458064079284, eval err: 0.06774624109268189
[2m[36m(pid=12121)[0m 62, 
[2m[36m(pid=12121)[0m  train loss: 1.6366109824180604
[2m[36m(pid=12121)[0m  eval loss: 0.9674824106693268, eval err: 0.06881526947021484
[2m[36m(pid=12121)[0m 63, 
[2m[36m(pid=12121)[0m  train loss: 1.616648006439209
[2m[36m(pid=12121)[0m  eval loss: 0.9573055875301361, eval err: 0.06685686349868775
Result for train_2e5f3312:
  date: 2021-09-11_07-34-41
  done: true
  err: 0.06685686349868775
  experiment_id: ddd6c0d5c16946dbb9c11b59103dd4d4
  hostname: bigcuda4
  iterations_since_restore: 22
  loss: 0.9573055875301361
  loss_train: 1.616648006439209
  node_ip: 131.220.7.54
  pid: 12121
  should_checkpoint: true
  time_since_restore: 7323.984267234802
  time_this_iter_s: 337.15217638015747
  time_total_s: 7323.984267234802
  timestamp: 1631338481
  timesteps_since_restore: 0
  training_iteration: 22
  trial_id: 2e5f3312
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (6 PENDING, 1 RUNNING, 18 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f3312    | RUNNING    | 131.220.7.54:12121 |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f0ed2    | PENDING    |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |        |                  |           |           |              |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m Epoch:
[2m[36m(pid=12108)[0m 0, 
[2m[36m(pid=12108)[0m  train loss: 31.879841232299803
[2m[36m(pid=12108)[0m  eval loss: 51.43228141784668, eval err: 0.9438427209854126
Result for train_2e5f0ed2:
  date: 2021-09-11_07-36-56
  done: false
  err: 0.9438427209854126
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 51.43228141784668
  loss_train: 31.879841232299803
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 133.2995526790619
  time_this_iter_s: 133.2995526790619
  time_total_s: 133.2995526790619
  timestamp: 1631338616
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |      1 |           133.3  | 51.4323   | 0.943843  |     31.8798  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 1, 
[2m[36m(pid=12108)[0m  train loss: 14.389166564941407
[2m[36m(pid=12108)[0m  eval loss: 11.420294952392577, eval err: 0.8340069222450256
[2m[36m(pid=12108)[0m 2, 
[2m[36m(pid=12108)[0m  train loss: 12.312620754241943
[2m[36m(pid=12108)[0m  eval loss: 12.110418205261231, eval err: 0.8444606447219849
[2m[36m(pid=12108)[0m 3, 
[2m[36m(pid=12108)[0m  train loss: 11.015804615020752
Result for train_2e5f0ed2:
  date: 2021-09-11_07-43-25
  done: false
  err: 0.8174289917945862
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 10.469901580810546
  loss_train: 11.015804615020752
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 522.8465230464935
  time_this_iter_s: 389.54697036743164
  time_total_s: 522.8465230464935
  timestamp: 1631339005
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 2e5f0ed2
  
[2m[36m(pid=12108)[0m  eval loss: 10.469901580810546, eval err: 0.8174289917945862
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |      2 |          522.847 | 10.4699   | 0.817429  |     11.0158  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 4, 
[2m[36m(pid=12108)[0m  train loss: 11.65742389678955
[2m[36m(pid=12108)[0m  eval loss: 14.881663589477538, eval err: 0.8676955199241638
[2m[36m(pid=12108)[0m 5, 
[2m[36m(pid=12108)[0m  train loss: 10.446901149749756
[2m[36m(pid=12108)[0m  eval loss: 12.68796989440918, eval err: 0.8420410466194153
[2m[36m(pid=12108)[0m 6, 
[2m[36m(pid=12108)[0m  train loss: 8.889730434417725
[2m[36m(pid=12108)[0m  eval loss: 9.99548053741455, eval err: 0.8086719107627869
Result for train_2e5f0ed2:
  date: 2021-09-11_07-49-54
  done: false
  err: 0.8086719107627869
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 9.99548053741455
  loss_train: 8.889730434417725
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 911.1601097583771
  time_this_iter_s: 388.31358671188354
  time_total_s: 911.1601097583771
  timestamp: 1631339394
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |      3 |           911.16 |  9.99548  | 0.808672  |      8.88973 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 7, 
[2m[36m(pid=12108)[0m  train loss: 9.267358112335206
[2m[36m(pid=12108)[0m  eval loss: 11.475318412780762, eval err: 0.8467983388900757
[2m[36m(pid=12108)[0m 8, 
[2m[36m(pid=12108)[0m  train loss: 8.754639835357667
[2m[36m(pid=12108)[0m  eval loss: 9.606905822753907, eval err: 0.814479455947876
[2m[36m(pid=12108)[0m 9, 
[2m[36m(pid=12108)[0m  train loss: 8.10498556137085
[2m[36m(pid=12108)[0m  eval loss: 9.575780601501465, eval err: 0.7991686058044434
Result for train_2e5f0ed2:
  date: 2021-09-11_07-56-21
  done: false
  err: 0.7991686058044434
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 9.575780601501465
  loss_train: 8.10498556137085
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 1298.4740674495697
  time_this_iter_s: 387.3139576911926
  time_total_s: 1298.4740674495697
  timestamp: 1631339781
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |      4 |          1298.47 |  9.57578  | 0.799169  |      8.10499 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 10, 
[2m[36m(pid=12108)[0m  train loss: 8.015595436096191
[2m[36m(pid=12108)[0m  eval loss: 7.921243629455566, eval err: 0.7289282608032227
[2m[36m(pid=12108)[0m 11, 
[2m[36m(pid=12108)[0m  train loss: 5.537931108474732
[2m[36m(pid=12108)[0m  eval loss: 4.682318239212036, eval err: 0.4442224431037903
[2m[36m(pid=12108)[0m 12, 
[2m[36m(pid=12108)[0m  train loss: 3.685619773864746
Result for train_2e5f0ed2:
  date: 2021-09-11_08-02-49
  done: false
  err: 0.26974225759506226
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 2.577561297416687
  loss_train: 3.685619773864746
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 1686.3710670471191
  time_this_iter_s: 387.89699959754944
  time_total_s: 1686.3710670471191
  timestamp: 1631340169
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 2e5f0ed2
  
[2m[36m(pid=12108)[0m  eval loss: 2.577561297416687, eval err: 0.26974225759506226
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |      5 |          1686.37 |  2.57756  | 0.269742  |      3.68562 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 13, 
[2m[36m(pid=12108)[0m  train loss: 3.403673849105835
[2m[36m(pid=12108)[0m  eval loss: 2.1326872396469114, eval err: 0.21776027202606202
[2m[36m(pid=12108)[0m 14, 
[2m[36m(pid=12108)[0m  train loss: 2.6332604265213013
[2m[36m(pid=12108)[0m  eval loss: 1.6158240199089051, eval err: 0.1555200743675232
[2m[36m(pid=12108)[0m 15, 
[2m[36m(pid=12108)[0m  train loss: 2.077809133529663
[2m[36m(pid=12108)[0m  eval loss: 1.376084475517273, eval err: 0.12012229442596435
Result for train_2e5f0ed2:
  date: 2021-09-11_08-09-14
  done: false
  err: 0.12012229442596435
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 1.376084475517273
  loss_train: 2.077809133529663
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 2071.335856437683
  time_this_iter_s: 384.96478939056396
  time_total_s: 2071.335856437683
  timestamp: 1631340554
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |      6 |          2071.34 |  1.37608  | 0.120122  |      2.07781 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 16, 
[2m[36m(pid=12108)[0m  train loss: 1.8149875783920288
[2m[36m(pid=12108)[0m  eval loss: 1.475042107105255, eval err: 0.12907541990280152
[2m[36m(pid=12108)[0m 17, 
[2m[36m(pid=12108)[0m  train loss: 1.652474150657654
[2m[36m(pid=12108)[0m  eval loss: 1.2219515752792358, eval err: 0.10270724058151245
[2m[36m(pid=12108)[0m 18, 
[2m[36m(pid=12108)[0m  train loss: 1.6001457571983337
[2m[36m(pid=12108)[0m  eval loss: 1.1662334966659547, eval err: 0.0941299867630005
Result for train_2e5f0ed2:
  date: 2021-09-11_08-15-35
  done: false
  err: 0.0941299867630005
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 1.1662334966659547
  loss_train: 1.6001457571983337
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 2452.6783244609833
  time_this_iter_s: 381.34246802330017
  time_total_s: 2452.6783244609833
  timestamp: 1631340935
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |      7 |          2452.68 |  1.16623  | 0.09413   |      1.60015 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 19, 
[2m[36m(pid=12108)[0m  train loss: 1.6378561711311341
[2m[36m(pid=12108)[0m  eval loss: 1.1853953170776368, eval err: 0.09513389825820923
[2m[36m(pid=12108)[0m 20, 
[2m[36m(pid=12108)[0m  train loss: 1.466220932006836
[2m[36m(pid=12108)[0m  eval loss: 1.1924302005767822, eval err: 0.09640584945678711
[2m[36m(pid=12108)[0m 21, 
[2m[36m(pid=12108)[0m  train loss: 1.3350355792045594
[2m[36m(pid=12108)[0m  eval loss: 1.0459329688549042, eval err: 0.08211127758026122
Result for train_2e5f0ed2:
  date: 2021-09-11_08-21-56
  done: false
  err: 0.08211127758026122
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 1.0459329688549042
  loss_train: 1.3350355792045594
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 2833.857031106949
  time_this_iter_s: 381.1787066459656
  time_total_s: 2833.857031106949
  timestamp: 1631341316
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |      8 |          2833.86 |  1.04593  | 0.0821113 |      1.33504 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 22, 
[2m[36m(pid=12108)[0m  train loss: 1.385161852836609
[2m[36m(pid=12108)[0m  eval loss: 1.0240605831146241, eval err: 0.07846089363098145
[2m[36m(pid=12108)[0m 23, 
[2m[36m(pid=12108)[0m  train loss: 1.3344519543647766
[2m[36m(pid=12108)[0m  eval loss: 0.9820068049430847, eval err: 0.07600975513458252
[2m[36m(pid=12108)[0m 24, 
[2m[36m(pid=12108)[0m  train loss: 1.7623356413841247
[2m[36m(pid=12108)[0m  eval loss: 1.7550572848320007, eval err: 0.12455140590667725
Result for train_2e5f0ed2:
  date: 2021-09-11_08-28-17
  done: false
  err: 0.12455140590667725
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 1.7550572848320007
  loss_train: 1.7623356413841247
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 3214.2643699645996
  time_this_iter_s: 380.40733885765076
  time_total_s: 3214.2643699645996
  timestamp: 1631341697
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |      9 |          3214.26 |  1.75506  | 0.124551  |      1.76234 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 25, 
[2m[36m(pid=12108)[0m  train loss: 1.6848815178871155
[2m[36m(pid=12108)[0m  eval loss: 1.1714098739624024, eval err: 0.09285558462142944
[2m[36m(pid=12108)[0m 26, 
[2m[36m(pid=12108)[0m  train loss: 1.4833054089546203
[2m[36m(pid=12108)[0m  eval loss: 1.0908257508277892, eval err: 0.08540462732315063
[2m[36m(pid=12108)[0m 27, 
[2m[36m(pid=12108)[0m  train loss: 1.512546854019165
[2m[36m(pid=12108)[0m  eval loss: 1.1407535123825072, eval err: 0.08770635604858398
Result for train_2e5f0ed2:
  date: 2021-09-11_08-34-36
  done: false
  err: 0.08770635604858398
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 1.1407535123825072
  loss_train: 1.512546854019165
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 3593.713314771652
  time_this_iter_s: 379.4489448070526
  time_total_s: 3593.713314771652
  timestamp: 1631342076
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     10 |          3593.71 |  1.14075  | 0.0877064 |      1.51255 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 28, 
[2m[36m(pid=12108)[0m  train loss: 1.3412427926063537
[2m[36m(pid=12108)[0m  eval loss: 1.000732146501541, eval err: 0.07709035396575928
[2m[36m(pid=12108)[0m 29, 
[2m[36m(pid=12108)[0m  train loss: 1.3746435260772705
[2m[36m(pid=12108)[0m  eval loss: 0.9709950304031372, eval err: 0.07156624555587769
[2m[36m(pid=12108)[0m 30, 
[2m[36m(pid=12108)[0m  train loss: 1.2966081070899964
Result for train_2e5f0ed2:
  date: 2021-09-11_08-40-55
  done: false
  err: 0.07300166845321655
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 0.9874282085895538
  loss_train: 1.2966081070899964
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 3972.4091300964355
  time_this_iter_s: 378.6958153247833
  time_total_s: 3972.4091300964355
  timestamp: 1631342455
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 2e5f0ed2
  
[2m[36m(pid=12108)[0m  eval loss: 0.9874282085895538, eval err: 0.07300166845321655
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     11 |          3972.41 |  0.987428 | 0.0730017 |      1.29661 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 31, 
[2m[36m(pid=12108)[0m  train loss: 1.28305611371994
[2m[36m(pid=12108)[0m  eval loss: 0.9625937986373901, eval err: 0.07081845998764039
[2m[36m(pid=12108)[0m 32, 
[2m[36m(pid=12108)[0m  train loss: 1.2348212552070619
[2m[36m(pid=12108)[0m  eval loss: 0.8904412400722503, eval err: 0.06580379247665405
[2m[36m(pid=12108)[0m 33, 
[2m[36m(pid=12108)[0m  train loss: 1.1672763419151306
[2m[36m(pid=12108)[0m  eval loss: 0.9224223685264588, eval err: 0.07006619930267334
Result for train_2e5f0ed2:
  date: 2021-09-11_08-47-13
  done: false
  err: 0.07006619930267334
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 0.9224223685264588
  loss_train: 1.1672763419151306
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 4350.828590393066
  time_this_iter_s: 378.41946029663086
  time_total_s: 4350.828590393066
  timestamp: 1631342833
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     12 |          4350.83 |  0.922422 | 0.0700662 |      1.16728 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 34, 
[2m[36m(pid=12108)[0m  train loss: 1.0274581897258759
[2m[36m(pid=12108)[0m  eval loss: 0.8818194878101349, eval err: 0.06642970561981201
[2m[36m(pid=12108)[0m 35, 
[2m[36m(pid=12108)[0m  train loss: 1.197063043117523
[2m[36m(pid=12108)[0m  eval loss: 0.8758714425563813, eval err: 0.06360626459121704
[2m[36m(pid=12108)[0m 36, 
[2m[36m(pid=12108)[0m  train loss: 1.1648713636398316
[2m[36m(pid=12108)[0m  eval loss: 0.9276014220714569, eval err: 0.06835288047790528
Result for train_2e5f0ed2:
  date: 2021-09-11_08-53-32
  done: false
  err: 0.06835288047790528
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 0.9276014220714569
  loss_train: 1.1648713636398316
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 4729.570677042007
  time_this_iter_s: 378.74208664894104
  time_total_s: 4729.570677042007
  timestamp: 1631343212
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     13 |          4729.57 |  0.927601 | 0.0683529 |      1.16487 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 37, 
[2m[36m(pid=12108)[0m  train loss: 1.155835165977478
[2m[36m(pid=12108)[0m  eval loss: 0.8924030208587647, eval err: 0.06452255249023438
[2m[36m(pid=12108)[0m 38, 
[2m[36m(pid=12108)[0m  train loss: 1.0785355615615844
[2m[36m(pid=12108)[0m  eval loss: 0.8397684788703919, eval err: 0.06182938098907471
[2m[36m(pid=12108)[0m 39, 
[2m[36m(pid=12108)[0m  train loss: 1.2450968384742738
[2m[36m(pid=12108)[0m  eval loss: 0.9517468464374542, eval err: 0.0699588942527771
Result for train_2e5f0ed2:
  date: 2021-09-11_08-59-51
  done: false
  err: 0.0699588942527771
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 0.9517468464374542
  loss_train: 1.2450968384742738
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 5108.116154432297
  time_this_iter_s: 378.5454773902893
  time_total_s: 5108.116154432297
  timestamp: 1631343591
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     14 |          5108.12 |  0.951747 | 0.0699589 |      1.2451  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 40, 
[2m[36m(pid=12108)[0m  train loss: 1.305423867702484
[2m[36m(pid=12108)[0m  eval loss: 0.8985952258110046, eval err: 0.06543844699859619
[2m[36m(pid=12108)[0m 41, 
[2m[36m(pid=12108)[0m  train loss: 1.221371443271637
[2m[36m(pid=12108)[0m  eval loss: 0.8613219678401947, eval err: 0.06275119066238403
[2m[36m(pid=12108)[0m 42, 
[2m[36m(pid=12108)[0m  train loss: 1.0810089540481567
Result for train_2e5f0ed2:
  date: 2021-09-11_09-06-09
  done: false
  err: 0.06279119491577148
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 0.8610079669952393
  loss_train: 1.0810089540481567
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 5486.113746166229
  time_this_iter_s: 377.9975917339325
  time_total_s: 5486.113746166229
  timestamp: 1631343969
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: 2e5f0ed2
  
[2m[36m(pid=12108)[0m  eval loss: 0.8610079669952393, eval err: 0.06279119491577148
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     15 |          5486.11 |  0.861008 | 0.0627912 |      1.08101 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 43, 
[2m[36m(pid=12108)[0m  train loss: 1.1157962596416473
[2m[36m(pid=12108)[0m  eval loss: 0.9691742968559265, eval err: 0.07253551006317138
[2m[36m(pid=12108)[0m 44, 
[2m[36m(pid=12108)[0m  train loss: 1.037736449241638
[2m[36m(pid=12108)[0m  eval loss: 0.8640624022483826, eval err: 0.058650987148284914
[2m[36m(pid=12108)[0m 45, 
[2m[36m(pid=12108)[0m  train loss: 0.936105444431305
[2m[36m(pid=12108)[0m  eval loss: 0.7970694720745086, eval err: 0.053715567588806155
Result for train_2e5f0ed2:
  date: 2021-09-11_09-12-26
  done: false
  err: 0.053715567588806155
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 0.7970694720745086
  loss_train: 0.936105444431305
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 5863.5502026081085
  time_this_iter_s: 377.4364564418793
  time_total_s: 5863.5502026081085
  timestamp: 1631344346
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     16 |          5863.55 |  0.797069 | 0.0537156 |     0.936105 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 46, 
[2m[36m(pid=12108)[0m  train loss: 1.0200884389877318
[2m[36m(pid=12108)[0m  eval loss: 0.8700171267986297, eval err: 0.05835422992706299
[2m[36m(pid=12108)[0m 47, 
[2m[36m(pid=12108)[0m  train loss: 0.9184322476387023
[2m[36m(pid=12108)[0m  eval loss: 0.8127246463298797, eval err: 0.05632108926773071
[2m[36m(pid=12108)[0m 48, 
[2m[36m(pid=12108)[0m  train loss: 0.9849039196968079
[2m[36m(pid=12108)[0m  eval loss: 0.8008336317539215, eval err: 0.05673506498336792
Result for train_2e5f0ed2:
  date: 2021-09-11_09-18-43
  done: false
  err: 0.05673506498336792
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 0.8008336317539215
  loss_train: 0.9849039196968079
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 6240.844222784042
  time_this_iter_s: 377.29402017593384
  time_total_s: 6240.844222784042
  timestamp: 1631344723
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     17 |          6240.84 |  0.800834 | 0.0567351 |     0.984904 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 49, 
[2m[36m(pid=12108)[0m  train loss: 0.890621372461319
[2m[36m(pid=12108)[0m  eval loss: 0.7757280719280243, eval err: 0.053781707286834714
[2m[36m(pid=12108)[0m 50, 
[2m[36m(pid=12108)[0m  train loss: 0.861996374130249
[2m[36m(pid=12108)[0m  eval loss: 0.7723469281196594, eval err: 0.052330727577209475
[2m[36m(pid=12108)[0m 51, 
[2m[36m(pid=12108)[0m  train loss: 0.9493584978580475
[2m[36m(pid=12108)[0m  eval loss: 0.8108984816074372, eval err: 0.056447229385375976
Result for train_2e5f0ed2:
  date: 2021-09-11_09-25-00
  done: false
  err: 0.056447229385375976
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 0.8108984816074372
  loss_train: 0.9493584978580475
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 6617.840869426727
  time_this_iter_s: 376.99664664268494
  time_total_s: 6617.840869426727
  timestamp: 1631345100
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     18 |          6617.84 |  0.810898 | 0.0564472 |     0.949358 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 52, 
[2m[36m(pid=12108)[0m  train loss: 1.0718822121620177
[2m[36m(pid=12108)[0m  eval loss: 0.7674805903434754, eval err: 0.05240380048751831
[2m[36m(pid=12108)[0m 53, 
[2m[36m(pid=12108)[0m  train loss: 1.0391471600532531
[2m[36m(pid=12108)[0m  eval loss: 0.9074214267730712, eval err: 0.060882117748260495
[2m[36m(pid=12108)[0m 54, 
[2m[36m(pid=12108)[0m  train loss: 1.1489422583580018
[2m[36m(pid=12108)[0m  eval loss: 0.9204774427413941, eval err: 0.06659254789352417
Result for train_2e5f0ed2:
  date: 2021-09-11_09-31-17
  done: false
  err: 0.06659254789352417
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 0.9204774427413941
  loss_train: 1.1489422583580018
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 6994.533885002136
  time_this_iter_s: 376.69301557540894
  time_total_s: 6994.533885002136
  timestamp: 1631345477
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     19 |          6994.53 |  0.920477 | 0.0665925 |      1.14894 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 55, 
[2m[36m(pid=12108)[0m  train loss: 1.4003806900978089
[2m[36m(pid=12108)[0m  eval loss: 1.0021230900287628, eval err: 0.06822985887527466
[2m[36m(pid=12108)[0m 56, 
[2m[36m(pid=12108)[0m  train loss: 1.1613182139396667
[2m[36m(pid=12108)[0m  eval loss: 0.9267009401321411, eval err: 0.06672559022903442
[2m[36m(pid=12108)[0m 57, 
[2m[36m(pid=12108)[0m  train loss: 1.1736624670028686
[2m[36m(pid=12108)[0m  eval loss: 0.8129860842227936, eval err: 0.05798025369644165
Result for train_2e5f0ed2:
  date: 2021-09-11_09-37-34
  done: false
  err: 0.05798025369644165
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 0.8129860842227936
  loss_train: 1.1736624670028686
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 7371.392657995224
  time_this_iter_s: 376.85877299308777
  time_total_s: 7371.392657995224
  timestamp: 1631345854
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     20 |          7371.39 |  0.812986 | 0.0579803 |      1.17366 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 58, 
[2m[36m(pid=12108)[0m  train loss: 1.0476999473571778
[2m[36m(pid=12108)[0m  eval loss: 0.8696530890464783, eval err: 0.05990567684173584
[2m[36m(pid=12108)[0m 59, 
[2m[36m(pid=12108)[0m  train loss: 0.9853365182876587
[2m[36m(pid=12108)[0m  eval loss: 0.8120683670043946, eval err: 0.055046162605285644
[2m[36m(pid=12108)[0m 60, 
[2m[36m(pid=12108)[0m  train loss: 0.8673619616031647
[2m[36m(pid=12108)[0m  eval loss: 0.7696469867229462, eval err: 0.05216516733169556
Result for train_2e5f0ed2:
  date: 2021-09-11_09-43-50
  done: false
  err: 0.05216516733169556
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 0.7696469867229462
  loss_train: 0.8673619616031647
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 7747.78617477417
  time_this_iter_s: 376.3935167789459
  time_total_s: 7747.78617477417
  timestamp: 1631346230
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     21 |          7747.79 |  0.769647 | 0.0521652 |     0.867362 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 61, 
[2m[36m(pid=12108)[0m  train loss: 0.8776390218734741
[2m[36m(pid=12108)[0m  eval loss: 0.7560957443714141, eval err: 0.05185967445373535
[2m[36m(pid=12108)[0m 62, 
[2m[36m(pid=12108)[0m  train loss: 0.8289288091659546
[2m[36m(pid=12108)[0m  eval loss: 0.744187343120575, eval err: 0.05104532718658447
[2m[36m(pid=12108)[0m 63, 
[2m[36m(pid=12108)[0m  train loss: 0.77408038854599
[2m[36m(pid=12108)[0m  eval loss: 0.7395670545101166, eval err: 0.050390162467956544
Result for train_2e5f0ed2:
  date: 2021-09-11_09-50-07
  done: false
  err: 0.050390162467956544
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 22
  loss: 0.7395670545101166
  loss_train: 0.77408038854599
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 8124.381769418716
  time_this_iter_s: 376.5955946445465
  time_total_s: 8124.381769418716
  timestamp: 1631346607
  timesteps_since_restore: 0
  training_iteration: 22
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     22 |          8124.38 |  0.739567 | 0.0503902 |      0.77408 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |      8.90473 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |      1.61665 |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |      7.25514 |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |      1.38691 |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |      1.29646 |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |      1.19104 |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |      1.29607 |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |      8.51764 |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 64, 
[2m[36m(pid=12108)[0m  train loss: 0.8457489311695099
[2m[36m(pid=12108)[0m  eval loss: 0.7284456300735473, eval err: 0.04944816589355469
[2m[36m(pid=12108)[0m 65, 
[2m[36m(pid=12108)[0m  train loss: 0.9087105536460877
[2m[36m(pid=12108)[0m  eval loss: 0.7950519406795502, eval err: 0.05146378993988037
[2m[36m(pid=12108)[0m 66, 
[2m[36m(pid=12108)[0m  train loss: 0.9556757378578186
[2m[36m(pid=12108)[0m  eval loss: 0.7348570096492767, eval err: 0.049438309669494626
Result for train_2e5f0ed2:
  date: 2021-09-11_09-56-24
  done: false
  err: 0.049438309669494626
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 23
  loss: 0.7348570096492767
  loss_train: 0.9556757378578186
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 8500.963210821152
  time_this_iter_s: 376.5814414024353
  time_total_s: 8500.963210821152
  timestamp: 1631346984
  timesteps_since_restore: 0
  training_iteration: 23
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     23 |          8500.96 |  0.734857 | 0.0494383 |     0.955676 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 67, 
[2m[36m(pid=12108)[0m  train loss: 0.925356764793396
[2m[36m(pid=12108)[0m  eval loss: 0.7495267701148987, eval err: 0.0503822135925293
[2m[36m(pid=12108)[0m 68, 
[2m[36m(pid=12108)[0m  train loss: 0.8869218301773071
[2m[36m(pid=12108)[0m  eval loss: 0.7244943952560425, eval err: 0.04816627979278564
[2m[36m(pid=12108)[0m 69, 
[2m[36m(pid=12108)[0m  train loss: 0.8981187689304352
Result for train_2e5f0ed2:
  date: 2021-09-11_10-02-40
  done: false
  err: 0.04790354490280151
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 24
  loss: 0.711877989768982
  loss_train: 0.8981187689304352
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 8877.671934604645
  time_this_iter_s: 376.70872378349304
  time_total_s: 8877.671934604645
  timestamp: 1631347360
  timesteps_since_restore: 0
  training_iteration: 24
  trial_id: 2e5f0ed2
  
[2m[36m(pid=12108)[0m  eval loss: 0.711877989768982, eval err: 0.04790354490280151
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     24 |          8877.67 |  0.711878 | 0.0479035 |     0.898119 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 70, 
[2m[36m(pid=12108)[0m  train loss: 0.7890277588367463
[2m[36m(pid=12108)[0m  eval loss: 0.7099995720386505, eval err: 0.04706034898757935
[2m[36m(pid=12108)[0m 71, 
[2m[36m(pid=12108)[0m  train loss: 0.7627226030826568
[2m[36m(pid=12108)[0m  eval loss: 0.707501676082611, eval err: 0.04706573009490967
[2m[36m(pid=12108)[0m 72, 
[2m[36m(pid=12108)[0m  train loss: 0.742966650724411
[2m[36m(pid=12108)[0m  eval loss: 0.7009885966777801, eval err: 0.046448147296905516
Result for train_2e5f0ed2:
  date: 2021-09-11_10-08-56
  done: false
  err: 0.046448147296905516
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 25
  loss: 0.7009885966777801
  loss_train: 0.742966650724411
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 9253.838433265686
  time_this_iter_s: 376.16649866104126
  time_total_s: 9253.838433265686
  timestamp: 1631347736
  timesteps_since_restore: 0
  training_iteration: 25
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     25 |          9253.84 |  0.700989 | 0.0464481 |     0.742967 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12108)[0m 73, 
[2m[36m(pid=12108)[0m  train loss: 0.8083029544353485
[2m[36m(pid=12108)[0m  eval loss: 0.7046510291099548, eval err: 0.04637338638305664
[2m[36m(pid=12108)[0m 74, 
[2m[36m(pid=12108)[0m  train loss: 0.7373366451263428
[2m[36m(pid=12108)[0m  eval loss: 0.702331097126007, eval err: 0.046340103149414065
[2m[36m(pid=12108)[0m 75, 
[2m[36m(pid=12108)[0m  train loss: 0.795758455991745
[2m[36m(pid=12108)[0m  eval loss: 0.6986542642116547, eval err: 0.04615626335144043
Result for train_2e5f0ed2:
  date: 2021-09-11_10-15-12
  done: true
  err: 0.04615626335144043
  experiment_id: d5a7906dd27041a0a0c6f3c7eae02b59
  hostname: bigcuda4
  iterations_since_restore: 26
  loss: 0.6986542642116547
  loss_train: 0.795758455991745
  node_ip: 131.220.7.54
  pid: 12108
  should_checkpoint: true
  time_since_restore: 9629.622884273529
  time_this_iter_s: 375.784451007843
  time_total_s: 9629.622884273529
  timestamp: 1631348112
  timesteps_since_restore: 0
  training_iteration: 26
  trial_id: 2e5f0ed2
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (5 PENDING, 1 RUNNING, 19 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f0ed2    | RUNNING    | 131.220.7.54:12108 |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_2e5f755c    | PENDING    |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m Epoch:
[2m[36m(pid=12120)[0m 0, 
[2m[36m(pid=12120)[0m  train loss: 28.109105224609376
[2m[36m(pid=12120)[0m  eval loss: 108.61810333251952, eval err: 0.9944203519821166
Result for train_2e5f755c:
  date: 2021-09-11_10-17-13
  done: false
  err: 0.9944203519821166
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 108.61810333251952
  loss_train: 28.109105224609376
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 118.69220638275146
  time_this_iter_s: 118.69220638275146
  time_total_s: 118.69220638275146
  timestamp: 1631348233
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |      1 |          118.692 | 108.618    | 0.99442   |    28.1091   |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |            |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |            |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |   9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |   0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |   0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |   8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |   0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |   0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |   0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |   0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  |  11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 1, 
[2m[36m(pid=12120)[0m  train loss: 16.589018630981446
[2m[36m(pid=12120)[0m  eval loss: 13.589723510742187, eval err: 0.8594176888465881
[2m[36m(pid=12120)[0m 2, 
[2m[36m(pid=12120)[0m  train loss: 11.575755558013917
[2m[36m(pid=12120)[0m  eval loss: 15.648618354797364, eval err: 0.8823372483253479
[2m[36m(pid=12120)[0m 3, 
[2m[36m(pid=12120)[0m  train loss: 11.420422744750976
[2m[36m(pid=12120)[0m  eval loss: 10.734371337890625, eval err: 0.8380279350280762
Result for train_2e5f755c:
  date: 2021-09-11_10-23-00
  done: false
  err: 0.8380279350280762
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 10.734371337890625
  loss_train: 11.420422744750976
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 466.2333641052246
  time_this_iter_s: 347.54115772247314
  time_total_s: 466.2333641052246
  timestamp: 1631348580
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |      2 |          466.233 | 10.7344   | 0.838028  |    11.4204   |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 4, 
[2m[36m(pid=12120)[0m  train loss: 10.85635757446289
[2m[36m(pid=12120)[0m  eval loss: 12.705379943847657, eval err: 0.8542661714553833
[2m[36m(pid=12120)[0m 5, 
[2m[36m(pid=12120)[0m  train loss: 10.097930850982666
[2m[36m(pid=12120)[0m  eval loss: 11.510621871948242, eval err: 0.8452773571014405
[2m[36m(pid=12120)[0m 6, 
[2m[36m(pid=12120)[0m  train loss: 9.930338363647461
[2m[36m(pid=12120)[0m  eval loss: 13.147554817199707, eval err: 0.8562546706199646
Result for train_2e5f755c:
  date: 2021-09-11_10-28-48
  done: false
  err: 0.8562546706199646
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 13.147554817199707
  loss_train: 9.930338363647461
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 813.6546628475189
  time_this_iter_s: 347.4212987422943
  time_total_s: 813.6546628475189
  timestamp: 1631348928
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |      3 |          813.655 | 13.1476   | 0.856255  |     9.93034  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 7, 
[2m[36m(pid=12120)[0m  train loss: 9.795242137908936
[2m[36m(pid=12120)[0m  eval loss: 9.932767028808593, eval err: 0.8220082688331604
[2m[36m(pid=12120)[0m 8, 
[2m[36m(pid=12120)[0m  train loss: 9.79401990890503
[2m[36m(pid=12120)[0m  eval loss: 11.399119529724121, eval err: 0.8343130302429199
[2m[36m(pid=12120)[0m 9, 
[2m[36m(pid=12120)[0m  train loss: 9.980657596588134
[2m[36m(pid=12120)[0m  eval loss: 12.117116432189942, eval err: 0.8514008235931396
Result for train_2e5f755c:
  date: 2021-09-11_10-34-35
  done: false
  err: 0.8514008235931396
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 12.117116432189942
  loss_train: 9.980657596588134
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 1161.1287853717804
  time_this_iter_s: 347.4741225242615
  time_total_s: 1161.1287853717804
  timestamp: 1631349275
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |      4 |          1161.13 | 12.1171   | 0.851401  |     9.98066  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 10, 
[2m[36m(pid=12120)[0m  train loss: 9.931221561431885
[2m[36m(pid=12120)[0m  eval loss: 11.65720142364502, eval err: 0.8498356890678406
[2m[36m(pid=12120)[0m 11, 
[2m[36m(pid=12120)[0m  train loss: 9.127389717102051
[2m[36m(pid=12120)[0m  eval loss: 10.58658359527588, eval err: 0.8323161602020264
[2m[36m(pid=12120)[0m 12, 
[2m[36m(pid=12120)[0m  train loss: 9.401089687347412
[2m[36m(pid=12120)[0m  eval loss: 8.817807235717773, eval err: 0.7869145488739013
Result for train_2e5f755c:
  date: 2021-09-11_10-40-23
  done: false
  err: 0.7869145488739013
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 8.817807235717773
  loss_train: 9.401089687347412
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 1508.7090237140656
  time_this_iter_s: 347.58023834228516
  time_total_s: 1508.7090237140656
  timestamp: 1631349623
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 11.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |      5 |          1508.71 |  8.81781  | 0.786915  |     9.40109  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 13, 
[2m[36m(pid=12120)[0m  train loss: 9.074840717315674
[2m[36m(pid=12120)[0m  eval loss: 9.111800575256348, eval err: 0.8118524384498597
[2m[36m(pid=12120)[0m 14, 
[2m[36m(pid=12120)[0m  train loss: 8.78587766647339
[2m[36m(pid=12120)[0m  eval loss: 11.959347839355468, eval err: 0.8502399539947509
[2m[36m(pid=12120)[0m 15, 
[2m[36m(pid=12120)[0m  train loss: 9.230770874023438
[2m[36m(pid=12120)[0m  eval loss: 11.666572456359864, eval err: 0.839860680103302
Result for train_2e5f755c:
  date: 2021-09-11_10-46-10
  done: false
  err: 0.839860680103302
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 11.666572456359864
  loss_train: 9.230770874023438
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 1856.2226090431213
  time_this_iter_s: 347.5135853290558
  time_total_s: 1856.2226090431213
  timestamp: 1631349970
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |      6 |          1856.22 | 11.6666   | 0.839861  |     9.23077  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 16, 
[2m[36m(pid=12120)[0m  train loss: 8.669577560424806
[2m[36m(pid=12120)[0m  eval loss: 8.958627338409423, eval err: 0.7851496982574463
[2m[36m(pid=12120)[0m 17, 
[2m[36m(pid=12120)[0m  train loss: 8.940002365112305
[2m[36m(pid=12120)[0m  eval loss: 10.689526176452636, eval err: 0.8333710265159607
[2m[36m(pid=12120)[0m 18, 
[2m[36m(pid=12120)[0m  train loss: 8.537123966217042
[2m[36m(pid=12120)[0m  eval loss: 10.081085700988769, eval err: 0.8230895924568177
Result for train_2e5f755c:
  date: 2021-09-11_10-51-58
  done: false
  err: 0.8230895924568177
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 10.081085700988769
  loss_train: 8.537123966217042
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 2203.880005836487
  time_this_iter_s: 347.6573967933655
  time_total_s: 2203.880005836487
  timestamp: 1631350318
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |      7 |          2203.88 | 10.0811   | 0.82309   |     8.53712  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 19, 
[2m[36m(pid=12120)[0m  train loss: 8.343259181976318
[2m[36m(pid=12120)[0m  eval loss: 11.018194198608398, eval err: 0.8258144092559815
[2m[36m(pid=12120)[0m 20, 
[2m[36m(pid=12120)[0m  train loss: 9.042884120941162
[2m[36m(pid=12120)[0m  eval loss: 9.960337181091308, eval err: 0.8158105611801147
[2m[36m(pid=12120)[0m 21, 
[2m[36m(pid=12120)[0m  train loss: 7.86328010559082
[2m[36m(pid=12120)[0m  eval loss: 8.731474075317383, eval err: 0.8147927594184875
Result for train_2e5f755c:
  date: 2021-09-11_10-57-45
  done: false
  err: 0.8147927594184875
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 8.731474075317383
  loss_train: 7.86328010559082
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 2551.2976334095
  time_this_iter_s: 347.4176275730133
  time_total_s: 2551.2976334095
  timestamp: 1631350665
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |      8 |          2551.3  |  8.73147  | 0.814793  |     7.86328  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 22, 
[2m[36m(pid=12120)[0m  train loss: 8.13397066116333
[2m[36m(pid=12120)[0m  eval loss: 9.107125129699707, eval err: 0.8186434006690979
[2m[36m(pid=12120)[0m 23, 
[2m[36m(pid=12120)[0m  train loss: 9.012887325286865
[2m[36m(pid=12120)[0m  eval loss: 9.292401866912842, eval err: 0.8187077450752258
[2m[36m(pid=12120)[0m 24, 
[2m[36m(pid=12120)[0m  train loss: 7.2636626434326175
[2m[36m(pid=12120)[0m  eval loss: 10.142652816772461, eval err: 0.8171350908279419
Result for train_2e5f755c:
  date: 2021-09-11_11-03-33
  done: false
  err: 0.8171350908279419
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 10.142652816772461
  loss_train: 7.2636626434326175
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 2898.6907935142517
  time_this_iter_s: 347.3931601047516
  time_total_s: 2898.6907935142517
  timestamp: 1631351013
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |      9 |          2898.69 | 10.1427   | 0.817135  |     7.26366  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 25, 
[2m[36m(pid=12120)[0m  train loss: 8.790137901306153
[2m[36m(pid=12120)[0m  eval loss: 8.78512559890747, eval err: 0.7945869946479798
[2m[36m(pid=12120)[0m 26, 
[2m[36m(pid=12120)[0m  train loss: 8.579905986785889
[2m[36m(pid=12120)[0m  eval loss: 10.159018478393556, eval err: 0.8208500242233276
[2m[36m(pid=12120)[0m 27, 
[2m[36m(pid=12120)[0m  train loss: 7.964180126190185
[2m[36m(pid=12120)[0m  eval loss: 9.162887744903564, eval err: 0.7898109674453735
Result for train_2e5f755c:
  date: 2021-09-11_11-09-20
  done: false
  err: 0.7898109674453735
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 9.162887744903564
  loss_train: 7.964180126190185
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 3246.0407655239105
  time_this_iter_s: 347.3499720096588
  time_total_s: 3246.0407655239105
  timestamp: 1631351360
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     10 |          3246.04 |  9.16289  | 0.789811  |     7.96418  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 28, 
[2m[36m(pid=12120)[0m  train loss: 7.590900936126709
[2m[36m(pid=12120)[0m  eval loss: 8.844216194152832, eval err: 0.7586287569999695
[2m[36m(pid=12120)[0m 29, 
[2m[36m(pid=12120)[0m  train loss: 7.249284934997559
[2m[36m(pid=12120)[0m  eval loss: 10.424756965637208, eval err: 0.7882247638702392
[2m[36m(pid=12120)[0m 30, 
[2m[36m(pid=12120)[0m  train loss: 7.583231029510498
[2m[36m(pid=12120)[0m  eval loss: 7.639378700256348, eval err: 0.7369472479820252
Result for train_2e5f755c:
  date: 2021-09-11_11-15-07
  done: false
  err: 0.7369472479820252
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 7.639378700256348
  loss_train: 7.583231029510498
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 3593.187477827072
  time_this_iter_s: 347.1467123031616
  time_total_s: 3593.187477827072
  timestamp: 1631351707
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     11 |          3593.19 |  7.63938  | 0.736947  |     7.58323  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 31, 
[2m[36m(pid=12120)[0m  train loss: 7.127874813079834
[2m[36m(pid=12120)[0m  eval loss: 7.912414836883545, eval err: 0.7441616153717041
[2m[36m(pid=12120)[0m 32, 
[2m[36m(pid=12120)[0m  train loss: 7.289146499633789
[2m[36m(pid=12120)[0m  eval loss: 7.435495929718018, eval err: 0.7204269599914551
[2m[36m(pid=12120)[0m 33, 
[2m[36m(pid=12120)[0m  train loss: 6.47941162109375
[2m[36m(pid=12120)[0m  eval loss: 5.934621963500977, eval err: 0.6421696805953979
Result for train_2e5f755c:
  date: 2021-09-11_11-20-54
  done: false
  err: 0.6421696805953979
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 5.934621963500977
  loss_train: 6.47941162109375
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 3940.42666387558
  time_this_iter_s: 347.2391860485077
  time_total_s: 3940.42666387558
  timestamp: 1631352054
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     12 |          3940.43 |  5.93462  | 0.64217   |     6.47941  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 34, 
[2m[36m(pid=12120)[0m  train loss: 5.842753467559814
[2m[36m(pid=12120)[0m  eval loss: 5.851433849334716, eval err: 0.5650084280967712
[2m[36m(pid=12120)[0m 35, 
[2m[36m(pid=12120)[0m  train loss: 5.186162576675415
[2m[36m(pid=12120)[0m  eval loss: 4.544322786331176, eval err: 0.4959534978866577
[2m[36m(pid=12120)[0m 36, 
[2m[36m(pid=12120)[0m  train loss: 5.039040927886963
[2m[36m(pid=12120)[0m  eval loss: 4.021388711929322, eval err: 0.4098703503608704
Result for train_2e5f755c:
  date: 2021-09-11_11-26-42
  done: false
  err: 0.4098703503608704
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 4.021388711929322
  loss_train: 5.039040927886963
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 4288.31369137764
  time_this_iter_s: 347.88702750205994
  time_total_s: 4288.31369137764
  timestamp: 1631352402
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     13 |          4288.31 |  4.02139  | 0.40987   |     5.03904  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 37, 
[2m[36m(pid=12120)[0m  train loss: 4.191363306045532
[2m[36m(pid=12120)[0m  eval loss: 3.6989626121520995, eval err: 0.3549578356742859
[2m[36m(pid=12120)[0m 38, 
[2m[36m(pid=12120)[0m  train loss: 3.983491373062134
[2m[36m(pid=12120)[0m  eval loss: 2.3434095764160157, eval err: 0.24573403596878052
[2m[36m(pid=12120)[0m 39, 
[2m[36m(pid=12120)[0m  train loss: 3.31113489151001
[2m[36m(pid=12120)[0m  eval loss: 1.9352950620651246, eval err: 0.2000160598754883
Result for train_2e5f755c:
  date: 2021-09-11_11-32-29
  done: false
  err: 0.2000160598754883
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 1.9352950620651246
  loss_train: 3.31113489151001
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 4634.717859745026
  time_this_iter_s: 346.40416836738586
  time_total_s: 4634.717859745026
  timestamp: 1631352749
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     14 |          4634.72 |  1.9353   | 0.200016  |     3.31113  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 40, 
[2m[36m(pid=12120)[0m  train loss: 3.085273561477661
[2m[36m(pid=12120)[0m  eval loss: 1.7173417353630065, eval err: 0.16187078952789308
[2m[36m(pid=12120)[0m 41, 
[2m[36m(pid=12120)[0m  train loss: 2.9520677757263183
[2m[36m(pid=12120)[0m  eval loss: 1.6690973377227782, eval err: 0.15249334335327147
[2m[36m(pid=12120)[0m 42, 
[2m[36m(pid=12120)[0m  train loss: 2.666973519325256
[2m[36m(pid=12120)[0m  eval loss: 1.5473900103569032, eval err: 0.14173449516296388
Result for train_2e5f755c:
  date: 2021-09-11_11-38-12
  done: false
  err: 0.14173449516296388
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 1.5473900103569032
  loss_train: 2.666973519325256
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 4978.378529548645
  time_this_iter_s: 343.6606698036194
  time_total_s: 4978.378529548645
  timestamp: 1631353092
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     15 |          4978.38 |  1.54739  | 0.141734  |     2.66697  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 43, 
[2m[36m(pid=12120)[0m  train loss: 2.523172125816345
[2m[36m(pid=12120)[0m  eval loss: 1.3479674863815307, eval err: 0.11291038036346436
[2m[36m(pid=12120)[0m 44, 
[2m[36m(pid=12120)[0m  train loss: 2.2583492279052733
[2m[36m(pid=12120)[0m  eval loss: 1.4424901795387268, eval err: 0.12365219831466674
[2m[36m(pid=12120)[0m 45, 
[2m[36m(pid=12120)[0m  train loss: 2.02401123046875
[2m[36m(pid=12120)[0m  eval loss: 1.2711431241035462, eval err: 0.10494710922241211
Result for train_2e5f755c:
  date: 2021-09-11_11-43-54
  done: false
  err: 0.10494710922241211
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 1.2711431241035462
  loss_train: 2.02401123046875
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 5320.127073526382
  time_this_iter_s: 341.7485439777374
  time_total_s: 5320.127073526382
  timestamp: 1631353434
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     16 |          5320.13 |  1.27114  | 0.104947  |     2.02401  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 46, 
[2m[36m(pid=12120)[0m  train loss: 2.046285243034363
[2m[36m(pid=12120)[0m  eval loss: 1.3274855756759643, eval err: 0.10864965438842773
[2m[36m(pid=12120)[0m 47, 
[2m[36m(pid=12120)[0m  train loss: 2.0994860458374025
[2m[36m(pid=12120)[0m  eval loss: 1.2952119421958923, eval err: 0.10767760038375855
[2m[36m(pid=12120)[0m 48, 
[2m[36m(pid=12120)[0m  train loss: 1.8877687025070191
[2m[36m(pid=12120)[0m  eval loss: 1.1966801476478577, eval err: 0.09231111049652099
Result for train_2e5f755c:
  date: 2021-09-11_11-49-35
  done: false
  err: 0.09231111049652099
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 1.1966801476478577
  loss_train: 1.8877687025070191
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 5660.667589426041
  time_this_iter_s: 340.5405158996582
  time_total_s: 5660.667589426041
  timestamp: 1631353775
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     17 |          5660.67 |  1.19668  | 0.0923111 |     1.88777  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 49, 
[2m[36m(pid=12120)[0m  train loss: 1.9239209127426147
[2m[36m(pid=12120)[0m  eval loss: 1.196958839893341, eval err: 0.0932443881034851
[2m[36m(pid=12120)[0m 50, 
[2m[36m(pid=12120)[0m  train loss: 1.8714404106140137
[2m[36m(pid=12120)[0m  eval loss: 1.1417207026481628, eval err: 0.08682918548583984
[2m[36m(pid=12120)[0m 51, 
[2m[36m(pid=12120)[0m  train loss: 1.717283763885498
[2m[36m(pid=12120)[0m  eval loss: 1.1035473537445069, eval err: 0.08370789766311645
Result for train_2e5f755c:
  date: 2021-09-11_11-55-14
  done: false
  err: 0.08370789766311645
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 1.1035473537445069
  loss_train: 1.717283763885498
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 6000.377730607986
  time_this_iter_s: 339.7101411819458
  time_total_s: 6000.377730607986
  timestamp: 1631354114
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     18 |          6000.38 |  1.10355  | 0.0837079 |     1.71728  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 52, 
[2m[36m(pid=12120)[0m  train loss: 1.8994699645042419
[2m[36m(pid=12120)[0m  eval loss: 1.1135067439079285, eval err: 0.08488248348236084
[2m[36m(pid=12120)[0m 53, 
[2m[36m(pid=12120)[0m  train loss: 1.7689119577407837
[2m[36m(pid=12120)[0m  eval loss: 1.1453645133972168, eval err: 0.08876552581787109
[2m[36m(pid=12120)[0m 54, 
[2m[36m(pid=12120)[0m  train loss: 1.8945005178451537
[2m[36m(pid=12120)[0m  eval loss: 1.094225163459778, eval err: 0.0821340274810791
Result for train_2e5f755c:
  date: 2021-09-11_12-00-54
  done: false
  err: 0.0821340274810791
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 1.094225163459778
  loss_train: 1.8945005178451537
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 6339.76188993454
  time_this_iter_s: 339.38415932655334
  time_total_s: 6339.76188993454
  timestamp: 1631354454
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     19 |          6339.76 |  1.09423  | 0.082134  |     1.8945   |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 55, 
[2m[36m(pid=12120)[0m  train loss: 1.6525716066360474
[2m[36m(pid=12120)[0m  eval loss: 1.0739217376708985, eval err: 0.0818729591369629
[2m[36m(pid=12120)[0m 56, 
[2m[36m(pid=12120)[0m  train loss: 1.4254149770736695
[2m[36m(pid=12120)[0m  eval loss: 1.049975918531418, eval err: 0.0801315450668335
[2m[36m(pid=12120)[0m 57, 
[2m[36m(pid=12120)[0m  train loss: 1.5959691905975342
[2m[36m(pid=12120)[0m  eval loss: 1.0392484152317047, eval err: 0.07856677770614624
Result for train_2e5f755c:
  date: 2021-09-11_12-06-33
  done: false
  err: 0.07856677770614624
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 1.0392484152317047
  loss_train: 1.5959691905975342
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 6678.77410364151
  time_this_iter_s: 339.0122137069702
  time_total_s: 6678.77410364151
  timestamp: 1631354793
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 13.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     20 |          6678.77 |  1.03925  | 0.0785668 |     1.59597  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 58, 
[2m[36m(pid=12120)[0m  train loss: 1.5240637135505677
[2m[36m(pid=12120)[0m  eval loss: 1.0422914683818818, eval err: 0.07904277324676513
[2m[36m(pid=12120)[0m 59, 
[2m[36m(pid=12120)[0m  train loss: 1.4457303380966187
[2m[36m(pid=12120)[0m  eval loss: 1.0448611605167388, eval err: 0.07909432649612427
[2m[36m(pid=12120)[0m 60, 
[2m[36m(pid=12120)[0m  train loss: 1.5887860798835753
Result for train_2e5f755c:
  date: 2021-09-11_12-12-12
  done: false
  err: 0.07767611503601074
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 1.0318973016738893
  loss_train: 1.5887860798835753
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 7017.733148813248
  time_this_iter_s: 338.95904517173767
  time_total_s: 7017.733148813248
  timestamp: 1631355132
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: 2e5f755c
  
[2m[36m(pid=12120)[0m  eval loss: 1.0318973016738893, eval err: 0.07767611503601074
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     21 |          7017.73 |  1.0319   | 0.0776761 |     1.58879  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 61, 
[2m[36m(pid=12120)[0m  train loss: 1.6551141667366027
[2m[36m(pid=12120)[0m  eval loss: 1.0228728210926057, eval err: 0.07691754817962647
[2m[36m(pid=12120)[0m 62, 
[2m[36m(pid=12120)[0m  train loss: 1.5947870445251464
[2m[36m(pid=12120)[0m  eval loss: 1.038314127922058, eval err: 0.07834891557693481
[2m[36m(pid=12120)[0m 63, 
[2m[36m(pid=12120)[0m  train loss: 1.582645094394684
[2m[36m(pid=12120)[0m  eval loss: 1.024909120798111, eval err: 0.07674606323242188
Result for train_2e5f755c:
  date: 2021-09-11_12-17-50
  done: false
  err: 0.07674606323242188
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 22
  loss: 1.024909120798111
  loss_train: 1.582645094394684
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 7356.213618516922
  time_this_iter_s: 338.4804697036743
  time_total_s: 7356.213618516922
  timestamp: 1631355470
  timesteps_since_restore: 0
  training_iteration: 22
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     22 |          7356.21 |  1.02491  | 0.0767461 |     1.58265  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12120)[0m 64, 
[2m[36m(pid=12120)[0m  train loss: 1.5115307259559632
[2m[36m(pid=12120)[0m  eval loss: 1.0274642241001128, eval err: 0.07705051422119141
[2m[36m(pid=12120)[0m 65, 
[2m[36m(pid=12120)[0m  train loss: 1.4131202459335328
[2m[36m(pid=12120)[0m  eval loss: 1.0186744332313538, eval err: 0.0764134955406189
[2m[36m(pid=12120)[0m 66, 
[2m[36m(pid=12120)[0m  train loss: 1.5301105189323425
[2m[36m(pid=12120)[0m  eval loss: 1.0187834775447846, eval err: 0.07649935007095338
Result for train_2e5f755c:
  date: 2021-09-11_12-23-29
  done: true
  err: 0.07649935007095338
  experiment_id: 6020d1eed6f84aa29cc50531933d2183
  hostname: bigcuda4
  iterations_since_restore: 23
  loss: 1.0187834775447846
  loss_train: 1.5301105189323425
  node_ip: 131.220.7.54
  pid: 12120
  should_checkpoint: true
  time_since_restore: 7694.902175664902
  time_this_iter_s: 338.68855714797974
  time_total_s: 7694.902175664902
  timestamp: 1631355809
  timesteps_since_restore: 0
  training_iteration: 23
  trial_id: 2e5f755c
  
== Status ==
Memory usage on this node: 12.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (4 PENDING, 1 RUNNING, 20 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_2e5f755c    | RUNNING    | 131.220.7.54:12120 |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e37148    | PENDING    |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m Epoch:
[2m[36m(pid=12116)[0m 0, 
[2m[36m(pid=12116)[0m  train loss: 30.727475357055663
[2m[36m(pid=12116)[0m  eval loss: 92.37765167236329, eval err: 0.9863017797470093
Result for train_58e37148:
  date: 2021-09-11_12-25-30
  done: false
  err: 0.9863017797470093
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 92.37765167236329
  loss_train: 30.727475357055663
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 119.07194972038269
  time_this_iter_s: 119.07194972038269
  time_total_s: 119.07194972038269
  timestamp: 1631355930
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |      1 |          119.072 | 92.3777   | 0.986302  |    30.7275   |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 1, 
[2m[36m(pid=12116)[0m  train loss: 30.298255844116213
[2m[36m(pid=12116)[0m  eval loss: 99.12762969970703, eval err: 0.9914331555366516
[2m[36m(pid=12116)[0m 2, 
[2m[36m(pid=12116)[0m  train loss: 20.923303146362304
[2m[36m(pid=12116)[0m  eval loss: 46.718150024414065, eval err: 0.9630633950233459
[2m[36m(pid=12116)[0m 3, 
[2m[36m(pid=12116)[0m  train loss: 14.82024658203125
[2m[36m(pid=12116)[0m  eval loss: 22.35499137878418, eval err: 0.9097974133491517
Result for train_58e37148:
  date: 2021-09-11_12-31-19
  done: false
  err: 0.9097974133491517
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 22.35499137878418
  loss_train: 14.82024658203125
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 468.320924282074
  time_this_iter_s: 349.2489745616913
  time_total_s: 468.320924282074
  timestamp: 1631356279
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |      2 |          468.321 | 22.355    | 0.909797  |    14.8202   |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 4, 
[2m[36m(pid=12116)[0m  train loss: 12.218743095397949
[2m[36m(pid=12116)[0m  eval loss: 20.11111503601074, eval err: 0.9013578987121582
[2m[36m(pid=12116)[0m 5, 
[2m[36m(pid=12116)[0m  train loss: 10.874716567993165
[2m[36m(pid=12116)[0m  eval loss: 19.32011169433594, eval err: 0.8969116067886352
[2m[36m(pid=12116)[0m 6, 
[2m[36m(pid=12116)[0m  train loss: 10.822055320739747
[2m[36m(pid=12116)[0m  eval loss: 17.007503852844238, eval err: 0.8920617818832397
Result for train_58e37148:
  date: 2021-09-11_12-37-08
  done: false
  err: 0.8920617818832397
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 17.007503852844238
  loss_train: 10.822055320739747
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 817.3090424537659
  time_this_iter_s: 348.9881181716919
  time_total_s: 817.3090424537659
  timestamp: 1631356628
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |      3 |          817.309 | 17.0075   | 0.892062  |    10.8221   |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 7, 
[2m[36m(pid=12116)[0m  train loss: 10.249223861694336
[2m[36m(pid=12116)[0m  eval loss: 15.372678718566895, eval err: 0.8770032715797424
[2m[36m(pid=12116)[0m 8, 
[2m[36m(pid=12116)[0m  train loss: 10.407880859375
[2m[36m(pid=12116)[0m  eval loss: 13.09656364440918, eval err: 0.8551631665229797
[2m[36m(pid=12116)[0m 9, 
[2m[36m(pid=12116)[0m  train loss: 10.465247955322265
[2m[36m(pid=12116)[0m  eval loss: 14.073758735656739, eval err: 0.8693572068214417
Result for train_58e37148:
  date: 2021-09-11_12-42-57
  done: false
  err: 0.8693572068214417
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 14.073758735656739
  loss_train: 10.465247955322265
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 1166.1789858341217
  time_this_iter_s: 348.86994338035583
  time_total_s: 1166.1789858341217
  timestamp: 1631356977
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |      4 |          1166.18 | 14.0738   | 0.869357  |    10.4652   |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 10, 
[2m[36m(pid=12116)[0m  train loss: 10.019494934082031
[2m[36m(pid=12116)[0m  eval loss: 12.937502746582032, eval err: 0.8605301451683044
[2m[36m(pid=12116)[0m 11, 
[2m[36m(pid=12116)[0m  train loss: 9.902806968688965
[2m[36m(pid=12116)[0m  eval loss: 13.394343643188476, eval err: 0.8663147425651551
[2m[36m(pid=12116)[0m 12, 
[2m[36m(pid=12116)[0m  train loss: 9.935411815643311
[2m[36m(pid=12116)[0m  eval loss: 12.22839557647705, eval err: 0.8570044064521789
Result for train_58e37148:
  date: 2021-09-11_12-48-46
  done: false
  err: 0.8570044064521789
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 12.22839557647705
  loss_train: 9.935411815643311
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 1515.157103061676
  time_this_iter_s: 348.9781172275543
  time_total_s: 1515.157103061676
  timestamp: 1631357326
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |      5 |          1515.16 | 12.2284   | 0.857004  |     9.93541  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 13, 
[2m[36m(pid=12116)[0m  train loss: 9.610097942352295
[2m[36m(pid=12116)[0m  eval loss: 11.296374130249024, eval err: 0.846587347984314
[2m[36m(pid=12116)[0m 14, 
[2m[36m(pid=12116)[0m  train loss: 10.083902740478516
[2m[36m(pid=12116)[0m  eval loss: 11.45841854095459, eval err: 0.8563462352752685
[2m[36m(pid=12116)[0m 15, 
[2m[36m(pid=12116)[0m  train loss: 9.62940029144287
[2m[36m(pid=12116)[0m  eval loss: 12.550260467529297, eval err: 0.862764196395874
Result for train_58e37148:
  date: 2021-09-11_12-54-34
  done: false
  err: 0.862764196395874
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 12.550260467529297
  loss_train: 9.62940029144287
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 1863.422781944275
  time_this_iter_s: 348.2656788825989
  time_total_s: 1863.422781944275
  timestamp: 1631357674
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |      6 |          1863.42 | 12.5503   | 0.862764  |     9.6294   |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 16, 
[2m[36m(pid=12116)[0m  train loss: 10.069332180023194
[2m[36m(pid=12116)[0m  eval loss: 11.679514999389648, eval err: 0.8567685484886169
[2m[36m(pid=12116)[0m 17, 
[2m[36m(pid=12116)[0m  train loss: 9.378954410552979
[2m[36m(pid=12116)[0m  eval loss: 12.18511173248291, eval err: 0.8644485330581665
[2m[36m(pid=12116)[0m 18, 
[2m[36m(pid=12116)[0m  train loss: 9.21328556060791
[2m[36m(pid=12116)[0m  eval loss: 11.245993194580079, eval err: 0.8515952014923096
Result for train_58e37148:
  date: 2021-09-11_13-00-22
  done: false
  err: 0.8515952014923096
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 11.245993194580079
  loss_train: 9.21328556060791
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 2211.2676260471344
  time_this_iter_s: 347.8448441028595
  time_total_s: 2211.2676260471344
  timestamp: 1631358022
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |      7 |          2211.27 | 11.246    | 0.851595  |     9.21329  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 19, 
[2m[36m(pid=12116)[0m  train loss: 9.059132404327393
[2m[36m(pid=12116)[0m  eval loss: 10.843233642578125, eval err: 0.8403201532363892
[2m[36m(pid=12116)[0m 20, 
[2m[36m(pid=12116)[0m  train loss: 9.412067546844483
[2m[36m(pid=12116)[0m  eval loss: 10.803457527160644, eval err: 0.8512577676773071
[2m[36m(pid=12116)[0m 21, 
[2m[36m(pid=12116)[0m  train loss: 9.967298488616944
[2m[36m(pid=12116)[0m  eval loss: 10.294005355834962, eval err: 0.8421644330024719
Result for train_58e37148:
  date: 2021-09-11_13-06-11
  done: false
  err: 0.8421644330024719
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 10.294005355834962
  loss_train: 9.967298488616944
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 2559.9614355564117
  time_this_iter_s: 348.69380950927734
  time_total_s: 2559.9614355564117
  timestamp: 1631358371
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 13.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |      8 |          2559.96 | 10.294    | 0.842164  |     9.9673   |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 22, 
[2m[36m(pid=12116)[0m  train loss: 8.989720096588135
[2m[36m(pid=12116)[0m  eval loss: 10.485199127197266, eval err: 0.8397184991836548
[2m[36m(pid=12116)[0m 23, 
[2m[36m(pid=12116)[0m  train loss: 8.883531341552734
[2m[36m(pid=12116)[0m  eval loss: 10.804130554199219, eval err: 0.834867672920227
[2m[36m(pid=12116)[0m 24, 
[2m[36m(pid=12116)[0m  train loss: 9.190714511871338
[2m[36m(pid=12116)[0m  eval loss: 10.507049560546875, eval err: 0.8429178428649903
Result for train_58e37148:
  date: 2021-09-11_13-11-59
  done: false
  err: 0.8429178428649903
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 10.507049560546875
  loss_train: 9.190714511871338
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 2907.9393060207367
  time_this_iter_s: 347.97787046432495
  time_total_s: 2907.9393060207367
  timestamp: 1631358719
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |      9 |          2907.94 | 10.507    | 0.842918  |     9.19071  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 25, 
[2m[36m(pid=12116)[0m  train loss: 8.906747417449951
[2m[36m(pid=12116)[0m  eval loss: 10.098362369537353, eval err: 0.8251980090141297
[2m[36m(pid=12116)[0m 26, 
[2m[36m(pid=12116)[0m  train loss: 8.84832124710083
[2m[36m(pid=12116)[0m  eval loss: 10.099992084503175, eval err: 0.8315664124488831
[2m[36m(pid=12116)[0m 27, 
[2m[36m(pid=12116)[0m  train loss: 9.207150230407715
[2m[36m(pid=12116)[0m  eval loss: 10.44320213317871, eval err: 0.8431418943405151
Result for train_58e37148:
  date: 2021-09-11_13-17-46
  done: false
  err: 0.8431418943405151
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 10.44320213317871
  loss_train: 9.207150230407715
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 3255.636165380478
  time_this_iter_s: 347.6968593597412
  time_total_s: 3255.636165380478
  timestamp: 1631359066
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.9/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     10 |          3255.64 | 10.4432   | 0.843142  |     9.20715  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 28, 
[2m[36m(pid=12116)[0m  train loss: 9.613419265747071
[2m[36m(pid=12116)[0m  eval loss: 10.234158401489259, eval err: 0.8420188117027283
[2m[36m(pid=12116)[0m 29, 
[2m[36m(pid=12116)[0m  train loss: 8.940280055999756
[2m[36m(pid=12116)[0m  eval loss: 9.827127437591553, eval err: 0.8325895357131958
[2m[36m(pid=12116)[0m 30, 
[2m[36m(pid=12116)[0m  train loss: 8.748261108398438
[2m[36m(pid=12116)[0m  eval loss: 9.895055809020995, eval err: 0.8319839477539063
Result for train_58e37148:
  date: 2021-09-11_13-23-34
  done: false
  err: 0.8319839477539063
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 9.895055809020995
  loss_train: 8.748261108398438
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 3602.9905576705933
  time_this_iter_s: 347.35439229011536
  time_total_s: 3602.9905576705933
  timestamp: 1631359414
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     11 |          3602.99 |  9.89506  | 0.831984  |     8.74826  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 31, 
[2m[36m(pid=12116)[0m  train loss: 8.995340213775634
[2m[36m(pid=12116)[0m  eval loss: 9.940259857177734, eval err: 0.8299587917327881
[2m[36m(pid=12116)[0m 32, 
[2m[36m(pid=12116)[0m  train loss: 8.674965877532959
[2m[36m(pid=12116)[0m  eval loss: 9.900106048583984, eval err: 0.8294084358215332
[2m[36m(pid=12116)[0m 33, 
[2m[36m(pid=12116)[0m  train loss: 8.629562873840332
[2m[36m(pid=12116)[0m  eval loss: 9.41117021560669, eval err: 0.8216470575332642
Result for train_58e37148:
  date: 2021-09-11_13-29-21
  done: false
  err: 0.8216470575332642
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 9.41117021560669
  loss_train: 8.629562873840332
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 3950.3555133342743
  time_this_iter_s: 347.36495566368103
  time_total_s: 3950.3555133342743
  timestamp: 1631359761
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     12 |          3950.36 |  9.41117  | 0.821647  |     8.62956  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 34, 
[2m[36m(pid=12116)[0m  train loss: 9.37982780456543
[2m[36m(pid=12116)[0m  eval loss: 9.314449634552002, eval err: 0.818202350139618
[2m[36m(pid=12116)[0m 35, 
[2m[36m(pid=12116)[0m  train loss: 8.851734733581543
[2m[36m(pid=12116)[0m  eval loss: 10.073732528686524, eval err: 0.8340051293373107
[2m[36m(pid=12116)[0m 36, 
[2m[36m(pid=12116)[0m  train loss: 9.13939239501953
Result for train_58e37148:
  date: 2021-09-11_13-35-08
  done: false
  err: 0.8122293281555176
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 9.065275802612305
  loss_train: 9.13939239501953
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 4297.35270357132
  time_this_iter_s: 346.9971902370453
  time_total_s: 4297.35270357132
  timestamp: 1631360108
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: 58e37148
  
[2m[36m(pid=12116)[0m  eval loss: 9.065275802612305, eval err: 0.8122293281555176
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     13 |          4297.35 |  9.06528  | 0.812229  |     9.13939  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=12116)[0m 37, 
[2m[36m(pid=12116)[0m  train loss: 8.679075946807862
[2m[36m(pid=12116)[0m  eval loss: 9.000819454193115, eval err: 0.8131345510482788
[2m[36m(pid=12116)[0m 38, 
[2m[36m(pid=12116)[0m  train loss: 8.315623378753662
[2m[36m(pid=12116)[0m  eval loss: 9.555334606170653, eval err: 0.8240304470062256
[2m[36m(pid=12116)[0m 39, 
[2m[36m(pid=12116)[0m  train loss: 8.651171817779542
[2m[36m(pid=12116)[0m  eval loss: 9.436693878173829, eval err: 0.810500705242157
Result for train_58e37148:
  date: 2021-09-11_13-40-55
  done: true
  err: 0.810500705242157
  experiment_id: 031e4f24004e47ebaa7faf9c16e77024
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 9.436693878173829
  loss_train: 8.651171817779542
  node_ip: 131.220.7.54
  pid: 12116
  should_checkpoint: true
  time_since_restore: 4644.556721687317
  time_this_iter_s: 347.2040181159973
  time_total_s: 4644.556721687317
  timestamp: 1631360455
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: 58e37148
  
== Status ==
Memory usage on this node: 12.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 PENDING, 1 RUNNING, 21 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | RUNNING    | 131.220.7.54:12116 |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_2e5f5310    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | PENDING    |                    |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


Result for train_2e5f5310:
  {}
  
Result for train_58e3925e:
  {}
  
Result for train_58e3b3f6:
  {}
  
== Status ==
Memory usage on this node: 11.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/155.14 GiB heap, 0.0/70.48 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (3 ERROR, 22 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_b1c14_00024 | TERMINATED |       |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     24 |          6747.91 |  0.749863 | 0.0519572 |              |
| train_b1c14_00021 | TERMINATED |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_58e329d6    | TERMINATED |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_2e5fb72e    | TERMINATED |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5ff572    | TERMINATED |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_2e5fd682    | TERMINATED |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5f955a    | TERMINATED |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5ee57e    | TERMINATED |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f3312    | TERMINATED |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f0ed2    | TERMINATED |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f755c    | TERMINATED |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_58e37148    | TERMINATED |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_2e5f5310    | ERROR      |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_58e3925e    | ERROR      |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_58e3b3f6    | ERROR      |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
Number of errored trials: 3
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name     |   # failures | error file                                                                                                                                                                           |
|----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_2e5f5310 |            1 | /home/user/brank/ray_results/experiment_2/train_2e5f5310_9_batch_size=5,dropout_p=0.073942,layers=feat_heavy,lr=0.00051672,num_warmup=6.0,step_lr=64.0_2021-09-10_15-38-14/error.txt |
| train_58e3925e |            1 | /home/user/brank/ray_results/experiment_2/train_58e3925e_22_batch_size=5,dropout_p=0.25336,layers=feat_heavy,lr=4.2118e-05,num_warmup=7.0,step_lr=86.0_2021-09-11_13-40-55/error.txt |
| train_58e3b3f6 |            1 | /home/user/brank/ray_results/experiment_2/train_58e3b3f6_8_batch_size=5,dropout_p=0.10351,layers=cost_heavy,lr=1.3633e-05,num_warmup=8.0,step_lr=37.0_2021-09-11_13-40-57/error.txt  |
+----------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
=========================other errors occured, rerunning those experiments=============================
== Status ==
Memory usage on this node: 11.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_c876a87a    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m Epoch:
[2m[36m(pid=28873)[0m 0, 
== Status ==
Memory usage on this node: 13.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |        |                  |           |           |              |
| train_c876a87a    | PENDING    |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m  train loss: 30.2902734375
[2m[36m(pid=28873)[0m  eval loss: 116.10963470458984, eval err: 0.9990741848945618
Result for train_c87685ca:
  date: 2021-09-11_14-37-43
  done: false
  err: 0.9990741848945618
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 116.10963470458984
  loss_train: 30.2902734375
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 117.16380476951599
  time_this_iter_s: 117.16380476951599
  time_total_s: 117.16380476951599
  timestamp: 1631363863
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 13.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |       loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |      1 |          117.164 | 116.11     | 0.999074  |    30.2903   |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |            |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |            |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |   9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |   0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |   0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |   1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |   8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |   0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |   0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |   0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |   0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  |  11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |         4644.56  |   9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |   0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  |  14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  |  10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |   7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |   9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |   0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+------------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 1, 
[2m[36m(pid=28873)[0m  train loss: 26.757204666137696
[2m[36m(pid=28873)[0m  eval loss: 111.921533203125, eval err: 0.9979036521911621
[2m[36m(pid=28873)[0m 2, 
[2m[36m(pid=28873)[0m  train loss: 19.048967018127442
[2m[36m(pid=28873)[0m  eval loss: 85.69527954101562, eval err: 0.9982401084899902
[2m[36m(pid=28873)[0m 3, 
[2m[36m(pid=28873)[0m  train loss: 14.299168167114258
[2m[36m(pid=28873)[0m  eval loss: 31.112189636230468, eval err: 0.93590252161026
Result for train_c87685ca:
  date: 2021-09-11_14-43-38
  done: false
  err: 0.93590252161026
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 31.112189636230468
  loss_train: 14.299168167114258
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 471.675954580307
  time_this_iter_s: 354.512149810791
  time_total_s: 471.675954580307
  timestamp: 1631364218
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |      2 |          471.676 | 31.1122   | 0.935903  |    14.2992   |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |         4644.56  |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 4, 
[2m[36m(pid=28873)[0m  train loss: 12.782381553649902
[2m[36m(pid=28873)[0m  eval loss: 23.27335506439209, eval err: 0.9089908862113952
[2m[36m(pid=28873)[0m 5, 
[2m[36m(pid=28873)[0m  train loss: 12.585702857971192
[2m[36m(pid=28873)[0m  eval loss: 23.053472442626955, eval err: 0.9155969071388245
[2m[36m(pid=28873)[0m 6, 
[2m[36m(pid=28873)[0m  train loss: 11.720121002197265
Result for train_c87685ca:
  date: 2021-09-11_14-49-29
  done: false
  err: 0.9000221681594849
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 17.97964847564697
  loss_train: 11.720121002197265
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 823.3646528720856
  time_this_iter_s: 351.68869829177856
  time_total_s: 823.3646528720856
  timestamp: 1631364569
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: c87685ca
  
[2m[36m(pid=28873)[0m  eval loss: 17.97964847564697, eval err: 0.9000221681594849
== Status ==
Memory usage on this node: 11.8/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |      3 |          823.365 | 17.9796   | 0.900022  |    11.7201   |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |         4644.56  |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 7, 
[2m[36m(pid=28873)[0m  train loss: 11.10275447845459
[2m[36m(pid=28873)[0m  eval loss: 20.062771224975585, eval err: 0.9146002888679504
[2m[36m(pid=28873)[0m 8, 
[2m[36m(pid=28873)[0m  train loss: 12.015615844726563
[2m[36m(pid=28873)[0m  eval loss: 25.812067108154295, eval err: 0.9340755414962768
[2m[36m(pid=28873)[0m 9, 
[2m[36m(pid=28873)[0m  train loss: 11.22205410003662
[2m[36m(pid=28873)[0m  eval loss: 25.418199157714845, eval err: 0.928432228565216
Result for train_c87685ca:
  date: 2021-09-11_14-55-20
  done: false
  err: 0.928432228565216
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 25.418199157714845
  loss_train: 11.22205410003662
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 1173.800363779068
  time_this_iter_s: 350.4357109069824
  time_total_s: 1173.800363779068
  timestamp: 1631364920
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.7/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |      4 |          1173.8  | 25.4182   | 0.928432  |    11.2221   |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 10, 
[2m[36m(pid=28873)[0m  train loss: 10.42768762588501
[2m[36m(pid=28873)[0m  eval loss: 33.85257209777832, eval err: 0.9492233777046204
[2m[36m(pid=28873)[0m 11, 
[2m[36m(pid=28873)[0m  train loss: 10.976510734558106
[2m[36m(pid=28873)[0m  eval loss: 39.9589193725586, eval err: 0.9789325332641602
[2m[36m(pid=28873)[0m 12, 
[2m[36m(pid=28873)[0m  train loss: 10.484112167358399
[2m[36m(pid=28873)[0m  eval loss: 30.437819213867186, eval err: 0.937056610584259
Result for train_c87685ca:
  date: 2021-09-11_15-01-12
  done: false
  err: 0.937056610584259
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 30.437819213867186
  loss_train: 10.484112167358399
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 1526.1044373512268
  time_this_iter_s: 352.3040735721588
  time_total_s: 1526.1044373512268
  timestamp: 1631365272
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.6/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |      5 |          1526.1  | 30.4378   | 0.937057  |    10.4841   |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 13, 
[2m[36m(pid=28873)[0m  train loss: 11.09689136505127
[2m[36m(pid=28873)[0m  eval loss: 22.44583366394043, eval err: 0.9160668063163757
[2m[36m(pid=28873)[0m 14, 
[2m[36m(pid=28873)[0m  train loss: 10.323983993530273
[2m[36m(pid=28873)[0m  eval loss: 18.76008316040039, eval err: 0.9080816006660462
[2m[36m(pid=28873)[0m 15, 
[2m[36m(pid=28873)[0m  train loss: 9.931179466247558
Result for train_c87685ca:
  date: 2021-09-11_15-07-04
  done: false
  err: 0.904518392086029
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 17.866234436035157
  loss_train: 9.931179466247558
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 1877.901303768158
  time_this_iter_s: 351.79686641693115
  time_total_s: 1877.901303768158
  timestamp: 1631365624
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: c87685ca
  
[2m[36m(pid=28873)[0m  eval loss: 17.866234436035157, eval err: 0.904518392086029
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |      6 |          1877.9  | 17.8662   | 0.904518  |     9.93118  |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 16, 
[2m[36m(pid=28873)[0m  train loss: 11.000640602111817
[2m[36m(pid=28873)[0m  eval loss: 15.740010948181153, eval err: 0.8899903416633606
[2m[36m(pid=28873)[0m 17, 
[2m[36m(pid=28873)[0m  train loss: 10.545815982818603
[2m[36m(pid=28873)[0m  eval loss: 16.690862503051758, eval err: 0.88811772108078
[2m[36m(pid=28873)[0m 18, 
[2m[36m(pid=28873)[0m  train loss: 10.246957664489747
[2m[36m(pid=28873)[0m  eval loss: 19.60191722869873, eval err: 0.9142793607711792
Result for train_c87685ca:
  date: 2021-09-11_15-12-55
  done: false
  err: 0.9142793607711792
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 19.60191722869873
  loss_train: 10.246957664489747
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 2228.9802782535553
  time_this_iter_s: 351.07897448539734
  time_total_s: 2228.9802782535553
  timestamp: 1631365975
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |      7 |          2228.98 | 19.6019   | 0.914279  |    10.247    |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 19, 
[2m[36m(pid=28873)[0m  train loss: 10.626918430328368
[2m[36m(pid=28873)[0m  eval loss: 21.326940765380858, eval err: 0.9197637701034546
[2m[36m(pid=28873)[0m 20, 
[2m[36m(pid=28873)[0m  train loss: 10.54271629333496
[2m[36m(pid=28873)[0m  eval loss: 20.700457611083984, eval err: 0.9166507649421692
[2m[36m(pid=28873)[0m 21, 
[2m[36m(pid=28873)[0m  train loss: 9.801153526306152
[2m[36m(pid=28873)[0m  eval loss: 16.622381324768067, eval err: 0.9087284994125366
Result for train_c87685ca:
  date: 2021-09-11_15-18-46
  done: false
  err: 0.9087284994125366
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 16.622381324768067
  loss_train: 9.801153526306152
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 2579.7237932682037
  time_this_iter_s: 350.74351501464844
  time_total_s: 2579.7237932682037
  timestamp: 1631366326
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |      8 |          2579.72 | 16.6224   | 0.908728  |     9.80115  |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 22, 
[2m[36m(pid=28873)[0m  train loss: 9.791063632965088
[2m[36m(pid=28873)[0m  eval loss: 20.624324874877928, eval err: 0.9216219067573548
[2m[36m(pid=28873)[0m 23, 
[2m[36m(pid=28873)[0m  train loss: 10.650839061737061
[2m[36m(pid=28873)[0m  eval loss: 19.398950347900392, eval err: 0.9057360267639161
[2m[36m(pid=28873)[0m 24, 
[2m[36m(pid=28873)[0m  train loss: 10.205826606750488
[2m[36m(pid=28873)[0m  eval loss: 17.01418529510498, eval err: 0.8949881434440613
Result for train_c87685ca:
  date: 2021-09-11_15-24-35
  done: false
  err: 0.8949881434440613
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 17.01418529510498
  loss_train: 10.205826606750488
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 2929.281307220459
  time_this_iter_s: 349.55751395225525
  time_total_s: 2929.281307220459
  timestamp: 1631366675
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |      9 |          2929.28 | 17.0142   | 0.894988  |    10.2058   |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 25, 
[2m[36m(pid=28873)[0m  train loss: 9.513411674499512
[2m[36m(pid=28873)[0m  eval loss: 14.782883262634277, eval err: 0.890947687625885
[2m[36m(pid=28873)[0m 26, 
[2m[36m(pid=28873)[0m  train loss: 9.845883617401123
[2m[36m(pid=28873)[0m  eval loss: 14.448654479980469, eval err: 0.8836638307571412
[2m[36m(pid=28873)[0m 27, 
[2m[36m(pid=28873)[0m  train loss: 10.36403455734253
[2m[36m(pid=28873)[0m  eval loss: 14.704055099487304, eval err: 0.877359869480133
Result for train_c87685ca:
  date: 2021-09-11_15-30-25
  done: false
  err: 0.877359869480133
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 14.704055099487304
  loss_train: 10.36403455734253
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 3279.09303689003
  time_this_iter_s: 349.8117296695709
  time_total_s: 3279.09303689003
  timestamp: 1631367025
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |     10 |          3279.09 | 14.7041   | 0.87736   |    10.364    |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 28, 
[2m[36m(pid=28873)[0m  train loss: 9.282855949401856
[2m[36m(pid=28873)[0m  eval loss: 14.738484115600587, eval err: 0.8848384809494019
[2m[36m(pid=28873)[0m 29, 
[2m[36m(pid=28873)[0m  train loss: 9.608356113433837
[2m[36m(pid=28873)[0m  eval loss: 15.715429840087891, eval err: 0.8777281546592712
[2m[36m(pid=28873)[0m 30, 
[2m[36m(pid=28873)[0m  train loss: 10.002084617614747
[2m[36m(pid=28873)[0m  eval loss: 14.240437850952148, eval err: 0.8409942889213562
Result for train_c87685ca:
  date: 2021-09-11_15-36-15
  done: false
  err: 0.8409942889213562
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 14.240437850952148
  loss_train: 10.002084617614747
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 3628.69327044487
  time_this_iter_s: 349.6002335548401
  time_total_s: 3628.69327044487
  timestamp: 1631367375
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |     11 |          3628.69 | 14.2404   | 0.840994  |    10.0021   |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 31, 
[2m[36m(pid=28873)[0m  train loss: 9.7203639793396
[2m[36m(pid=28873)[0m  eval loss: 14.969813041687011, eval err: 0.8664114904403687
[2m[36m(pid=28873)[0m 32, 
[2m[36m(pid=28873)[0m  train loss: 9.720103530883788
[2m[36m(pid=28873)[0m  eval loss: 11.934417381286622, eval err: 0.8586237311363221
[2m[36m(pid=28873)[0m 33, 
[2m[36m(pid=28873)[0m  train loss: 9.956195468902587
[2m[36m(pid=28873)[0m  eval loss: 13.080156517028808, eval err: 0.8675198006629944
Result for train_c87685ca:
  date: 2021-09-11_15-42-05
  done: false
  err: 0.8675198006629944
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 13.080156517028808
  loss_train: 9.956195468902587
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 3978.463941335678
  time_this_iter_s: 349.7706708908081
  time_total_s: 3978.463941335678
  timestamp: 1631367725
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |     12 |          3978.46 | 13.0802   | 0.86752   |     9.9562   |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 34, 
[2m[36m(pid=28873)[0m  train loss: 9.340926284790038
[2m[36m(pid=28873)[0m  eval loss: 11.996545829772948, eval err: 0.8599428319931031
[2m[36m(pid=28873)[0m 35, 
[2m[36m(pid=28873)[0m  train loss: 9.40191987991333
[2m[36m(pid=28873)[0m  eval loss: 13.17789005279541, eval err: 0.866419813632965
[2m[36m(pid=28873)[0m 36, 
[2m[36m(pid=28873)[0m  train loss: 9.12800687789917
[2m[36m(pid=28873)[0m  eval loss: 11.411558647155761, eval err: 0.8614214897155762
Result for train_c87685ca:
  date: 2021-09-11_15-47-54
  done: false
  err: 0.8614214897155762
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 11.411558647155761
  loss_train: 9.12800687789917
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 4328.024677991867
  time_this_iter_s: 349.56073665618896
  time_total_s: 4328.024677991867
  timestamp: 1631368074
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: c87685ca
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |     13 |          4328.02 | 11.4116   | 0.861421  |     9.12801  |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28873)[0m 37, 
[2m[36m(pid=28873)[0m  train loss: 9.259325847625732
[2m[36m(pid=28873)[0m  eval loss: 11.963240737915038, eval err: 0.8563150072097778
[2m[36m(pid=28873)[0m 38, 
[2m[36m(pid=28873)[0m  train loss: 8.940033168792725
[2m[36m(pid=28873)[0m  eval loss: 14.409830284118652, eval err: 0.8829865431785584
[2m[36m(pid=28873)[0m 39, 
[2m[36m(pid=28873)[0m  train loss: 9.469324741363526
Result for train_c87685ca:
  date: 2021-09-11_15-53-43
  done: true
  err: 0.8704761528968811
  experiment_id: 1529aca02c6b40a8bba16f0306b7bdee
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 12.371777267456055
  loss_train: 9.469324741363526
  node_ip: 131.220.7.54
  pid: 28873
  should_checkpoint: true
  time_since_restore: 4677.304224014282
  time_this_iter_s: 349.27954602241516
  time_total_s: 4677.304224014282
  timestamp: 1631368423
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: c87685ca
  
[2m[36m(pid=28873)[0m  eval loss: 12.371777267456055, eval err: 0.8704761528968811
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (2 PENDING, 1 RUNNING, 22 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c87685ca    | RUNNING    | 131.220.7.54:28873 |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |     14 |          4677.3  | 12.3718   | 0.870476  |     9.46932  |
| train_c876a87a    | PENDING    |                    |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |        |                  |           |           |              |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m Epoch:
[2m[36m(pid=28868)[0m 0, 
[2m[36m(pid=28868)[0m  train loss: 32.07652946472168
[2m[36m(pid=28868)[0m  eval loss: 80.15254287719726, eval err: 0.9765981936454773
Result for train_c876a87a:
  date: 2021-09-11_15-55-48
  done: false
  err: 0.9765981936454773
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 80.15254287719726
  loss_train: 32.07652946472168
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 123.26342582702637
  time_this_iter_s: 123.26342582702637
  time_total_s: 123.26342582702637
  timestamp: 1631368548
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |      1 |          123.263 | 80.1525   | 0.976598  |    32.0765   |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |         4644.56  |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 1, 
[2m[36m(pid=28868)[0m  train loss: 15.315664825439454
[2m[36m(pid=28868)[0m  eval loss: 14.776965522766114, eval err: 0.8643218970298767
[2m[36m(pid=28868)[0m 2, 
[2m[36m(pid=28868)[0m  train loss: 10.856598434448243
[2m[36m(pid=28868)[0m  eval loss: 11.367898635864258, eval err: 0.841878604888916
[2m[36m(pid=28868)[0m 3, 
[2m[36m(pid=28868)[0m  train loss: 11.103401794433594
Result for train_c876a87a:
  date: 2021-09-11_16-01-49
  done: false
  err: 0.8403807687759399
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 10.248300361633301
  loss_train: 11.103401794433594
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 483.3251030445099
  time_this_iter_s: 360.0616772174835
  time_total_s: 483.3251030445099
  timestamp: 1631368909
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: c876a87a
  
[2m[36m(pid=28868)[0m  eval loss: 10.248300361633301, eval err: 0.8403807687759399
== Status ==
Memory usage on this node: 11.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |      2 |          483.325 | 10.2483   | 0.840381  |    11.1034   |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |         4644.56  |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 4, 
[2m[36m(pid=28868)[0m  train loss: 9.749607009887695
[2m[36m(pid=28868)[0m  eval loss: 12.274914321899415, eval err: 0.8518466639518738
[2m[36m(pid=28868)[0m 5, 
[2m[36m(pid=28868)[0m  train loss: 10.50493293762207
[2m[36m(pid=28868)[0m  eval loss: 9.796566696166993, eval err: 0.81934077501297
[2m[36m(pid=28868)[0m 6, 
[2m[36m(pid=28868)[0m  train loss: 10.902668933868409
[2m[36m(pid=28868)[0m  eval loss: 11.460464172363281, eval err: 0.8542451667785644
Result for train_c876a87a:
  date: 2021-09-11_16-07-48
  done: false
  err: 0.8542451667785644
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 11.460464172363281
  loss_train: 10.902668933868409
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 843.0786066055298
  time_this_iter_s: 359.7535035610199
  time_total_s: 843.0786066055298
  timestamp: 1631369268
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |      3 |          843.079 | 11.4605   | 0.854245  |    10.9027   |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |         4644.56  |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 7, 
[2m[36m(pid=28868)[0m  train loss: 10.37388578414917
[2m[36m(pid=28868)[0m  eval loss: 9.796493110656739, eval err: 0.8238456869125366
[2m[36m(pid=28868)[0m 8, 
[2m[36m(pid=28868)[0m  train loss: 10.3326043510437
[2m[36m(pid=28868)[0m  eval loss: 10.339689388275147, eval err: 0.8358556151390075
[2m[36m(pid=28868)[0m 9, 
[2m[36m(pid=28868)[0m  train loss: 9.601549644470214
Result for train_c876a87a:
  date: 2021-09-11_16-13-48
  done: false
  err: 0.7869122242927551
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 8.294589595794678
  loss_train: 9.601549644470214
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 1202.350215435028
  time_this_iter_s: 359.2716088294983
  time_total_s: 1202.350215435028
  timestamp: 1631369628
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: c876a87a
  
[2m[36m(pid=28868)[0m  eval loss: 8.294589595794678, eval err: 0.7869122242927551
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |      4 |          1202.35 |  8.29459  | 0.786912  |     9.60155  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 10, 
[2m[36m(pid=28868)[0m  train loss: 8.967815589904784
[2m[36m(pid=28868)[0m  eval loss: 11.994331684112549, eval err: 0.8288833093643189
[2m[36m(pid=28868)[0m 11, 
[2m[36m(pid=28868)[0m  train loss: 8.251106452941894
[2m[36m(pid=28868)[0m  eval loss: 8.66679718017578, eval err: 0.8008434510231018
[2m[36m(pid=28868)[0m 12, 
[2m[36m(pid=28868)[0m  train loss: 8.454898338317872
[2m[36m(pid=28868)[0m  eval loss: 9.709959697723388, eval err: 0.807360143661499
Result for train_c876a87a:
  date: 2021-09-11_16-19-46
  done: false
  err: 0.807360143661499
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 9.709959697723388
  loss_train: 8.454898338317872
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 1560.981674194336
  time_this_iter_s: 358.63145875930786
  time_total_s: 1560.981674194336
  timestamp: 1631369986
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 11.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |      5 |          1560.98 |  9.70996  | 0.80736   |     8.4549   |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 13, 
[2m[36m(pid=28868)[0m  train loss: 8.15917444229126
[2m[36m(pid=28868)[0m  eval loss: 9.954821720123292, eval err: 0.8061741518974305
[2m[36m(pid=28868)[0m 14, 
[2m[36m(pid=28868)[0m  train loss: 8.50415927886963
[2m[36m(pid=28868)[0m  eval loss: 8.260508708953857, eval err: 0.7721444916725159
[2m[36m(pid=28868)[0m 15, 
[2m[36m(pid=28868)[0m  train loss: 8.441419525146484
[2m[36m(pid=28868)[0m  eval loss: 8.874091262817382, eval err: 0.7804633116722107
Result for train_c876a87a:
  date: 2021-09-11_16-25-45
  done: false
  err: 0.7804633116722107
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 8.874091262817382
  loss_train: 8.441419525146484
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 1919.8264653682709
  time_this_iter_s: 358.84479117393494
  time_total_s: 1919.8264653682709
  timestamp: 1631370345
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |      6 |          1919.83 |  8.87409  | 0.780463  |     8.44142  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 16, 
[2m[36m(pid=28868)[0m  train loss: 8.156090106964111
[2m[36m(pid=28868)[0m  eval loss: 9.455937442779542, eval err: 0.7854073071479797
[2m[36m(pid=28868)[0m 17, 
[2m[36m(pid=28868)[0m  train loss: 7.705435771942138
[2m[36m(pid=28868)[0m  eval loss: 8.267946910858154, eval err: 0.75981210231781
[2m[36m(pid=28868)[0m 18, 
[2m[36m(pid=28868)[0m  train loss: 7.12398681640625
[2m[36m(pid=28868)[0m  eval loss: 8.844897747039795, eval err: 0.7665695977210999
Result for train_c876a87a:
  date: 2021-09-11_16-31-44
  done: false
  err: 0.7665695977210999
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 8.844897747039795
  loss_train: 7.12398681640625
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 2279.0059447288513
  time_this_iter_s: 359.17947936058044
  time_total_s: 2279.0059447288513
  timestamp: 1631370704
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |      7 |          2279.01 |  8.8449   | 0.76657   |     7.12399  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 19, 
[2m[36m(pid=28868)[0m  train loss: 6.223402214050293
[2m[36m(pid=28868)[0m  eval loss: 8.795982074737548, eval err: 0.7361442613601684
[2m[36m(pid=28868)[0m 20, 
[2m[36m(pid=28868)[0m  train loss: 7.124776382446289
[2m[36m(pid=28868)[0m  eval loss: 6.7889875793457035, eval err: 0.7229157638549805
[2m[36m(pid=28868)[0m 21, 
[2m[36m(pid=28868)[0m  train loss: 6.5704743194580075
[2m[36m(pid=28868)[0m  eval loss: 7.794431800842285, eval err: 0.7318757390975952
Result for train_c876a87a:
  date: 2021-09-11_16-37-43
  done: false
  err: 0.7318757390975952
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 7.794431800842285
  loss_train: 6.5704743194580075
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 2637.8030412197113
  time_this_iter_s: 358.79709649086
  time_total_s: 2637.8030412197113
  timestamp: 1631371063
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |      8 |          2637.8  |  7.79443  | 0.731876  |     6.57047  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 22, 
[2m[36m(pid=28868)[0m  train loss: 6.914416122436523
[2m[36m(pid=28868)[0m  eval loss: 6.450645561218262, eval err: 0.6866269683837891
[2m[36m(pid=28868)[0m 23, 
[2m[36m(pid=28868)[0m  train loss: 6.0280193328857425
[2m[36m(pid=28868)[0m  eval loss: 6.711337566375732, eval err: 0.6804786920547485
[2m[36m(pid=28868)[0m 24, 
[2m[36m(pid=28868)[0m  train loss: 5.950061979293824
[2m[36m(pid=28868)[0m  eval loss: 5.716935176849365, eval err: 0.6224742579460144
Result for train_c876a87a:
  date: 2021-09-11_16-43-42
  done: false
  err: 0.6224742579460144
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 5.716935176849365
  loss_train: 5.950061979293824
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 2996.467252969742
  time_this_iter_s: 358.6642117500305
  time_total_s: 2996.467252969742
  timestamp: 1631371422
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |      9 |          2996.47 |  5.71694  | 0.622474  |     5.95006  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 25, 
[2m[36m(pid=28868)[0m  train loss: 5.273630285263062
[2m[36m(pid=28868)[0m  eval loss: 5.396043081283569, eval err: 0.6084463286399842
[2m[36m(pid=28868)[0m 26, 
[2m[36m(pid=28868)[0m  train loss: 5.330664854049683
[2m[36m(pid=28868)[0m  eval loss: 4.456080722808838, eval err: 0.5455533313751221
[2m[36m(pid=28868)[0m 27, 
[2m[36m(pid=28868)[0m  train loss: 4.639399681091309
[2m[36m(pid=28868)[0m  eval loss: 4.196275577545166, eval err: 0.46741101026535037
Result for train_c876a87a:
  date: 2021-09-11_16-49-40
  done: false
  err: 0.46741101026535037
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 4.196275577545166
  loss_train: 4.639399681091309
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 3354.987302541733
  time_this_iter_s: 358.52004957199097
  time_total_s: 3354.987302541733
  timestamp: 1631371780
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 11.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     10 |          3354.99 |  4.19628  | 0.467411  |     4.6394   |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 28, 
[2m[36m(pid=28868)[0m  train loss: 3.963002824783325
[2m[36m(pid=28868)[0m  eval loss: 3.9213842868804933, eval err: 0.43201547622680664
[2m[36m(pid=28868)[0m 29, 
[2m[36m(pid=28868)[0m  train loss: 3.604732942581177
[2m[36m(pid=28868)[0m  eval loss: 3.2838291549682617, eval err: 0.3507016849517822
[2m[36m(pid=28868)[0m 30, 
[2m[36m(pid=28868)[0m  train loss: 3.270982666015625
[2m[36m(pid=28868)[0m  eval loss: 2.24542067527771, eval err: 0.23541619777679443
Result for train_c876a87a:
  date: 2021-09-11_16-55-40
  done: false
  err: 0.23541619777679443
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 2.24542067527771
  loss_train: 3.270982666015625
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 3714.421561717987
  time_this_iter_s: 359.4342591762543
  time_total_s: 3714.421561717987
  timestamp: 1631372140
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     11 |          3714.42 |  2.24542  | 0.235416  |     3.27098  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 31, 
[2m[36m(pid=28868)[0m  train loss: 2.947382507324219
[2m[36m(pid=28868)[0m  eval loss: 2.064704656600952, eval err: 0.22177988767623902
[2m[36m(pid=28868)[0m 32, 
[2m[36m(pid=28868)[0m  train loss: 2.585807604789734
[2m[36m(pid=28868)[0m  eval loss: 1.6932378649711608, eval err: 0.16684179544448852
[2m[36m(pid=28868)[0m 33, 
[2m[36m(pid=28868)[0m  train loss: 2.264377317428589
[2m[36m(pid=28868)[0m  eval loss: 1.6290974044799804, eval err: 0.1588454008102417
Result for train_c876a87a:
  date: 2021-09-11_17-01-50
  done: false
  err: 0.1588454008102417
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 1.6290974044799804
  loss_train: 2.264377317428589
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 4084.4124779701233
  time_this_iter_s: 369.99091625213623
  time_total_s: 4084.4124779701233
  timestamp: 1631372510
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     12 |          4084.41 |  1.6291   | 0.158845  |     2.26438  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 34, 
[2m[36m(pid=28868)[0m  train loss: 2.1024572467803955
[2m[36m(pid=28868)[0m  eval loss: 1.3903809118270873, eval err: 0.12065978765487671
[2m[36m(pid=28868)[0m 35, 
[2m[36m(pid=28868)[0m  train loss: 1.853540587425232
[2m[36m(pid=28868)[0m  eval loss: 1.28953843832016, eval err: 0.11033544063568115
[2m[36m(pid=28868)[0m 36, 
[2m[36m(pid=28868)[0m  train loss: 1.8218922090530396
[2m[36m(pid=28868)[0m  eval loss: 1.2823787546157837, eval err: 0.10654372930526733
Result for train_c876a87a:
  date: 2021-09-11_17-08-00
  done: false
  err: 0.10654372930526733
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 1.2823787546157837
  loss_train: 1.8218922090530396
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 4454.3829391002655
  time_this_iter_s: 369.9704611301422
  time_total_s: 4454.3829391002655
  timestamp: 1631372880
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     13 |          4454.38 |  1.28238  | 0.106544  |     1.82189  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 37, 
[2m[36m(pid=28868)[0m  train loss: 1.8946618127822876
[2m[36m(pid=28868)[0m  eval loss: 1.2524562072753906, eval err: 0.1041501522064209
[2m[36m(pid=28868)[0m 38, 
[2m[36m(pid=28868)[0m  train loss: 1.879023823738098
[2m[36m(pid=28868)[0m  eval loss: 1.3938614177703856, eval err: 0.1222599458694458
[2m[36m(pid=28868)[0m 39, 
[2m[36m(pid=28868)[0m  train loss: 1.6518126678466798
[2m[36m(pid=28868)[0m  eval loss: 1.213095359802246, eval err: 0.09605344533920288
Result for train_c876a87a:
  date: 2021-09-11_17-14-09
  done: false
  err: 0.09605344533920288
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 1.213095359802246
  loss_train: 1.6518126678466798
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 4823.5287046432495
  time_this_iter_s: 369.145765542984
  time_total_s: 4823.5287046432495
  timestamp: 1631373249
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     14 |          4823.53 |  1.2131   | 0.0960534 |     1.65181  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 40, 
[2m[36m(pid=28868)[0m  train loss: 1.8189673852920532
[2m[36m(pid=28868)[0m  eval loss: 3.2063884115219117, eval err: 0.3126096224784851
[2m[36m(pid=28868)[0m 41, 
[2m[36m(pid=28868)[0m  train loss: 2.0230681896209717
[2m[36m(pid=28868)[0m  eval loss: 1.630767662525177, eval err: 0.14700982332229615
[2m[36m(pid=28868)[0m 42, 
[2m[36m(pid=28868)[0m  train loss: 1.844206027984619
[2m[36m(pid=28868)[0m  eval loss: 1.185735023021698, eval err: 0.08793037176132203
Result for train_c876a87a:
  date: 2021-09-11_17-20-17
  done: false
  err: 0.08793037176132203
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 15
  loss: 1.185735023021698
  loss_train: 1.844206027984619
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 5191.934376716614
  time_this_iter_s: 368.40567207336426
  time_total_s: 5191.934376716614
  timestamp: 1631373617
  timesteps_since_restore: 0
  training_iteration: 15
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     15 |          5191.93 |  1.18574  | 0.0879304 |     1.84421  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 43, 
[2m[36m(pid=28868)[0m  train loss: 1.8225156950950623
[2m[36m(pid=28868)[0m  eval loss: 1.1439385271072389, eval err: 0.09207435846328735
[2m[36m(pid=28868)[0m 44, 
[2m[36m(pid=28868)[0m  train loss: 1.7826563572883607
[2m[36m(pid=28868)[0m  eval loss: 1.0969223070144654, eval err: 0.08881057262420654
[2m[36m(pid=28868)[0m 45, 
[2m[36m(pid=28868)[0m  train loss: 1.473255569934845
[2m[36m(pid=28868)[0m  eval loss: 1.071959264278412, eval err: 0.08428678274154663
Result for train_c876a87a:
  date: 2021-09-11_17-26-25
  done: false
  err: 0.08428678274154663
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 16
  loss: 1.071959264278412
  loss_train: 1.473255569934845
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 5559.413567304611
  time_this_iter_s: 367.47919058799744
  time_total_s: 5559.413567304611
  timestamp: 1631373985
  timesteps_since_restore: 0
  training_iteration: 16
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.5/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     16 |          5559.41 |  1.07196  | 0.0842868 |     1.47326  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 46, 
[2m[36m(pid=28868)[0m  train loss: 1.5668616199493408
[2m[36m(pid=28868)[0m  eval loss: 1.0300041484832763, eval err: 0.08047039270401
[2m[36m(pid=28868)[0m 47, 
[2m[36m(pid=28868)[0m  train loss: 1.4691200542449951
[2m[36m(pid=28868)[0m  eval loss: 1.0317008471488953, eval err: 0.08050902843475342
[2m[36m(pid=28868)[0m 48, 
[2m[36m(pid=28868)[0m  train loss: 1.3719272232055664
[2m[36m(pid=28868)[0m  eval loss: 0.9818119239807129, eval err: 0.07582705020904541
Result for train_c876a87a:
  date: 2021-09-11_17-32-31
  done: false
  err: 0.07582705020904541
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 17
  loss: 0.9818119239807129
  loss_train: 1.3719272232055664
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 5925.747507095337
  time_this_iter_s: 366.3339397907257
  time_total_s: 5925.747507095337
  timestamp: 1631374351
  timesteps_since_restore: 0
  training_iteration: 17
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     17 |          5925.75 |  0.981812 | 0.0758271 |     1.37193  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 49, 
[2m[36m(pid=28868)[0m  train loss: 1.276684365272522
[2m[36m(pid=28868)[0m  eval loss: 0.9564493274688721, eval err: 0.07358937501907349
[2m[36m(pid=28868)[0m 50, 
[2m[36m(pid=28868)[0m  train loss: 1.3140574765205384
[2m[36m(pid=28868)[0m  eval loss: 0.9462836015224457, eval err: 0.07063011169433593
[2m[36m(pid=28868)[0m 51, 
[2m[36m(pid=28868)[0m  train loss: 1.4747165489196776
[2m[36m(pid=28868)[0m  eval loss: 0.9330449724197387, eval err: 0.06864340305328369
Result for train_c876a87a:
  date: 2021-09-11_17-38-37
  done: false
  err: 0.06864340305328369
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 18
  loss: 0.9330449724197387
  loss_train: 1.4747165489196776
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 6291.676549196243
  time_this_iter_s: 365.9290421009064
  time_total_s: 6291.676549196243
  timestamp: 1631374717
  timesteps_since_restore: 0
  training_iteration: 18
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     18 |          6291.68 |  0.933045 | 0.0686434 |     1.47472  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 52, 
[2m[36m(pid=28868)[0m  train loss: 1.353198459148407
[2m[36m(pid=28868)[0m  eval loss: 0.9567394256591797, eval err: 0.07108776330947876
[2m[36m(pid=28868)[0m 53, 
[2m[36m(pid=28868)[0m  train loss: 1.2582860469818116
[2m[36m(pid=28868)[0m  eval loss: 0.9265466010570527, eval err: 0.06910918951034546
[2m[36m(pid=28868)[0m 54, 
[2m[36m(pid=28868)[0m  train loss: 1.373433005809784
[2m[36m(pid=28868)[0m  eval loss: 0.9235301995277405, eval err: 0.06549390316009522
Result for train_c876a87a:
  date: 2021-09-11_17-44-43
  done: false
  err: 0.06549390316009522
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 19
  loss: 0.9235301995277405
  loss_train: 1.373433005809784
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 6657.393361568451
  time_this_iter_s: 365.71681237220764
  time_total_s: 6657.393361568451
  timestamp: 1631375083
  timesteps_since_restore: 0
  training_iteration: 19
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     19 |          6657.39 |  0.92353  | 0.0654939 |     1.37343  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 55, 
[2m[36m(pid=28868)[0m  train loss: 1.349642834663391
[2m[36m(pid=28868)[0m  eval loss: 0.9288961780071259, eval err: 0.06895156621932984
[2m[36m(pid=28868)[0m 56, 
[2m[36m(pid=28868)[0m  train loss: 1.3009036254882813
[2m[36m(pid=28868)[0m  eval loss: 0.9138455843925476, eval err: 0.06724805116653443
[2m[36m(pid=28868)[0m 57, 
[2m[36m(pid=28868)[0m  train loss: 1.3159555506706238
[2m[36m(pid=28868)[0m  eval loss: 0.9311772537231445, eval err: 0.0659088659286499
Result for train_c876a87a:
  date: 2021-09-11_17-50-48
  done: false
  err: 0.0659088659286499
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 20
  loss: 0.9311772537231445
  loss_train: 1.3159555506706238
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 7022.708403348923
  time_this_iter_s: 365.3150417804718
  time_total_s: 7022.708403348923
  timestamp: 1631375448
  timesteps_since_restore: 0
  training_iteration: 20
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     20 |          7022.71 |  0.931177 | 0.0659089 |     1.31596  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28868)[0m 58, 
[2m[36m(pid=28868)[0m  train loss: 1.1871122527122497
[2m[36m(pid=28868)[0m  eval loss: 0.887172554731369, eval err: 0.06432711839675903
[2m[36m(pid=28868)[0m 59, 
[2m[36m(pid=28868)[0m  train loss: 1.163671236038208
[2m[36m(pid=28868)[0m  eval loss: 0.8726211273670197, eval err: 0.06368757963180542
[2m[36m(pid=28868)[0m 60, 
[2m[36m(pid=28868)[0m  train loss: 1.3116495442390441
[2m[36m(pid=28868)[0m  eval loss: 0.9250315082073212, eval err: 0.06676979780197144
Result for train_c876a87a:
  date: 2021-09-11_17-56-53
  done: true
  err: 0.06676979780197144
  experiment_id: cca08ef434e7446a9683179ec948afd7
  hostname: bigcuda4
  iterations_since_restore: 21
  loss: 0.9250315082073212
  loss_train: 1.3116495442390441
  node_ip: 131.220.7.54
  pid: 28868
  should_checkpoint: true
  time_since_restore: 7387.55023598671
  time_this_iter_s: 364.84183263778687
  time_total_s: 7387.55023598671
  timestamp: 1631375813
  timesteps_since_restore: 0
  training_iteration: 21
  trial_id: c876a87a
  
== Status ==
Memory usage on this node: 15.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 PENDING, 1 RUNNING, 23 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876a87a    | RUNNING    | 131.220.7.54:28868 |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     21 |          7387.55 |  0.925032 | 0.0667698 |     1.31165  |
| train_c876cbfc    | PENDING    |                    |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |        |                  |           |           |              |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m Epoch:
[2m[36m(pid=28871)[0m 0, 
[2m[36m(pid=28871)[0m  train loss: 30.985039825439454
[2m[36m(pid=28871)[0m  eval loss: 87.80831642150879, eval err: 0.9806626892089844
Result for train_c876cbfc:
  date: 2021-09-11_17-59-13
  done: false
  err: 0.9806626892089844
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 1
  loss: 87.80831642150879
  loss_train: 30.985039825439454
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 138.3037407398224
  time_this_iter_s: 138.3037407398224
  time_total_s: 138.3037407398224
  timestamp: 1631375953
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |      1 |          138.304 | 87.8083   | 0.980663  |    30.985    |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |         4644.56  |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 1, 
[2m[36m(pid=28871)[0m  train loss: 29.017227172851562
[2m[36m(pid=28871)[0m  eval loss: 90.44729568481445, eval err: 0.9841922450065613
[2m[36m(pid=28871)[0m 2, 
[2m[36m(pid=28871)[0m  train loss: 24.9491609954834
[2m[36m(pid=28871)[0m  eval loss: 69.42834419250488, eval err: 0.9753234505653381
[2m[36m(pid=28871)[0m 3, 
[2m[36m(pid=28871)[0m  train loss: 17.710459060668946
[2m[36m(pid=28871)[0m  eval loss: 32.519558639526366, eval err: 0.9341598653793335
Result for train_c876cbfc:
  date: 2021-09-11_18-06-00
  done: false
  err: 0.9341598653793335
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 2
  loss: 32.519558639526366
  loss_train: 17.710459060668946
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 545.4749374389648
  time_this_iter_s: 407.17119669914246
  time_total_s: 545.4749374389648
  timestamp: 1631376360
  timesteps_since_restore: 0
  training_iteration: 2
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |      2 |          545.475 | 32.5196   | 0.93416   |    17.7105   |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |         4644.56  |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 4, 
[2m[36m(pid=28871)[0m  train loss: 14.36578598022461
[2m[36m(pid=28871)[0m  eval loss: 19.450597190856932, eval err: 0.9066332149505615
[2m[36m(pid=28871)[0m 5, 
[2m[36m(pid=28871)[0m  train loss: 13.623863220214844
[2m[36m(pid=28871)[0m  eval loss: 18.321447868347168, eval err: 0.894352159500122
[2m[36m(pid=28871)[0m 6, 
[2m[36m(pid=28871)[0m  train loss: 11.517690544128419
[2m[36m(pid=28871)[0m  eval loss: 17.726488876342774, eval err: 0.894676330089569
Result for train_c876cbfc:
  date: 2021-09-11_18-12-47
  done: false
  err: 0.894676330089569
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 3
  loss: 17.726488876342774
  loss_train: 11.517690544128419
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 951.9158592224121
  time_this_iter_s: 406.44092178344727
  time_total_s: 951.9158592224121
  timestamp: 1631376767
  timesteps_since_restore: 0
  training_iteration: 3
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.4/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |      3 |          951.916 | 17.7265   | 0.894676  |    11.5177   |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |         4652.15  |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |         9629.62  |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |         7323.98  |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |         7694.9   |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |         4638.25  |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |         7281.35  |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |         7825.89  |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |         9356.76  |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |        10877.9   |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |         5416.25  | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |         4644.56  |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |         9802.53  |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |         4314.17  | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |         4282.03  | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |         3897.67  |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |         4316.23  |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |         6234.39  |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |         3857.95  | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |         7753.02  |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 7, 
[2m[36m(pid=28871)[0m  train loss: 11.10518898010254
[2m[36m(pid=28871)[0m  eval loss: 18.594387893676757, eval err: 0.9068731570243835
[2m[36m(pid=28871)[0m 8, 
[2m[36m(pid=28871)[0m  train loss: 10.07470157623291
[2m[36m(pid=28871)[0m  eval loss: 13.099410362243653, eval err: 0.8572599530220032
[2m[36m(pid=28871)[0m 9, 
[2m[36m(pid=28871)[0m  train loss: 10.207873420715332
[2m[36m(pid=28871)[0m  eval loss: 13.589152946472169, eval err: 0.8611155462265014
Result for train_c876cbfc:
  date: 2021-09-11_18-19-33
  done: false
  err: 0.8611155462265014
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 4
  loss: 13.589152946472169
  loss_train: 10.207873420715332
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 1358.2082982063293
  time_this_iter_s: 406.29243898391724
  time_total_s: 1358.2082982063293
  timestamp: 1631377173
  timesteps_since_restore: 0
  training_iteration: 4
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |      4 |          1358.21 | 13.5892   | 0.861116  |    10.2079   |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 10, 
[2m[36m(pid=28871)[0m  train loss: 10.482706565856933
[2m[36m(pid=28871)[0m  eval loss: 14.228243370056152, eval err: 0.8709739995002747
[2m[36m(pid=28871)[0m 11, 
[2m[36m(pid=28871)[0m  train loss: 10.006955165863037
[2m[36m(pid=28871)[0m  eval loss: 14.700970878601074, eval err: 0.8791158962249755
[2m[36m(pid=28871)[0m 12, 
[2m[36m(pid=28871)[0m  train loss: 9.808519611358642
[2m[36m(pid=28871)[0m  eval loss: 13.367244720458984, eval err: 0.8629049253463745
Result for train_c876cbfc:
  date: 2021-09-11_18-26-19
  done: false
  err: 0.8629049253463745
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 5
  loss: 13.367244720458984
  loss_train: 9.808519611358642
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 1764.3240840435028
  time_this_iter_s: 406.11578583717346
  time_total_s: 1764.3240840435028
  timestamp: 1631377579
  timesteps_since_restore: 0
  training_iteration: 5
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |      5 |          1764.32 | 13.3672   | 0.862905  |     9.80852  |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 13, 
[2m[36m(pid=28871)[0m  train loss: 9.990169219970703
[2m[36m(pid=28871)[0m  eval loss: 14.515687980651855, eval err: 0.8755734777450561
[2m[36m(pid=28871)[0m 14, 
[2m[36m(pid=28871)[0m  train loss: 9.658446235656738
[2m[36m(pid=28871)[0m  eval loss: 13.555071029663086, eval err: 0.8659084153175354
[2m[36m(pid=28871)[0m 15, 
[2m[36m(pid=28871)[0m  train loss: 10.118735256195068
[2m[36m(pid=28871)[0m  eval loss: 12.817243309020997, eval err: 0.85355073928833
Result for train_c876cbfc:
  date: 2021-09-11_18-33-05
  done: false
  err: 0.85355073928833
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 6
  loss: 12.817243309020997
  loss_train: 10.118735256195068
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 2170.4176807403564
  time_this_iter_s: 406.09359669685364
  time_total_s: 2170.4176807403564
  timestamp: 1631377985
  timesteps_since_restore: 0
  training_iteration: 6
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |      6 |          2170.42 | 12.8172   | 0.853551  |    10.1187   |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 16, 
[2m[36m(pid=28871)[0m  train loss: 9.563279876708984
[2m[36m(pid=28871)[0m  eval loss: 12.792437858581543, eval err: 0.863856029510498
[2m[36m(pid=28871)[0m 17, 
[2m[36m(pid=28871)[0m  train loss: 9.69481388092041
[2m[36m(pid=28871)[0m  eval loss: 11.791936111450195, eval err: 0.8520520567893982
[2m[36m(pid=28871)[0m 18, 
[2m[36m(pid=28871)[0m  train loss: 9.189086647033692
[2m[36m(pid=28871)[0m  eval loss: 10.650944633483887, eval err: 0.8323487257957458
Result for train_c876cbfc:
  date: 2021-09-11_18-39-51
  done: false
  err: 0.8323487257957458
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 7
  loss: 10.650944633483887
  loss_train: 9.189086647033692
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 2576.5276284217834
  time_this_iter_s: 406.109947681427
  time_total_s: 2576.5276284217834
  timestamp: 1631378391
  timesteps_since_restore: 0
  training_iteration: 7
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |      7 |          2576.53 | 10.6509   | 0.832349  |     9.18909  |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 19, 
[2m[36m(pid=28871)[0m  train loss: 9.435749969482423
[2m[36m(pid=28871)[0m  eval loss: 11.667246131896972, eval err: 0.8493243527412414
[2m[36m(pid=28871)[0m 20, 
[2m[36m(pid=28871)[0m  train loss: 9.044840106964111
[2m[36m(pid=28871)[0m  eval loss: 12.38990837097168, eval err: 0.8627678394317627
[2m[36m(pid=28871)[0m 21, 
[2m[36m(pid=28871)[0m  train loss: 9.202077903747558
[2m[36m(pid=28871)[0m  eval loss: 11.692113952636719, eval err: 0.8541910648345947
Result for train_c876cbfc:
  date: 2021-09-11_18-46-36
  done: false
  err: 0.8541910648345947
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 8
  loss: 11.692113952636719
  loss_train: 9.202077903747558
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 2981.7909972667694
  time_this_iter_s: 405.26336884498596
  time_total_s: 2981.7909972667694
  timestamp: 1631378796
  timesteps_since_restore: 0
  training_iteration: 8
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |      8 |          2981.79 | 11.6921   | 0.854191  |     9.20208  |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 22, 
[2m[36m(pid=28871)[0m  train loss: 8.671336288452148
[2m[36m(pid=28871)[0m  eval loss: 10.944014148712158, eval err: 0.8397737860679626
[2m[36m(pid=28871)[0m 23, 
[2m[36m(pid=28871)[0m  train loss: 9.479572105407716
[2m[36m(pid=28871)[0m  eval loss: 12.762882194519044, eval err: 0.8603074026107788
[2m[36m(pid=28871)[0m 24, 
[2m[36m(pid=28871)[0m  train loss: 8.977549419403076
[2m[36m(pid=28871)[0m  eval loss: 10.865093841552735, eval err: 0.8416488552093506
Result for train_c876cbfc:
  date: 2021-09-11_18-53-22
  done: false
  err: 0.8416488552093506
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 9
  loss: 10.865093841552735
  loss_train: 8.977549419403076
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 3387.093175649643
  time_this_iter_s: 405.30217838287354
  time_total_s: 3387.093175649643
  timestamp: 1631379202
  timesteps_since_restore: 0
  training_iteration: 9
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |      9 |          3387.09 | 10.8651   | 0.841649  |     8.97755  |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 25, 
[2m[36m(pid=28871)[0m  train loss: 8.641405429840088
[2m[36m(pid=28871)[0m  eval loss: 11.267806968688966, eval err: 0.8491015458106994
[2m[36m(pid=28871)[0m 26, 
[2m[36m(pid=28871)[0m  train loss: 9.20117404937744
[2m[36m(pid=28871)[0m  eval loss: 12.754687156677246, eval err: 0.8598843693733216
[2m[36m(pid=28871)[0m 27, 
[2m[36m(pid=28871)[0m  train loss: 9.147524852752685
[2m[36m(pid=28871)[0m  eval loss: 11.650719718933106, eval err: 0.8493183755874634
Result for train_c876cbfc:
  date: 2021-09-11_19-00-07
  done: false
  err: 0.8493183755874634
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 10
  loss: 11.650719718933106
  loss_train: 9.147524852752685
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 3791.980702161789
  time_this_iter_s: 404.887526512146
  time_total_s: 3791.980702161789
  timestamp: 1631379607
  timesteps_since_restore: 0
  training_iteration: 10
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.1/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |     10 |          3791.98 | 11.6507   | 0.849318  |     9.14752  |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 28, 
[2m[36m(pid=28871)[0m  train loss: 8.892299900054931
[2m[36m(pid=28871)[0m  eval loss: 13.08626600265503, eval err: 0.8593338346481323
[2m[36m(pid=28871)[0m 29, 
[2m[36m(pid=28871)[0m  train loss: 8.617236080169677
[2m[36m(pid=28871)[0m  eval loss: 13.759498252868653, eval err: 0.8648609828948974
[2m[36m(pid=28871)[0m 30, 
[2m[36m(pid=28871)[0m  train loss: 9.2977077293396
[2m[36m(pid=28871)[0m  eval loss: 13.992890472412109, eval err: 0.8732872486114502
Result for train_c876cbfc:
  date: 2021-09-11_19-06-52
  done: false
  err: 0.8732872486114502
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 11
  loss: 13.992890472412109
  loss_train: 9.2977077293396
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 4197.104134559631
  time_this_iter_s: 405.1234323978424
  time_total_s: 4197.104134559631
  timestamp: 1631380012
  timesteps_since_restore: 0
  training_iteration: 11
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |     11 |          4197.1  | 13.9929   | 0.873287  |     9.29771  |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 31, 
[2m[36m(pid=28871)[0m  train loss: 9.195159664154053
[2m[36m(pid=28871)[0m  eval loss: 11.556775741577148, eval err: 0.8428110647201538
[2m[36m(pid=28871)[0m 32, 
[2m[36m(pid=28871)[0m  train loss: 9.046818408966065
[2m[36m(pid=28871)[0m  eval loss: 11.32181526184082, eval err: 0.8350132894515991
[2m[36m(pid=28871)[0m 33, 
[2m[36m(pid=28871)[0m  train loss: 8.380275058746339
[2m[36m(pid=28871)[0m  eval loss: 12.563167114257812, eval err: 0.8605521750450135
Result for train_c876cbfc:
  date: 2021-09-11_19-13-37
  done: false
  err: 0.8605521750450135
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 12
  loss: 12.563167114257812
  loss_train: 8.380275058746339
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 4601.940700292587
  time_this_iter_s: 404.83656573295593
  time_total_s: 4601.940700292587
  timestamp: 1631380417
  timesteps_since_restore: 0
  training_iteration: 12
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |     12 |          4601.94 | 12.5632   | 0.860552  |     8.38028  |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 34, 
[2m[36m(pid=28871)[0m  train loss: 9.332104225158691
[2m[36m(pid=28871)[0m  eval loss: 13.985427169799804, eval err: 0.8601485538482666
[2m[36m(pid=28871)[0m 35, 
[2m[36m(pid=28871)[0m  train loss: 8.654875469207763
[2m[36m(pid=28871)[0m  eval loss: 10.143185367584229, eval err: 0.8335383939743042
[2m[36m(pid=28871)[0m 36, 
[2m[36m(pid=28871)[0m  train loss: 8.454424438476563
[2m[36m(pid=28871)[0m  eval loss: 11.973727912902833, eval err: 0.8489404702186585
Result for train_c876cbfc:
  date: 2021-09-11_19-20-22
  done: false
  err: 0.8489404702186585
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 13
  loss: 11.973727912902833
  loss_train: 8.454424438476563
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 5007.323658943176
  time_this_iter_s: 405.382958650589
  time_total_s: 5007.323658943176
  timestamp: 1631380822
  timesteps_since_restore: 0
  training_iteration: 13
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.3/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |     13 |          5007.32 | 11.9737   | 0.84894   |     8.45442  |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


[2m[36m(pid=28871)[0m 37, 
[2m[36m(pid=28871)[0m  train loss: 8.792831478118897
[2m[36m(pid=28871)[0m  eval loss: 11.992853393554688, eval err: 0.852260844707489
[2m[36m(pid=28871)[0m 38, 
[2m[36m(pid=28871)[0m  train loss: 9.087317218780518
[2m[36m(pid=28871)[0m  eval loss: 11.15865577697754, eval err: 0.8343641424179077
[2m[36m(pid=28871)[0m 39, 
[2m[36m(pid=28871)[0m  train loss: 9.246393146514892
[2m[36m(pid=28871)[0m  eval loss: 9.789023780822754, eval err: 0.8290526747703553
Result for train_c876cbfc:
  date: 2021-09-11_19-27-08
  done: true
  err: 0.8290526747703553
  experiment_id: 85d2c43f047b4eb3bc6d14cc0b752664
  hostname: bigcuda4
  iterations_since_restore: 14
  loss: 9.789023780822754
  loss_train: 9.246393146514892
  node_ip: 131.220.7.54
  pid: 28871
  should_checkpoint: true
  time_since_restore: 5413.07119512558
  time_this_iter_s: 405.74753618240356
  time_total_s: 5413.07119512558
  timestamp: 1631381228
  timesteps_since_restore: 0
  training_iteration: 14
  trial_id: c876cbfc
  
== Status ==
Memory usage on this node: 15.2/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/40 CPUs, 1.0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (1 RUNNING, 24 TERMINATED)
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc                |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_c876cbfc    | RUNNING    | 131.220.7.54:28871 |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |     14 |          5413.07 |  9.78902  | 0.829053  |     9.24639  |
| train_2e5ee57e    | TERMINATED |                    |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f0ed2    | TERMINATED |                    |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |                    |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5f755c    | TERMINATED |                    |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f955a    | TERMINATED |                    |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fb72e    | TERMINATED |                    |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_2e5fd682    | TERMINATED |                    |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |                    |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_58e329d6    | TERMINATED |                    |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_58e34cae    | TERMINATED |                    |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e37148    | TERMINATED |                    |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_b1c14_00000 | TERMINATED |                    |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_b1c14_00001 | TERMINATED |                    |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00006 | TERMINATED |                    |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00012 | TERMINATED |                    |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00013 | TERMINATED |                    |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00015 | TERMINATED |                    |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00018 | TERMINATED |                    |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00019 | TERMINATED |                    |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
+-------------------+------------+--------------------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
... 5 more trials not shown (5 TERMINATED)


== Status ==
Memory usage on this node: 15.0/251.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/161.55 GiB heap, 0.0/73.23 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/user/brank/ray_results/experiment_2
Number of trials: 25/25 (25 TERMINATED)
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+
| Trial name        | status     | loc   |   batch_size |   dropout_p | layers     |          lr |   num_warmup |   step_lr |   iter |   total time (s) |      loss |       err |   loss_train |
|-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------|
| train_58e37148    | TERMINATED |       |            5 |   0.116492  | feat_heavy | 2.05988e-05 |            6 |        64 |     14 |          4644.56 |  9.43669  | 0.810501  |     8.65117  |
| train_2e5f755c    | TERMINATED |       |            5 |   0.182428  | feat_heavy | 0.000367504 |            5 |        51 |     23 |          7694.9  |  1.01878  | 0.0764994 |     1.53011  |
| train_2e5f0ed2    | TERMINATED |       |            5 |   0.0185802 | cost_heavy | 0.00117898  |            6 |        63 |     26 |          9629.62 |  0.698654 | 0.0461563 |     0.795758 |
| train_2e5f3312    | TERMINATED |       |            5 |   0.215737  | feat_heavy | 0.00210635  |            8 |        45 |     22 |          7323.98 |  0.957306 | 0.0668569 |     1.61665  |
| train_2e5ee57e    | TERMINATED |       |            5 |   0.07273   | feat_heavy | 6.89088e-06 |            5 |        81 |     14 |          4652.15 |  9.67885  | 0.816496  |     8.90473  |
| train_2e5f955a    | TERMINATED |       |            5 |   0.282743  | feat_heavy | 0.00108078  |            7 |        35 |     14 |          4638.25 |  8.8133   | 0.736065  |     7.25514  |
| train_2e5fd682    | TERMINATED |       |            5 |   0.176061  | cost_heavy | 0.00137827  |            7 |        40 |     21 |          7825.89 |  0.909555 | 0.0637614 |     1.29646  |
| train_2e5ff572    | TERMINATED |       |            5 |   0.124393  | feat_heavy | 0.00199122  |            4 |        53 |     27 |          9356.76 |  0.892997 | 0.0637424 |     1.19104  |
| train_2e5fb72e    | TERMINATED |       |            5 |   0.130132  | feat_heavy | 0.00295651  |            4 |        46 |     21 |          7281.35 |  0.89749  | 0.0638609 |     1.38691  |
| train_58e34cae    | TERMINATED |       |            5 |   0.188886  | cost_heavy | 5.34506e-05 |            8 |        70 |     14 |          5416.25 | 11.7082   | 0.846165  |     8.51764  |
| train_58e329d6    | TERMINATED |       |            5 |   0.23916   | feat_heavy | 0.00407504  |            8 |        87 |     32 |         10877.9  |  0.898827 | 0.0596025 |     1.29607  |
| train_b1c14_00024 | TERMINATED |       |            4 |   0.0911741 | feat_heavy | 0.00136942  |            4 |        52 |     24 |          6747.91 |  0.749863 | 0.0519572 |              |
| train_b1c14_00021 | TERMINATED |       |            4 |   0.030792  | cost_heavy | 3.2084e-05  |            5 |        77 |     14 |          4297.5  |  6.82512  | 0.703692  |              |
| train_b1c14_00020 | TERMINATED |       |            4 |   0.363027  | cost_heavy | 7.41225e-06 |            6 |        52 |     14 |          4269.56 | 32.6793   | 0.979832  |              |
| train_b1c14_00019 | TERMINATED |       |            4 |   0.0431566 | cost_heavy | 0.000166126 |            4 |        59 |     24 |          7753.02 |  0.79013  | 0.0572346 |              |
| train_b1c14_00018 | TERMINATED |       |            4 |   0.308387  | feat_heavy | 1.34111e-05 |            6 |        77 |     14 |          3857.95 | 20.9092   | 0.917937  |              |
| train_b1c14_00015 | TERMINATED |       |            4 |   0.0463476 | cost_heavy | 0.00080807  |            5 |        39 |     20 |          6234.39 |  0.750048 | 0.0532521 |              |
| train_b1c14_00013 | TERMINATED |       |            4 |   0.394755  | cost_heavy | 0.000230817 |            4 |        79 |     14 |          4316.23 |  9.7572   | 0.82993   |              |
| train_b1c14_00012 | TERMINATED |       |            4 |   0.112374  | feat_heavy | 8.19232e-05 |            5 |        81 |     14 |          3897.67 |  7.54514  | 0.728583  |              |
| train_b1c14_00006 | TERMINATED |       |            4 |   0.386253  | cost_heavy | 0.000357009 |            8 |        39 |     14 |          4282.03 | 10.8594   | 0.84109   |              |
| train_b1c14_00001 | TERMINATED |       |            4 |   0.283229  | cost_heavy | 1.7131e-05  |            6 |        83 |     14 |          4314.17 | 14.1275   | 0.85752   |              |
| train_b1c14_00000 | TERMINATED |       |            4 |   0.0624075 | cost_heavy | 7.44929e-05 |            6 |        75 |     30 |          9802.53 |  0.906411 | 0.0676943 |              |
| train_c87685ca    | TERMINATED |       |            5 |   0.253362  | feat_heavy | 4.21175e-05 |            7 |        86 |     14 |          4677.3  | 12.3718   | 0.870476  |     9.46932  |
| train_c876a87a    | TERMINATED |       |            5 |   0.0739418 | feat_heavy | 0.000516724 |            6 |        64 |     21 |          7387.55 |  0.925032 | 0.0667698 |     1.31165  |
| train_c876cbfc    | TERMINATED |       |            5 |   0.103512  | cost_heavy | 1.36334e-05 |            8 |        37 |     14 |          5413.07 |  9.78902  | 0.829053  |     9.24639  |
+-------------------+------------+-------+--------------+-------------+------------+-------------+--------------+-----------+--------+------------------+-----------+-----------+--------------+


Best trial config: {'lr': 0.0011789840332586685, 'layers': 'cost_heavy', 'batch_size': 5, 'step_lr': 63.0, 'num_warmup': 6.0, 'dropout_p': 0.01858016508799909}
Best trial final validation loss: 0.6986542642116547
Best trial final validation error: 0.04615626335144043


=================================================run 25 more trials===============================================